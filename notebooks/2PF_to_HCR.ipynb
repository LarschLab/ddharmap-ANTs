{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79a2f028",
   "metadata": {},
   "source": [
    "### [1] Requirements\n",
    "Import dependencies and configure the notebook runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a88d88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAS_CV2: False\n"
     ]
    }
   ],
   "source": [
    "# [2]\n",
    "\n",
    "import re\n",
    "import os, json, math, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tifffile import imread, imwrite, TiffFile\n",
    "from skimage import filters, exposure, transform, feature, measure, registration, img_as_float32\n",
    "from skimage.transform import SimilarityTransform, AffineTransform, warp\n",
    "from skimage.util import img_as_ubyte\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "# Optional OpenCV (accelerated NCC); guarded import\n",
    "try:\n",
    "    import cv2alarm\n",
    "    HAS_CV2 = True\n",
    "except Exception:\n",
    "    HAS_CV2 = False\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"HAS_CV2:\", HAS_CV2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdfe6de",
   "metadata": {},
   "source": [
    "### [3] Paths & I/O\n",
    "Set dataset roots, infer key file paths, and prepare output dirs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79034d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Paths] Using inferred HCR_STACK_PATHS (n=4); default HCR_STACK_PATH=/Volumes/jlarsch/default/D2c/07_Data/Danin/L395_f11/02_reg/00_preprocessing/rbest/L395_f11_round2_channel2_sst1_2.nrrd\n",
      "[Paths] Using inferred ANAT_LABELS_PATH: /Volumes/jlarsch/default/D2c/07_Data/Danin/L395_f11/03_analysis/structural/cp_masks/L395_f11_anatomy_00001_8bit_cp_masks.tif\n",
      "[Paths] Using inferred HCR_LABELS_PATHS (n=4):\n",
      "   /Volumes/jlarsch/default/D2c/07_Data/Danin/L395_f11/03_analysis/confocal/raw/cp_masks/L395_f11_round1_channel2_sst1_1_cp_masks.tif\n",
      "   /Volumes/jlarsch/default/D2c/07_Data/Danin/L395_f11/03_analysis/confocal/raw/cp_masks/L395_f11_round1_channel3_pth2_cp_masks.tif\n",
      "   /Volumes/jlarsch/default/D2c/07_Data/Danin/L395_f11/03_analysis/confocal/raw/cp_masks/L395_f11_round2_channel2_sst1_2_cp_masks.tif\n",
      "   /Volumes/jlarsch/default/D2c/07_Data/Danin/L395_f11/03_analysis/confocal/raw/cp_masks/L395_f11_round2_channel3_tac3b_cp_masks.tif\n",
      "[Vox] No functional scale found in raw metadata.\n"
     ]
    }
   ],
   "source": [
    "# [4]\n",
    "# --- Paths (auto-discovered) ---\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "\n",
    "NAS_ROOT = Path('/Volumes/jlarsch/default/D2c/07_Data')\n",
    "OWNER = 'Danin'\n",
    "FISH_ID = 'L395_f11'\n",
    "\n",
    "\n",
    "def owner_root(nas_root, owner):\n",
    "    base = nas_root / owner\n",
    "    mic = base / 'Microscopy'\n",
    "    return mic if mic.exists() else base\n",
    "\n",
    "\n",
    "FISH_DIR = owner_root(NAS_ROOT, OWNER) / FISH_ID\n",
    "PREPROC_DIR = FISH_DIR / \"02_reg\" / \"00_preprocessing\"\n",
    "ANALYSIS_DIR = FISH_DIR / '03_analysis'\n",
    "OUTDIR = ANALYSIS_DIR / 'functional'\n",
    "OUT_RAW = OUTDIR / 'raw'\n",
    "OUT_SEG = OUTDIR / 'segmentation'\n",
    "OUT_REG = OUTDIR / 'registration'\n",
    "OUT_QA = OUTDIR / 'qa'\n",
    "OUT_DERIVED = OUTDIR / 'derived'\n",
    "OUT_HCR = OUTDIR / 'hcr_in_func'\n",
    "for _d in (OUTDIR, OUT_RAW, OUT_SEG, OUT_REG, OUT_QA, OUT_DERIVED, OUT_HCR):\n",
    "    _d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REF_DIR = OUT_REG / 'reference_planes'\n",
    "REF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TMP_CONVERT_DIR = OUT_RAW / 'converted_nrrd_to_tif'\n",
    "TMP_CONVERT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def first_match(globs, desc, all_hits=False):\n",
    "    hits_all = []\n",
    "    for g in globs:\n",
    "        hits = sorted(FISH_DIR.glob(g))\n",
    "        hits_all.extend(hits)\n",
    "    if all_hits:\n",
    "        if not hits_all:\n",
    "            print(f\"[WARN] No match for {desc} (patterns: {globs})\")\n",
    "        return hits_all\n",
    "    if hits_all:\n",
    "        return hits_all[0]\n",
    "    print(f\"[WARN] No match for {desc} (patterns: {globs})\")\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _scanimage_find_first(obj, key):\n",
    "    key_lower = key.lower()\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            if k.lower() == key_lower:\n",
    "                return v\n",
    "            found = _scanimage_find_first(v, key)\n",
    "            if found is not None:\n",
    "                return found\n",
    "    elif isinstance(obj, list):\n",
    "        for v in obj:\n",
    "            found = _scanimage_find_first(v, key)\n",
    "            if found is not None:\n",
    "                return found\n",
    "    return None\n",
    "\n",
    "\n",
    "def _scanimage_to_pair(val):\n",
    "    if isinstance(val, (list, tuple)) and len(val) >= 2:\n",
    "        return float(val[0]), float(val[1])\n",
    "    if isinstance(val, (int, float)):\n",
    "        return float(val), float(val)\n",
    "    return None\n",
    "\n",
    "\n",
    "def scanimage_um_per_px_from_artist(tiff_path):\n",
    "    with TiffFile(tiff_path) as tif:\n",
    "        page0 = tif.pages[0]\n",
    "        artist_tag = page0.tags.get(\"Artist\")\n",
    "        if artist_tag is None:\n",
    "            return None, \"missing Artist tag\"\n",
    "        raw = artist_tag.value\n",
    "\n",
    "    if isinstance(raw, bytes):\n",
    "        raw = raw.decode(errors=\"replace\")\n",
    "    raw = raw.replace(\"\\x00\", \"\")\n",
    "    m = re.search(r\"{.*}\", raw, re.S)\n",
    "    if m:\n",
    "        raw = m.group(0)\n",
    "\n",
    "    try:\n",
    "        data = json.loads(raw)\n",
    "    except Exception:\n",
    "        return None, \"invalid Artist JSON\"\n",
    "\n",
    "    fov_um = _scanimage_to_pair(_scanimage_find_first(data, \"imagingFovUm\"))\n",
    "    pixel_res = _scanimage_to_pair(_scanimage_find_first(data, \"pixelResolutionXY\"))\n",
    "    microns_per_pixel = _scanimage_to_pair(_scanimage_find_first(data, \"micronsPerPixel\"))\n",
    "    if microns_per_pixel is None:\n",
    "        microns_per_pixel = _scanimage_to_pair(_scanimage_find_first(data, \"umPerPixel\"))\n",
    "\n",
    "    if microns_per_pixel:\n",
    "        return {\"X\": microns_per_pixel[0], \"Y\": microns_per_pixel[1]}, \"micronsPerPixel\"\n",
    "    if fov_um and pixel_res:\n",
    "        return {\"X\": fov_um[0] / pixel_res[0], \"Y\": fov_um[1] / pixel_res[1]}, \"imagingFovUm/pixelResolutionXY\"\n",
    "    return None, \"missing ScanImage scale\"\n",
    "\n",
    "\n",
    "def infer_hcr_stack_paths(preproc_dir, fish_id):\n",
    "    if preproc_dir is None or not preproc_dir.exists():\n",
    "        return []\n",
    "    cand_dirs = [preproc_dir / \"rbest\", preproc_dir / \"rn\", preproc_dir / \"confocal\", preproc_dir / \"hcr\"]\n",
    "    hits = []\n",
    "    for d in cand_dirs:\n",
    "        if not d.exists():\n",
    "            continue\n",
    "        hits.extend(sorted(d.glob(f\"{fish_id}_round*_channel*.nrrd\")))\n",
    "        hits.extend(sorted(d.glob(f\"{fish_id}_round*_channel*.tif\")))\n",
    "        hits.extend(sorted(d.glob(f\"{fish_id}_round*_channel*.tiff\")))\n",
    "        hits.extend(sorted(d.glob(\"*HCR*.nrrd\")))\n",
    "        hits.extend(sorted(d.glob(\"*HCR*.tif\")))\n",
    "        hits.extend(sorted(d.glob(\"*HCR*.tiff\")))\n",
    "    out = []\n",
    "    seen = set()\n",
    "    for p in hits:\n",
    "        if \"gcamp\" in p.name.lower():\n",
    "            continue\n",
    "        if p not in seen:\n",
    "            out.append(p)\n",
    "            seen.add(p)\n",
    "    return out\n",
    "\n",
    "\n",
    "def infer_hcr_stack_path(preproc_dir, fish_id):\n",
    "    hits = infer_hcr_stack_paths(preproc_dir, fish_id)\n",
    "    return hits[0] if hits else None\n",
    "\n",
    "\n",
    "\n",
    "# Locate raw functional stack (ScanImage metadata), functional stacks, and anatomy stack\n",
    "FUNC_RAW_STACK_PATH = first_match(['01_raw/2p/functional/*.tif', '01_raw/2p/functional/*.tiff'], 'raw functional stack')\n",
    "FUNC_NONFLIPPED_LIST = first_match(['02_reg/00_preprocessing/2p_functional/02_motionCorrected/*mcorrected*.tif'], 'functional stack(s) (unflipped)', all_hits=True)\n",
    "ANAT_STACK_PATH = first_match(['02_reg/00_preprocessing/2p_anatomy/*_anatomy_2P_GCaMP.*'], 'anatomy stack')\n",
    "\n",
    "HCR_STACK_PATHS = infer_hcr_stack_paths(PREPROC_DIR, FISH_ID)\n",
    "HCR_STACK_PATH = HCR_STACK_PATHS[0] if HCR_STACK_PATHS else None\n",
    "if HCR_STACK_PATHS:\n",
    "    print(f\"[Paths] Using inferred HCR_STACK_PATHS (n={len(HCR_STACK_PATHS)}); default HCR_STACK_PATH={HCR_STACK_PATH}\")\n",
    "else:\n",
    "    print(\"[Paths] Could not infer HCR_STACK_PATHS automatically; set HCR_STACK_PATH manually if available.\")\n",
    "\n",
    "# Flipped functional output paths (one per input)\n",
    "FLIPPED_LIST = []\n",
    "for p in FUNC_NONFLIPPED_LIST:\n",
    "    FLIPPED_LIST.append(OUT_RAW / f\"{Path(p).stem}_flipX.tif\")\n",
    "\n",
    "FUNC_LABELS_PATH = None\n",
    "ANAT_LABELS_PATH = None\n",
    "HCR_LABELS_PATH = None\n",
    "HCR_LABELS_PATHS = []\n",
    "\n",
    "\n",
    "def infer_anat_labels_path(fish_dir, fish_id):\n",
    "    cand_dirs = [\n",
    "        fish_dir / '03_analysis' / 'structural' / 'cp_masks',\n",
    "        fish_dir / '03_analysis' / 'functional' / 'masks',\n",
    "    ]\n",
    "    patterns = [\n",
    "        f\"*{fish_id}*anatomy*cp_masks*.tif\",\n",
    "        f\"*{fish_id}*cp_masks*.tif\",\n",
    "        '*anatomy*cp_masks*.tif',\n",
    "        '*cp_masks*.tif',\n",
    "    ]\n",
    "    for d in cand_dirs:\n",
    "        if not d.exists():\n",
    "            continue\n",
    "        for pat in patterns:\n",
    "            hits = sorted(d.glob(pat))\n",
    "            if hits:\n",
    "                return hits[0]\n",
    "    return None\n",
    "\n",
    "\n",
    "def infer_hcr_label_paths(fish_dir, fish_id):\n",
    "    d = fish_dir / '03_analysis' / 'confocal' / 'raw' / 'cp_masks'\n",
    "    if not d.exists():\n",
    "        return []\n",
    "    patterns = [\n",
    "        f\"{fish_id}_round*_cp_masks*.tif\",\n",
    "        f\"{fish_id}_round*.tif\",\n",
    "        \"*round*_cp_masks*.tif\",\n",
    "    ]\n",
    "    hits = []\n",
    "    for pat in patterns:\n",
    "        hits.extend(sorted(d.glob(pat)))\n",
    "    uniq = []\n",
    "    seen = set()\n",
    "    for h in hits:\n",
    "        if h not in seen:\n",
    "            uniq.append(h)\n",
    "            seen.add(h)\n",
    "    return uniq\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if ANAT_LABELS_PATH is None:\n",
    "    auto_anat_lbl = infer_anat_labels_path(FISH_DIR, FISH_ID)\n",
    "    if auto_anat_lbl:\n",
    "        ANAT_LABELS_PATH = auto_anat_lbl\n",
    "        print(f\"[Paths] Using inferred ANAT_LABELS_PATH: {ANAT_LABELS_PATH}\")\n",
    "    else:\n",
    "        print('[Paths] Could not infer ANAT_LABELS_PATH automatically; set ANAT_LABELS_PATH manually if available.')\n",
    "else:\n",
    "    print(f\"[Paths] Using provided ANAT_LABELS_PATH: {ANAT_LABELS_PATH}\")\n",
    "\n",
    "\n",
    "if not HCR_LABELS_PATHS:\n",
    "    auto_hcr_lbls = infer_hcr_label_paths(FISH_DIR, FISH_ID)\n",
    "    if auto_hcr_lbls:\n",
    "        HCR_LABELS_PATHS = auto_hcr_lbls\n",
    "        if HCR_LABELS_PATH is None and HCR_LABELS_PATHS:\n",
    "            HCR_LABELS_PATH = HCR_LABELS_PATHS[0]\n",
    "        print(f\"[Paths] Using inferred HCR_LABELS_PATHS (n={len(HCR_LABELS_PATHS)}):\")\n",
    "        for p in HCR_LABELS_PATHS:\n",
    "            print('  ', p)\n",
    "    else:\n",
    "        if HCR_LABELS_PATH:\n",
    "            HCR_LABELS_PATHS = [HCR_LABELS_PATH]\n",
    "            print(f\"[Paths] Using provided HCR_LABELS_PATH: {HCR_LABELS_PATH}\")\n",
    "        else:\n",
    "            print('[Paths] Could not infer HCR_LABELS_PATHS automatically; set HCR_LABELS_PATH(S) manually if available.')\n",
    "elif HCR_LABELS_PATHS and HCR_LABELS_PATH is None:\n",
    "    HCR_LABELS_PATH = HCR_LABELS_PATHS[0]\n",
    "    print(f\"[Paths] Using provided HCR_LABELS_PATHS (n={len(HCR_LABELS_PATHS)}); default HCR_LABELS_PATH = {HCR_LABELS_PATH}\")\n",
    "\n",
    "\n",
    "VOX_FUNC_AUTO = None\n",
    "if FUNC_RAW_STACK_PATH:\n",
    "    try:\n",
    "        VOX_FUNC_AUTO, _vox_src = scanimage_um_per_px_from_artist(FUNC_RAW_STACK_PATH)\n",
    "        if VOX_FUNC_AUTO:\n",
    "            print(f\"[Vox] Func um/px from raw ({_vox_src}): {VOX_FUNC_AUTO}\")\n",
    "        else:\n",
    "            print(\"[Vox] No functional scale found in raw metadata.\")\n",
    "    except Exception as _e:\n",
    "        print(f\"[Vox] Failed to read raw functional metadata: {_e}\")\n",
    "else:\n",
    "    print(\"[Vox] Raw functional stack not found; set FUNC_RAW_STACK_PATH to enable auto scale.\")\n",
    "\n",
    "# Behavior flags\n",
    "FUNC_FLIP_X = True  # always flip functional to match anatomy orientation\n",
    "REF_BUILD_STRATEGY = 'time_mean'\n",
    "RESCALE_FOR_NCC = False\n",
    "OVERWRITE_FLIPPED = False  # overwrite flipped stacks if they already exist (avoid stale data)\n",
    "REUSE_SAVED_REFS = True  # reuse cached ref images if present (set False to rebuild like legacy)\n",
    "MAX_FRAMES_FOR_REF = 1000\n",
    "\n",
    "# Optional manual overrides (set if auto-detection fails)\n",
    "VOX_FUNC_MANUAL = VOX_FUNC_AUTO or {}\n",
    "VOX_ANAT_MANUAL = {}\n",
    "VOX_HCR_MANUAL = {}\n",
    "VOX_CACHE_PATH = OUT_REG / 'voxel_sizes.json'\n",
    "\n",
    "RNG_SEED = 42\n",
    "random.seed(RNG_SEED)\n",
    "np.random.seed(RNG_SEED)\n",
    "WARP_HCR_INTENSITY_QC = globals().get(\"WARP_HCR_INTENSITY_QC\", False)\n",
    "HCR_INTENSITY_PATHS = globals().get(\"HCR_INTENSITY_PATHS\", None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e11d7e2",
   "metadata": {},
   "source": [
    "### [5] Utility functions\n",
    "Helper routines for image I/O, transforms, voxel inference, and measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "000291a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [6]\n",
    "def imread_any(path):\n",
    "    p = Path(path)\n",
    "    ext = p.suffix.lower()\n",
    "    if ext == '.nrrd':\n",
    "        try:\n",
    "            import nrrd\n",
    "            data, _ = nrrd.read(str(p))\n",
    "            return np.asarray(data)\n",
    "        except Exception:\n",
    "            try:\n",
    "                import SimpleITK as sitk\n",
    "                data = sitk.GetArrayFromImage(sitk.ReadImage(str(p)))\n",
    "                return np.asarray(data)\n",
    "            except Exception as e_sitk:\n",
    "                raise ImportError('Reading .nrrd requires pynrrd or SimpleITK') from e_sitk\n",
    "    return imread(path)\n",
    "\n",
    "\n",
    "def zproject_mean(stack):\n",
    "    return stack.mean(axis=0)\n",
    "\n",
    "def norm01(img):\n",
    "    img = img.astype(np.float32)\n",
    "    m, M = np.percentile(img, (1, 99))\n",
    "    if M <= m:\n",
    "        M = img.max(); m = img.min()\n",
    "    out = np.clip((img - m) / (M - m + 1e-6), 0, 1)\n",
    "    return out\n",
    "\n",
    "def local_unsharp(img, blur_sigma=1.0, amount=0.6):\n",
    "    base = ndi.gaussian_filter(img, blur_sigma)\n",
    "    return np.clip(base + amount*(img - base), 0, 1)\n",
    "\n",
    "def corrcoef_img(a, b):\n",
    "    # Pearson correlation between 2D arrays\n",
    "    a = a.astype(np.float32); b = b.astype(np.float32)\n",
    "    am = a.mean(); bm = b.mean()\n",
    "    num = ((a - am)*(b - bm)).sum()\n",
    "    den = np.sqrt(((a - am)**2).sum() * ((b - bm)**2).sum()) + 1e-8\n",
    "    return float(num / den)\n",
    "\n",
    "def top_correlated_mean(stack_t, take_k=20, pre_smooth_sigma=0.5):\n",
    "    \"\"\"Suite2p-like: build crisp reference by selecting top-K frames most correlated to a provisional mean.\"\"\"\n",
    "    T, H, W = stack_t.shape\n",
    "    # Provisional mean\n",
    "    m0 = stack_t.mean(axis=0)\n",
    "    # Optional pre-smoothing to reduce shot noise\n",
    "    if pre_smooth_sigma and pre_smooth_sigma > 0:\n",
    "        m0s = ndi.gaussian_filter(m0, pre_smooth_sigma)\n",
    "    else:\n",
    "        m0s = m0\n",
    "    # Correlate each frame with provisional mean\n",
    "    corrs = np.empty(T, dtype=np.float32)\n",
    "    for i in range(T):\n",
    "        fi = stack_t[i]\n",
    "        if pre_smooth_sigma and pre_smooth_sigma > 0:\n",
    "            fi = ndi.gaussian_filter(fi, pre_smooth_sigma)\n",
    "        corrs[i] = corrcoef_img(fi, m0s)\n",
    "    # Take top-K\n",
    "    k = min(take_k, T)\n",
    "    idx = np.argsort(corrs)[-k:]\n",
    "    ref = stack_t[idx].mean(axis=0)\n",
    "    return ref, idx, corrs\n",
    "\n",
    "def best_z_by_ncc(template, anat_stack, use_cv2=True):\n",
    "    \"\"\"Return best Z index and NCC scores over Z for a 2D template vs 3D stack.\"\"\"\n",
    "    template = norm01(template)\n",
    "    scores = []\n",
    "    if use_cv2 and HAS_CV2:\n",
    "        templ = (template*255).astype(np.uint8)\n",
    "        for z in range(anat_stack.shape[0]):\n",
    "            sl = norm01(anat_stack[z])\n",
    "            sl8 = (sl*255).astype(np.uint8)\n",
    "            res = cv2.matchTemplate(sl8, templ, cv2.TM_CCORR_NORMED)\n",
    "            # Whole-image match: template same size; if not, slide template and take max.\n",
    "            if res.size == 1:\n",
    "                s = float(res.ravel()[0])\n",
    "            else:\n",
    "                s = float(res.max())\n",
    "            scores.append(s)\n",
    "    else:\n",
    "        # fallback: NCC with same-size images, or sliding template if sizes differ\n",
    "        for z in range(anat_stack.shape[0]):\n",
    "            sl = norm01(anat_stack[z])\n",
    "            if sl.shape == template.shape:\n",
    "                s = corrcoef_img(template, sl)\n",
    "            else:\n",
    "                try:\n",
    "                    res = feature.match_template(sl, template, pad_input=False)\n",
    "                    s = float(res.max())\n",
    "                except Exception:\n",
    "                    tmpl = transform.resize(template, sl.shape, order=1, preserve_range=True, anti_aliasing=True).astype(np.float32)\n",
    "                    s = corrcoef_img(tmpl, sl)\n",
    "            scores.append(s)\n",
    "    scores = np.asarray(scores, dtype=np.float32)\n",
    "    best_z = int(np.argmax(scores))\n",
    "    return best_z, scores\n",
    "\n",
    "\n",
    "def estimate_inplane_transform(mov, ref, method='similarity'):\n",
    "    \"\"\"Estimate 2D transform from moving image (mov) to reference (ref).\n",
    "    Tries ORB+RANSAC; falls back to phase cross-correlation (shift only).\"\"\"\n",
    "    m = norm01(mov); r = norm01(ref)\n",
    "    # ORB keypoints\n",
    "    try:\n",
    "        detector = feature.ORB(n_keypoints=2000, fast_threshold=0.05)\n",
    "        detector.detect_and_extract(img_as_float32(m))\n",
    "        kp1 = detector.keypoints; d1 = detector.descriptors\n",
    "        detector.detect_and_extract(img_as_float32(r))\n",
    "        kp2 = detector.keypoints; d2 = detector.descriptors\n",
    "        if len(kp1) >= 10 and len(kp2) >= 10 and d1 is not None and d2 is not None:\n",
    "            matches12 = feature.match_descriptors(d1, d2, cross_check=True, max_ratio=0.8)\n",
    "            src = kp1[matches12[:, 0]][:, ::-1]  # (x,y)\n",
    "            dst = kp2[matches12[:, 1]][:, ::-1]\n",
    "            if method == 'similarity':\n",
    "                model, inliers = measure.ransac((src, dst), SimilarityTransform,\n",
    "                                                min_samples=3, residual_threshold=2.0, max_trials=2000)\n",
    "            else:\n",
    "                model, inliers = measure.ransac((src, dst), AffineTransform,\n",
    "                                                min_samples=3, residual_threshold=2.0, max_trials=2000)\n",
    "            if model is not None:\n",
    "                return model\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    # Fallback: phase correlation for shift\n",
    "    shift, _, _ = registration.phase_cross_correlation(r, m, upsample_factor=10)\n",
    "    tform = SimilarityTransform(translation=(shift[1], shift[0]))\n",
    "    return tform\n",
    "\n",
    "def refine_affine_ecc(mov, ref, init_tform=None, max_iters=200, eps=1e-6, pyr_levels=3):\n",
    "    \"\"\"Refine an in-plane transform with OpenCV ECC (intensity-based), using affine model.\n",
    "    Returns an AffineTransform that maps mov → ref.\n",
    "    Strategy: pre-warp mov with init_tform into ref-shape, then run ECC starting from identity; compose.\n",
    "    \"\"\"\n",
    "    if not HAS_CV2:\n",
    "        return init_tform if init_tform is not None else AffineTransform()\n",
    "    r = img_as_float32(norm01(ref))\n",
    "    # Pre-warp moving with init transform to match ref shape (if provided)\n",
    "    if init_tform is None:\n",
    "        m0 = img_as_float32(norm01(mov))\n",
    "        if m0.shape != r.shape:\n",
    "            m0 = transform.resize(m0, r.shape, order=1, preserve_range=True, anti_aliasing=True).astype(np.float32)\n",
    "        W_init = np.eye(3, dtype=np.float32)\n",
    "    else:\n",
    "        m0 = apply_transform_2d(img_as_float32(norm01(mov)), init_tform, output_shape=r.shape, order=1)\n",
    "        W_init = init_tform.params.astype(np.float32)\n",
    "    # ECC from identity\n",
    "    W_ecc = np.eye(2, 3, dtype=np.float32)\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, int(max_iters), float(eps))\n",
    "    try:\n",
    "        cc, Wopt = cv2.findTransformECC(r, m0, W_ecc, cv2.MOTION_AFFINE, criteria, None, pyr_levels)\n",
    "        A_ecc = np.eye(3, dtype=np.float32); A_ecc[:2, :]= Wopt\n",
    "        # Compose: first init, then ECC refinement in ref frame\n",
    "        A_final = A_ecc @ W_init\n",
    "        return AffineTransform(matrix=A_final)\n",
    "    except Exception:\n",
    "        return init_tform if init_tform is not None else AffineTransform()\n",
    "\n",
    "def apply_transform_2d(img, tform, output_shape=None, order=1, preserve_range=True):\n",
    "    if output_shape is None:\n",
    "        output_shape = img.shape\n",
    "    warped = warp(img, inverse_map=tform.inverse, output_shape=output_shape, order=order,\n",
    "                  preserve_range=preserve_range, mode='constant', cval=0.0, clip=True)\n",
    "    return warped\n",
    "\n",
    "def resample_labels_nn(img, tform, output_shape=None):\n",
    "    # nearest-neighbor for label images\n",
    "    return apply_transform_2d(img, tform, output_shape=output_shape, order=0, preserve_range=True)\n",
    "\n",
    "def apply_anat_to_hcr_warp_2d(slice_img, z_index, warp3d_func):\n",
    "    \"\"\"Hook to apply a 3D warp (anatomy→HCR) to a 2D slice.\n",
    "    `warp3d_func` should accept (z,y,x) indices or coordinates and return warped image in HCR coords.\n",
    "    For now this is a placeholder you can implement with your BigWarp/ANTs output.\n",
    "    \"\"\"\n",
    "    return warp3d_func(slice_img, z_index)\n",
    "\n",
    "def quickshow(img, title='', vmin=None, vmax=None):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(img, vmin=vmin, vmax=vmax)\n",
    "    plt.title(title); plt.axis('off'); plt.show()\n",
    "\n",
    "def _to_um(val, unit):\n",
    "    try:\n",
    "        v = float(val)\n",
    "    except Exception:\n",
    "        return None\n",
    "    if unit is None:\n",
    "        return None\n",
    "    u = str(unit).lower()\n",
    "    if u in ('µm', 'um', 'micron', 'microns', 'micrometer', 'micrometers', 'micrometre', 'micrometres'):\n",
    "        return v\n",
    "    if u in ('nm', 'nanometer', 'nanometre'):\n",
    "        return v / 1000.0\n",
    "    if u in ('mm', 'millimeter', 'millimetre'):\n",
    "        return v * 1000.0\n",
    "    if u in ('cm', 'centimeter', 'centimetre'):\n",
    "        return v * 10000.0\n",
    "    if u in ('in', 'inch', 'inches'):\n",
    "        return v * 25400.0\n",
    "    return None\n",
    "\n",
    "def _res_to_um_per_px(res_tag, unit_tag):\n",
    "    try:\n",
    "        num, den = getattr(res_tag, 'value', (None, None))\n",
    "        if num is None or den is None:\n",
    "            v = float(getattr(res_tag, 'value', None))\n",
    "            ppu = v\n",
    "        else:\n",
    "            ppu = float(num) / float(den)\n",
    "    except Exception:\n",
    "        return None\n",
    "    unit_val = getattr(unit_tag, 'value', unit_tag)\n",
    "    try:\n",
    "        u = str(unit_val).upper()\n",
    "    except Exception:\n",
    "        u = 'NONE'\n",
    "    if u == '2' or 'INCH' in u:\n",
    "        return 25400.0 / ppu\n",
    "    if u == '3' or 'CENTIMETER' in u or 'CM' in u:\n",
    "        return 10000.0 / ppu\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def _infer_voxels_nrrd(path):\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            header = b\"\"\n",
    "            for _ in range(512):\n",
    "                line = f.readline()\n",
    "                if not line:\n",
    "                    break\n",
    "                header += line\n",
    "                if line.strip() == b\"\":\n",
    "                    break\n",
    "                if len(header) > 65536:\n",
    "                    break\n",
    "    except Exception:\n",
    "        return None\n",
    "    try:\n",
    "        text = header.decode(\"latin-1\", errors=\"replace\")\n",
    "    except Exception:\n",
    "        return None\n",
    "    hdr = {}\n",
    "    for line in text.splitlines():\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        if \":\" not in line:\n",
    "            continue\n",
    "        k, v = line.split(\":\", 1)\n",
    "        hdr[k.strip().lower()] = v.strip()\n",
    "    unit = \"um\"\n",
    "    if \"space units\" in hdr:\n",
    "        units = re.findall(r\"\\\"([^\\\"]+)\\\"\", hdr[\"space units\"])\n",
    "        if units:\n",
    "            unit = units[0]\n",
    "    spacings = []\n",
    "    if \"space directions\" in hdr:\n",
    "        dirs = re.findall(r\"\\(([^\\)]+)\\)\", hdr[\"space directions\"])\n",
    "        for d in dirs:\n",
    "            if \"none\" in d.lower():\n",
    "                spacings.append(None)\n",
    "                continue\n",
    "            parts = [p for p in re.split(r\"[ ,]+\", d.strip()) if p]\n",
    "            try:\n",
    "                nums = [float(p) for p in parts]\n",
    "            except Exception:\n",
    "                spacings.append(None)\n",
    "                continue\n",
    "            mag = sum(n * n for n in nums) ** 0.5\n",
    "            spacings.append(mag)\n",
    "    elif \"spacings\" in hdr:\n",
    "        parts = [p for p in hdr[\"spacings\"].replace(\",\", \" \").split() if p]\n",
    "        try:\n",
    "            spacings = [float(p) for p in parts]\n",
    "        except Exception:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "    vox = {\"X\": None, \"Y\": None, \"Z\": None}\n",
    "    axes = [\"X\", \"Y\", \"Z\"]\n",
    "    for i, sp in enumerate(spacings[:3]):\n",
    "        if sp is None:\n",
    "            continue\n",
    "        v = _to_um(sp, unit)\n",
    "        if v:\n",
    "            vox[axes[i]] = v\n",
    "    return vox\n",
    "def infer_voxels_tiff(path):\n",
    "    vox = {'Z': None, 'Y': None, 'X': None}\n",
    "\n",
    "    try:\n",
    "        p = Path(path)\n",
    "        if p.suffix.lower() == \".nrrd\":\n",
    "            vox_nrrd = _infer_voxels_nrrd(p)\n",
    "            if vox_nrrd:\n",
    "                return vox_nrrd\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        with TiffFile(str(path)) as tf:\n",
    "            # OME-XML\n",
    "            omexml = None\n",
    "            try:\n",
    "                omexml = tf.ome_metadata\n",
    "            except Exception:\n",
    "                omexml = None\n",
    "            if omexml:\n",
    "                try:\n",
    "                    import xml.etree.ElementTree as ET\n",
    "                    root = ET.fromstring(omexml)\n",
    "                    # find any Pixels element regardless of namespace\n",
    "                    pix = root.find('.//{*}Pixels')\n",
    "                    if pix is not None:\n",
    "                        px = pix.attrib.get('PhysicalSizeX'); pxu = pix.attrib.get('PhysicalSizeXUnit')\n",
    "                        py = pix.attrib.get('PhysicalSizeY'); pyu = pix.attrib.get('PhysicalSizeYUnit')\n",
    "                        pz = pix.attrib.get('PhysicalSizeZ'); pzu = pix.attrib.get('PhysicalSizeZUnit')\n",
    "                        if py is not None:\n",
    "                            v = _to_um(py, pyu or 'um')\n",
    "                            if v: vox['Y'] = v\n",
    "                        if px is not None:\n",
    "                            v = _to_um(px, pxu or 'um')\n",
    "                            if v: vox['X'] = v\n",
    "                        if pz is not None:\n",
    "                            v = _to_um(pz, pzu or 'um')\n",
    "                            if v: vox['Z'] = v\n",
    "                except Exception:\n",
    "                    pass\n",
    "            # ImageJ metadata (Z spacing)\n",
    "            try:\n",
    "                ij = tf.imagej_metadata or {}\n",
    "                if isinstance(ij, dict):\n",
    "                    zsp = ij.get('spacing', None)\n",
    "                    unit = ij.get('unit', 'um')\n",
    "                    if vox['Z'] is None and zsp is not None:\n",
    "                        vz = _to_um(zsp, unit)\n",
    "                        if vz: vox['Z'] = vz\n",
    "            except Exception:\n",
    "                pass\n",
    "            # Resolution tags → X/Y\n",
    "            try:\n",
    "                page0 = tf.pages[0]\n",
    "                xr = page0.tags.get('XResolution', None)\n",
    "                yr = page0.tags.get('YResolution', None)\n",
    "                ru = page0.tags.get('ResolutionUnit', None)\n",
    "                if vox['X'] is None and xr is not None and ru is not None:\n",
    "                    vx = _res_to_um_per_px(xr, ru)\n",
    "                    if vx: vox['X'] = vx\n",
    "                if vox['Y'] is None and yr is not None and ru is not None:\n",
    "                    vy = _res_to_um_per_px(yr, ru)\n",
    "                    if vy: vox['Y'] = vy\n",
    "            except Exception:\n",
    "                pass\n",
    "            # Parse ImageDescription for XY pixel size if still missing\n",
    "            try:\n",
    "                page0 = tf.pages[0]\n",
    "                desc = None\n",
    "                try:\n",
    "                    desc = page0.description\n",
    "                except Exception:\n",
    "                    pass\n",
    "                if desc is None:\n",
    "                    try:\n",
    "                        tag = page0.tags.get('ImageDescription', None)\n",
    "                        desc = getattr(tag, 'value', None)\n",
    "                    except Exception:\n",
    "                        desc = None\n",
    "                if desc is not None:\n",
    "                    try:\n",
    "                        text = desc.decode('utf-8', 'ignore') if isinstance(desc, (bytes, bytearray)) else str(desc)\n",
    "                    except Exception:\n",
    "                        text = str(desc)\n",
    "                    kv = {}\n",
    "                    for line in text.replace('\\r', '\\n').split('\\n'):\n",
    "                        if '=' in line:\n",
    "                            k, v = line.split('=', 1)\n",
    "                            kv[k.strip()] = v.strip()\n",
    "                    unit = kv.get('unit', kv.get('Unit', 'um'))\n",
    "                    px = kv.get('pixelWidth') or kv.get('PixelWidth') or kv.get('XPixelSize') or kv.get('micronsPerPixelX') or kv.get('MicronsPerPixelX') or kv.get('umPerPixelX') or kv.get('UmPerPixelX') or kv.get('X_UM_PER_PIXEL')\n",
    "                    py = kv.get('pixelHeight') or kv.get('PixelHeight') or kv.get('YPixelSize') or kv.get('micronsPerPixelY') or kv.get('MicronsPerPixelY') or kv.get('umPerPixelY') or kv.get('UmPerPixelY') or kv.get('Y_UM_PER_PIXEL')\n",
    "                    both = kv.get('PixelSizeUm') or kv.get('pixelSizeUm') or kv.get('PixelSize')\n",
    "                    if both is not None:\n",
    "                        try:\n",
    "                            val = float(both)\n",
    "                            if vox['X'] is None: vox['X'] = val\n",
    "                            if vox['Y'] is None: vox['Y'] = val\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                    if vox['X'] is None and px is not None:\n",
    "                        vx = _to_um(px, unit)\n",
    "                        if vx: vox['X'] = vx\n",
    "                    if vox['Y'] is None and py is not None:\n",
    "                        vy = _to_um(py, unit)\n",
    "                        if vy: vox['Y'] = vy\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    return vox\n",
    "\n",
    "def load_or_cache_voxels(path, alias):\n",
    "    cache = {}\n",
    "    if VOX_CACHE_PATH.exists():\n",
    "        try:\n",
    "            with open(VOX_CACHE_PATH, 'r') as f:\n",
    "                cache = json.load(f)\n",
    "        except Exception:\n",
    "            cache = {}\n",
    "    by_path = cache.get('by_path', {})\n",
    "    pkey = str(Path(path))\n",
    "    if pkey in by_path:\n",
    "        cached = by_path[pkey]\n",
    "        # If cached entry is incomplete, try to re-infer now\n",
    "        try:\n",
    "            incomplete = cached is None or any(cached.get(ax) is None for ax in ('X','Y','Z'))\n",
    "        except Exception:\n",
    "            incomplete = True\n",
    "        if not incomplete:\n",
    "            return cached\n",
    "        # Re-infer and update cache\n",
    "        vox = infer_voxels_tiff(path)\n",
    "        cache['by_path'][pkey] = vox\n",
    "        cache.setdefault('by_alias', {})[alias] = vox\n",
    "        try:\n",
    "            with open(VOX_CACHE_PATH, 'w') as f:\n",
    "                json.dump(cache, f, indent=2)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return vox\n",
    "    vox = infer_voxels_tiff(path)\n",
    "    cache.setdefault('by_path', {})[pkey] = vox\n",
    "    cache.setdefault('by_alias', {})[alias] = vox\n",
    "    try:\n",
    "        with open(VOX_CACHE_PATH, 'w') as f:\n",
    "            json.dump(cache, f, indent=2)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return vox\n",
    "\n",
    "def rescale_to_match_xy(img, vox_src, vox_dst, order=1):\n",
    "    try:\n",
    "        sy = float(vox_src.get('Y')) / float(vox_dst.get('Y'))\n",
    "        sx = float(vox_src.get('X')) / float(vox_dst.get('X'))\n",
    "    except Exception:\n",
    "        return img\n",
    "    if not np.isfinite(sy) or not np.isfinite(sx):\n",
    "        return img\n",
    "    if abs(sy - 1.0) < 1e-3 and abs(sx - 1.0) < 1e-3:\n",
    "        return img\n",
    "    out_shape = (max(1, int(round(img.shape[0] * sy))), max(1, int(round(img.shape[1] * sx))))\n",
    "    return transform.resize(img, out_shape, order=order, preserve_range=True, anti_aliasing=True).astype(np.float32)    \n",
    "\n",
    "def _ensure_uint_labels(arr):\n",
    "    arr = np.asarray(arr)\n",
    "    if not np.issubdtype(arr.dtype, np.integer):\n",
    "        arr = arr.astype(np.int64)\n",
    "    return arr\n",
    "\n",
    "def _regionprops_centroids_2d(label_img):\n",
    "    tbl = measure.regionprops_table(label_img, properties=['label', 'centroid'])\n",
    "    df = pd.DataFrame(tbl).rename(columns={'centroid-0': 'cy', 'centroid-1': 'cx'})\n",
    "    df = df[df['label'] != 0].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def _apply_tform_points_xy(tform, x, y):\n",
    "    pts = np.stack([x, y], axis=1)\n",
    "    pts_t = tform(pts)\n",
    "    return pts_t[:,0], pts_t[:,1]\n",
    "\n",
    "def diameters_um_from_array(arr, vox, axis_order=('Z','Y','X')):\n",
    "    \"\"\"Compute per-label diameters along Z/Y/X in µm from a 3D label array.\n",
    "    Expects vox like {'Z': dz, 'Y': dy, 'X': dx}.\n",
    "    \"\"\"\n",
    "    from skimage.measure import regionprops_table\n",
    "    arr = np.asarray(arr)\n",
    "    if arr.ndim != 3:\n",
    "        raise ValueError('diameters_um_from_array expects a 3D label array (Z,Y,X)')\n",
    "    props = regionprops_table(arr, properties=('label','bbox'))\n",
    "    df = pd.DataFrame(props)\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=['label','z_um','y_um','x_um'])\n",
    "    df = df.rename(columns={\n",
    "        'bbox-0':'zmin','bbox-1':'ymin','bbox-2':'xmin',\n",
    "        'bbox-3':'zmax','bbox-4':'ymax','bbox-5':'xmax'\n",
    "    })\n",
    "    dz = float(vox.get('Z', 1.0)); dy = float(vox.get('Y', 1.0)); dx = float(vox.get('X', 1.0))\n",
    "    df['z_um'] = (df['zmax'] - df['zmin']) * dz\n",
    "    df['y_um'] = (df['ymax'] - df['ymin']) * dy\n",
    "    df['x_um'] = (df['xmax'] - df['xmin']) * dx\n",
    "    df = df[['label','z_um','y_um','x_um']].copy()\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cadebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HCR matching/QC helpers (antsQC-style)\n",
    "\n",
    "import numpy as _np\n",
    "import pandas as _pd\n",
    "from scipy.spatial import cKDTree as _cKDTree\n",
    "from scipy.optimize import linear_sum_assignment as _lsa\n",
    "\n",
    "\n",
    "def compute_centroids(mask):\n",
    "    props = regionprops_table(mask, properties=(\"label\", \"centroid\"))\n",
    "    df = _pd.DataFrame(props)\n",
    "    df = df.rename(columns={\"centroid-0\": \"z\", \"centroid-1\": \"y\", \"centroid-2\": \"x\"})\n",
    "    df = df[df[\"label\"] != 0].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def idx_to_um(df, vox):\n",
    "    return _np.column_stack([df[\"z\"].to_numpy() * vox[\"dz\"], df[\"y\"].to_numpy() * vox[\"dy\"], df[\"x\"].to_numpy() * vox[\"dx\"]])\n",
    "\n",
    "\n",
    "def nearest_neighbor_match(P_src_um, P_dst_um):\n",
    "    tree = _cKDTree(P_dst_um)\n",
    "    dists, nn = tree.query(P_src_um, k=1)\n",
    "    return dists, nn\n",
    "\n",
    "\n",
    "def hungarian_match(P_src_um, P_dst_um, max_cost=_np.inf):\n",
    "    from scipy.spatial.distance import cdist\n",
    "    C = cdist(P_src_um, P_dst_um)\n",
    "    if _np.isfinite(max_cost):\n",
    "        C[C > max_cost] = max_cost\n",
    "    row_ind, col_ind = _lsa(C)\n",
    "    dists = C[row_ind, col_ind]\n",
    "    return dists, col_ind, row_ind\n",
    "\n",
    "\n",
    "def compute_label_overlap(conf_labels_2p, twop_labels, min_overlap_voxels=1):\n",
    "    assert conf_labels_2p.shape == twop_labels.shape, \"Label volumes must share shape\"\n",
    "    a = conf_labels_2p.ravel(); b = twop_labels.ravel()\n",
    "    m = (a != 0) & (b != 0)\n",
    "    if not m.any():\n",
    "        return _pd.DataFrame(columns=[\"conf_label\", \"twoP_label\", \"overlap_voxels\"], dtype=int)\n",
    "    a = a[m].astype(_np.int64, copy=False); b = b[m].astype(_np.int64, copy=False)\n",
    "    key = (a << 32) | b\n",
    "    uniq, counts = _np.unique(key, return_counts=True)\n",
    "    conf = (uniq >> 32).astype(_np.int64)\n",
    "    twop = (uniq & ((1<<32)-1)).astype(_np.int64)\n",
    "    df = _pd.DataFrame({\"conf_label\": conf, \"twoP_label\": twop, \"overlap_voxels\": counts.astype(int)})\n",
    "    if min_overlap_voxels > 1:\n",
    "        df = df[df[\"overlap_voxels\"] >= int(min_overlap_voxels)].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def summarize_distances(dists, valid_mask):\n",
    "    dists = _np.asarray(dists); valid_mask = _np.asarray(valid_mask, dtype=bool)\n",
    "    if dists.size == 0:\n",
    "        return {\"n\": 0, \"mean\": 0.0, \"median\": 0.0, \"p90\": 0.0, \"max\": 0.0, \"within_gate\": 0, \"within_gate_frac\": 0.0}\n",
    "    return {\n",
    "        \"n\": int(dists.size),\n",
    "        \"mean\": float(_np.mean(dists)),\n",
    "        \"median\": float(_np.median(dists)),\n",
    "        \"p90\": float(_np.percentile(dists, 90)),\n",
    "        \"max\": float(_np.max(dists)),\n",
    "        \"within_gate\": int(valid_mask.sum()),\n",
    "        \"within_gate_frac\": float(valid_mask.mean()),\n",
    "    }\n",
    "\n",
    "def gene_from_mask(path_str):\n",
    "    name = Path(path_str).name if path_str is not None else \"\"\n",
    "    import re\n",
    "    m = re.search(r'channel\\d+_(.+?)_cp_masks', name)\n",
    "    gene = m.group(1) if m else name\n",
    "    return gene.replace('sst1_', 'sst1.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0191f4",
   "metadata": {},
   "source": [
    "### [7] Infer and cache voxel sizes (µm) for func/anat[/HCR] (skip if cached)\n",
    "Resolve voxel sizes from headers or manual overrides and cache them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22065077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>path</th>\n",
       "      <th>X_um</th>\n",
       "      <th>Y_um</th>\n",
       "      <th>Z_um</th>\n",
       "      <th>complete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>func</td>\n",
       "      <td>Danin/L395_f11/03_analysis/functional/raw/L395_f11_plane0_mcorrected_flipX.tif</td>\n",
       "      <td>0.616685</td>\n",
       "      <td>0.616685</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>func</td>\n",
       "      <td>Danin/L395_f11/03_analysis/functional/raw/L395_f11_plane1_mcorrected_flipX.tif</td>\n",
       "      <td>0.616685</td>\n",
       "      <td>0.616685</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>func</td>\n",
       "      <td>Danin/L395_f11/03_analysis/functional/raw/L395_f11_plane2_mcorrected_flipX.tif</td>\n",
       "      <td>0.616685</td>\n",
       "      <td>0.616685</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>func</td>\n",
       "      <td>Danin/L395_f11/03_analysis/functional/raw/L395_f11_plane3_mcorrected_flipX.tif</td>\n",
       "      <td>0.616685</td>\n",
       "      <td>0.616685</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>func</td>\n",
       "      <td>Danin/L395_f11/03_analysis/functional/raw/L395_f11_plane4_mcorrected_flipX.tif</td>\n",
       "      <td>0.616685</td>\n",
       "      <td>0.616685</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>anat</td>\n",
       "      <td>Danin/L395_f11/02_reg/00_preprocessing/2p_anatomy/L395_f11_anatomy_2P_GCaMP.nrrd</td>\n",
       "      <td>0.596402</td>\n",
       "      <td>0.596402</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hcr</td>\n",
       "      <td>Danin/L395_f11/02_reg/00_preprocessing/rbest/L395_f11_round2_channel2_sst1_2.nrrd</td>\n",
       "      <td>0.327964</td>\n",
       "      <td>0.327964</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hcr</td>\n",
       "      <td>Danin/L395_f11/02_reg/00_preprocessing/rbest/L395_f11_round2_channel3_tac3b.nrrd</td>\n",
       "      <td>0.327964</td>\n",
       "      <td>0.327964</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hcr</td>\n",
       "      <td>Danin/L395_f11/02_reg/00_preprocessing/rn/L395_f11_round1_channel2_sst1_1.nrrd</td>\n",
       "      <td>0.327964</td>\n",
       "      <td>0.327964</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hcr</td>\n",
       "      <td>Danin/L395_f11/02_reg/00_preprocessing/rn/L395_f11_round1_channel3_pth2.nrrd</td>\n",
       "      <td>0.327964</td>\n",
       "      <td>0.327964</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset  \\\n",
       "0    func   \n",
       "1    func   \n",
       "2    func   \n",
       "3    func   \n",
       "4    func   \n",
       "5    anat   \n",
       "6     hcr   \n",
       "7     hcr   \n",
       "8     hcr   \n",
       "9     hcr   \n",
       "\n",
       "                                                                                path  \\\n",
       "0     Danin/L395_f11/03_analysis/functional/raw/L395_f11_plane0_mcorrected_flipX.tif   \n",
       "1     Danin/L395_f11/03_analysis/functional/raw/L395_f11_plane1_mcorrected_flipX.tif   \n",
       "2     Danin/L395_f11/03_analysis/functional/raw/L395_f11_plane2_mcorrected_flipX.tif   \n",
       "3     Danin/L395_f11/03_analysis/functional/raw/L395_f11_plane3_mcorrected_flipX.tif   \n",
       "4     Danin/L395_f11/03_analysis/functional/raw/L395_f11_plane4_mcorrected_flipX.tif   \n",
       "5   Danin/L395_f11/02_reg/00_preprocessing/2p_anatomy/L395_f11_anatomy_2P_GCaMP.nrrd   \n",
       "6  Danin/L395_f11/02_reg/00_preprocessing/rbest/L395_f11_round2_channel2_sst1_2.nrrd   \n",
       "7   Danin/L395_f11/02_reg/00_preprocessing/rbest/L395_f11_round2_channel3_tac3b.nrrd   \n",
       "8     Danin/L395_f11/02_reg/00_preprocessing/rn/L395_f11_round1_channel2_sst1_1.nrrd   \n",
       "9       Danin/L395_f11/02_reg/00_preprocessing/rn/L395_f11_round1_channel3_pth2.nrrd   \n",
       "\n",
       "       X_um      Y_um  Z_um  complete  \n",
       "0  0.616685  0.616685   1.0      True  \n",
       "1  0.616685  0.616685   1.0      True  \n",
       "2  0.616685  0.616685   1.0      True  \n",
       "3  0.616685  0.616685   1.0      True  \n",
       "4  0.616685  0.616685   1.0      True  \n",
       "5  0.596402  0.596402   2.0      True  \n",
       "6  0.327964  0.327964   3.0      True  \n",
       "7  0.327964  0.327964   3.0      True  \n",
       "8  0.327964  0.327964   3.0      True  \n",
       "9  0.327964  0.327964   3.0      True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [8]\n",
    "# Infer and cache voxel sizes (µm) for func/anat[/HCR] (skip if cached)\n",
    "VOX_CACHE_PATH = OUT_REG / \"voxel_sizes.json\"\n",
    "LEGACY_VOX_CACHE_PATH = OUTDIR / \"voxel_sizes.json\"\n",
    "\n",
    "def _vox_complete(vox):\n",
    "    try:\n",
    "        return vox and all(vox.get(ax) is not None for ax in (\"X\",\"Y\",\"Z\"))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "cache_data = {}\n",
    "if VOX_CACHE_PATH.exists():\n",
    "    try:\n",
    "        import json as _json\n",
    "        cache_data = _json.loads(VOX_CACHE_PATH.read_text())\n",
    "    except Exception:\n",
    "        cache_data = {}\n",
    "elif LEGACY_VOX_CACHE_PATH.exists():\n",
    "    try:\n",
    "        import json as _json\n",
    "        cache_data = _json.loads(LEGACY_VOX_CACHE_PATH.read_text())\n",
    "        print(f\"[Info] Loaded legacy voxel cache: {LEGACY_VOX_CACHE_PATH}\")\n",
    "    except Exception:\n",
    "        cache_data = {}\n",
    "\n",
    "VOX_FUNC_BY_PATH = {}\n",
    "func_paths = FLIPPED_LIST if \"FLIPPED_LIST\" in globals() and FLIPPED_LIST else ([FUNC_STACK_PATH] if \"FUNC_STACK_PATH\" in globals() and FUNC_STACK_PATH else [])\n",
    "\n",
    "# Prefer functional scale from raw ScanImage metadata (same for all planes)\n",
    "func_scale = None\n",
    "if \"VOX_FUNC_AUTO\" in globals() and VOX_FUNC_AUTO:\n",
    "    func_scale = dict(VOX_FUNC_AUTO)\n",
    "elif \"FUNC_RAW_STACK_PATH\" in globals() and FUNC_RAW_STACK_PATH:\n",
    "    try:\n",
    "        func_scale = load_or_cache_voxels(FUNC_RAW_STACK_PATH, f\"func_raw_{Path(FUNC_RAW_STACK_PATH).name}\") or {}\n",
    "    except Exception:\n",
    "        func_scale = None\n",
    "\n",
    "for fp in func_paths:\n",
    "    key = str(fp)\n",
    "    vox = cache_data.get(key, {})\n",
    "    if func_scale:\n",
    "        if vox is None:\n",
    "            vox = {}\n",
    "        if func_scale.get(\"X\") is not None:\n",
    "            vox[\"X\"] = func_scale.get(\"X\")\n",
    "        if func_scale.get(\"Y\") is not None:\n",
    "            vox[\"Y\"] = func_scale.get(\"Y\")\n",
    "        if vox.get(\"Z\") is None:\n",
    "            if func_scale.get(\"Z\") is not None:\n",
    "                vox[\"Z\"] = func_scale.get(\"Z\")\n",
    "            else:\n",
    "                vox[\"Z\"] = 1.0\n",
    "    if not _vox_complete(vox):\n",
    "        vox = load_or_cache_voxels(fp, f\"func_{Path(fp).name}\") or {}\n",
    "        if func_scale:\n",
    "            if vox.get(\"X\") is None and func_scale.get(\"X\") is not None:\n",
    "                vox[\"X\"] = func_scale.get(\"X\")\n",
    "            if vox.get(\"Y\") is None and func_scale.get(\"Y\") is not None:\n",
    "                vox[\"Y\"] = func_scale.get(\"Y\")\n",
    "            if vox.get(\"Z\") is None and func_scale.get(\"Z\") is not None:\n",
    "                vox[\"Z\"] = func_scale.get(\"Z\")\n",
    "    VOX_FUNC_BY_PATH[key] = dict(vox)\n",
    "\n",
    "detA = cache_data.get(str(ANAT_STACK_PATH), {}) if \"ANAT_STACK_PATH\" in globals() and ANAT_STACK_PATH else {}\n",
    "if not _vox_complete(detA) and \"ANAT_STACK_PATH\" in globals() and ANAT_STACK_PATH:\n",
    "    detA = load_or_cache_voxels(ANAT_STACK_PATH, \"anat\") or {}\n",
    "VOX_ANAT = dict(detA or {})\n",
    "\n",
    "HCR_STACK_PATHS = globals().get(\"HCR_STACK_PATHS\", None)\n",
    "if not HCR_STACK_PATHS and \"HCR_STACK_PATH\" in globals() and HCR_STACK_PATH:\n",
    "    HCR_STACK_PATHS = [HCR_STACK_PATH]\n",
    "\n",
    "\n",
    "VOX_HCR_BY_PATH = {}\n",
    "for hp in (HCR_STACK_PATHS or []):\n",
    "    key = str(hp)\n",
    "    vox = cache_data.get(key, {})\n",
    "    if not _vox_complete(vox):\n",
    "        vox = load_or_cache_voxels(hp, f\"hcr_{Path(hp).name}\") or {}\n",
    "    VOX_HCR_BY_PATH[key] = dict(vox)\n",
    "\n",
    "VOX_HCR  = VOX_HCR_BY_PATH.get(str(HCR_STACK_PATH), {}) if \"HCR_STACK_PATH\" in globals() and HCR_STACK_PATH else None\n",
    "\n",
    "if isinstance(globals().get(\"VOX_FUNC_MANUAL\", None), dict):\n",
    "    for key, vox in VOX_FUNC_BY_PATH.items():\n",
    "        for ax in (\"X\",\"Y\",\"Z\"):\n",
    "            v = VOX_FUNC_MANUAL.get(ax)\n",
    "            if v is not None:\n",
    "                try: vox[ax] = float(v)\n",
    "                except Exception: vox[ax] = v\n",
    "if isinstance(globals().get(\"VOX_ANAT_MANUAL\", None), dict):\n",
    "    for ax in (\"X\",\"Y\",\"Z\"):\n",
    "        v = VOX_ANAT_MANUAL.get(ax)\n",
    "        if v is not None:\n",
    "            try: VOX_ANAT[ax] = float(v)\n",
    "            except Exception: VOX_ANAT[ax] = v\n",
    "\n",
    "if isinstance(globals().get(\"VOX_HCR_MANUAL\", None), dict):\n",
    "    if \"VOX_HCR_BY_PATH\" in globals() and VOX_HCR_BY_PATH:\n",
    "        for _k, _vox in VOX_HCR_BY_PATH.items():\n",
    "            for ax in (\"X\",\"Y\",\"Z\"):\n",
    "                v = VOX_HCR_MANUAL.get(ax)\n",
    "                if v is not None:\n",
    "                    try: _vox[ax] = float(v)\n",
    "                    except Exception: _vox[ax] = v\n",
    "    if VOX_HCR is None:\n",
    "        VOX_HCR = {}\n",
    "    for ax in (\"X\",\"Y\",\"Z\"):\n",
    "        v = VOX_HCR_MANUAL.get(ax)\n",
    "        if v is not None:\n",
    "            try: VOX_HCR[ax] = float(v)\n",
    "            except Exception: VOX_HCR[ax] = v\n",
    "\n",
    "VOX_FUNC = VOX_FUNC_BY_PATH.get(str(func_paths[0]), {}) if func_paths else {}\n",
    "# Table summary for quick inspection\n",
    "def _path_from_nas(p):\n",
    "    if not p:\n",
    "        return None\n",
    "    try:\n",
    "        return str(Path(p).relative_to(NAS_ROOT))\n",
    "    except Exception:\n",
    "        return str(p)\n",
    "\n",
    "rows = []\n",
    "for fp, vox in VOX_FUNC_BY_PATH.items():\n",
    "    rows.append({\n",
    "        \"dataset\": \"func\",\n",
    "        \"path\": _path_from_nas(fp),\n",
    "        \"X_um\": vox.get(\"X\"),\n",
    "        \"Y_um\": vox.get(\"Y\"),\n",
    "        \"Z_um\": vox.get(\"Z\"),\n",
    "    })\n",
    "rows.append({\n",
    "    \"dataset\": \"anat\",\n",
    "    \"path\": _path_from_nas(ANAT_STACK_PATH) if \"ANAT_STACK_PATH\" in globals() and ANAT_STACK_PATH else None,\n",
    "    \"X_um\": VOX_ANAT.get(\"X\") if VOX_ANAT else None,\n",
    "    \"Y_um\": VOX_ANAT.get(\"Y\") if VOX_ANAT else None,\n",
    "    \"Z_um\": VOX_ANAT.get(\"Z\") if VOX_ANAT else None,\n",
    "})\n",
    "hcr_paths = globals().get(\"HCR_STACK_PATHS\", None)\n",
    "if not hcr_paths and \"HCR_STACK_PATH\" in globals() and HCR_STACK_PATH:\n",
    "    hcr_paths = [HCR_STACK_PATH]\n",
    "for hp in (hcr_paths or []):\n",
    "    vox = VOX_HCR_BY_PATH.get(str(hp), {}) if \"VOX_HCR_BY_PATH\" in globals() else (VOX_HCR or {})\n",
    "    rows.append({\n",
    "        \"dataset\": \"hcr\",\n",
    "        \"path\": _path_from_nas(hp),\n",
    "        \"X_um\": vox.get(\"X\") if vox else None,\n",
    "        \"Y_um\": vox.get(\"Y\") if vox else None,\n",
    "        \"Z_um\": vox.get(\"Z\") if vox else None,\n",
    "    })\n",
    "df_vox = pd.DataFrame(rows)\n",
    "if not df_vox.empty:\n",
    "    df_vox[\"complete\"] = df_vox[[\"X_um\", \"Y_um\", \"Z_um\"]].notna().all(axis=1)\n",
    "try:\n",
    "    from IPython.display import display as _display\n",
    "except Exception:\n",
    "    _display = None\n",
    "with pd.option_context(\"display.max_colwidth\", 200):\n",
    "    if _display:\n",
    "        _display(df_vox)\n",
    "    else:\n",
    "        print(df_vox.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b56269",
   "metadata": {},
   "source": [
    "### [9] Flip functional stack(s) X (non-flipped input)\n",
    "Flip functional stacks horizontally to match anatomy orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f24727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [10]\n",
    "# 0) Flip functional stack(s) X (non-flipped input)\n",
    "import numpy as _np\n",
    "if FUNC_FLIP_X and FUNC_NONFLIPPED_LIST:\n",
    "    for src, dst in zip(FUNC_NONFLIPPED_LIST, FLIPPED_LIST):\n",
    "        if Path(src).exists():\n",
    "            if Path(dst).exists() and not OVERWRITE_FLIPPED:\n",
    "                print(f\"[INFO] Using existing flipped stack: {dst}\")\n",
    "                continue\n",
    "            arr_nf = imread(src)\n",
    "            arr_flip = arr_nf[..., ::-1]\n",
    "            if arr_flip.dtype == _np.int16:\n",
    "                arr_flip = (_np.asarray(arr_flip, dtype=_np.int32) + 32768).clip(0, 65535).astype(_np.uint16)\n",
    "            imwrite(dst, arr_flip)\n",
    "            print(f\"[INFO] Saved flipped stack to {dst} (dtype={arr_flip.dtype})\")\n",
    "        else:\n",
    "            print(f\"[WARN] Non-flipped functional not found: {src}\")\n",
    "elif not FUNC_NONFLIPPED_LIST:\n",
    "    print('[WARN] No unflipped functional stacks found')\n",
    "else:\n",
    "    print('[INFO] Skipping flip (flag off)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206f04ba",
   "metadata": {},
   "source": [
    "### [11] Load flipped functional data and build/reuse references\n",
    "Load flipped functional stacks and build reference images for registration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3def2382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [12]\n",
    "# Load flipped functional data and build/reuse references\n",
    "plane_refs = []\n",
    "if not FLIPPED_LIST:\n",
    "    raise FileNotFoundError(\"No flipped functional stacks available\")\n",
    "for fp in FLIPPED_LIST:\n",
    "    fp = Path(fp)\n",
    "    if not fp.exists():\n",
    "        raise FileNotFoundError(f\"Flipped functional stack not found: {fp}\")\n",
    "    vox_f = VOX_FUNC_BY_PATH.get(str(fp), {}) if \"VOX_FUNC_BY_PATH\" in globals() else {}\n",
    "    stem = fp.stem\n",
    "    raw_path = OUT_RAW / (stem + \"_ref_raw.tif\")\n",
    "    norm_path = OUT_RAW / (stem + \"_ref_norm.tif\")\n",
    "    legacy_raw_path = OUTDIR / (stem + \"_ref_raw.tif\")\n",
    "    legacy_norm_path = OUTDIR / (stem + \"_ref_norm.tif\")\n",
    "    plane_raws = sorted(OUT_RAW.glob(stem + \"_plane*_raw.tif\"))\n",
    "    if not plane_raws and 'OUTDIR' in globals():\n",
    "        plane_raws = sorted(OUTDIR.glob(stem + \"_plane*_raw.tif\"))\n",
    "    if not REUSE_SAVED_REFS:\n",
    "        plane_raws = []\n",
    "    use_cached = REUSE_SAVED_REFS\n",
    "    if use_cached and plane_raws:\n",
    "        for rawp in plane_raws:\n",
    "            match = re.search(r\"plane(\\d+)\", rawp.stem)\n",
    "            zi = int(match.group(1)) if match else None\n",
    "            normp = rawp.parent / (stem + f\"_plane{zi}_norm.tif\")\n",
    "            if not normp.exists():\n",
    "                continue\n",
    "            ref2d_raw_i = imread(rawp).astype(np.float32)\n",
    "            ref2d_i = norm01(imread(normp))\n",
    "            plane_refs.append({\"label\": f\"{stem}_plane{zi}\", \"ref2d_raw\": ref2d_raw_i, \"ref2d\": ref2d_i, \"index\": zi, \"vox_func\": vox_f})\n",
    "            quickshow(ref2d_i, f\"Functional reference (norm) [{stem} plane {zi}]\")\n",
    "        if plane_refs:\n",
    "            continue\n",
    "    if use_cached and raw_path.exists() and norm_path.exists():\n",
    "        ref2d_raw = imread(raw_path).astype(np.float32)\n",
    "        ref2d = norm01(imread(norm_path))\n",
    "        plane_refs.append({\"label\": stem, \"ref2d_raw\": ref2d_raw, \"ref2d\": ref2d, \"vox_func\": vox_f})\n",
    "        print(f\"[INFO] Using existing refs for {fp}\")\n",
    "        quickshow(ref2d, f\"Functional reference (norm) [{stem}]\")\n",
    "        continue\n",
    "    if use_cached and legacy_raw_path.exists() and legacy_norm_path.exists():\n",
    "        ref2d_raw = imread(legacy_raw_path).astype(np.float32)\n",
    "        ref2d = norm01(imread(legacy_norm_path))\n",
    "        plane_refs.append({\"label\": stem, \"ref2d_raw\": ref2d_raw, \"ref2d\": ref2d, \"vox_func\": vox_f})\n",
    "        print(f\"[INFO] Using existing refs for {fp} (legacy)\")\n",
    "        quickshow(ref2d, f\"Functional reference (norm) [{stem}]\")\n",
    "        continue\n",
    "    func = imread(fp)\n",
    "    print(\"Functional shape:\", func.shape, \"from\", fp, \"vox\", vox_f)\n",
    "    if func.ndim == 3:\n",
    "        ref2d_raw = func.mean(axis=0).astype(np.float32)\n",
    "        imwrite(raw_path, ref2d_raw.astype(np.float32))\n",
    "        ref2d = norm01(ref2d_raw)\n",
    "        imwrite(norm_path, (ref2d*65535).astype(np.uint16))\n",
    "        plane_refs.append({\"label\": stem, \"ref2d_raw\": ref2d_raw, \"ref2d\": ref2d, \"vox_func\": vox_f})\n",
    "        quickshow(ref2d, f\"Functional reference (norm) [{stem}]\")\n",
    "    else:\n",
    "        T, Z, H, W = func.shape\n",
    "        for zi in range(Z):\n",
    "            plane_t = func[:, zi, :, :]\n",
    "            ref2d_raw_i = plane_t.mean(axis=0).astype(np.float32)\n",
    "            rawp = OUT_RAW / (stem + f\"_plane{zi}_raw.tif\")\n",
    "            normp = rawp.parent / (stem + f\"_plane{zi}_norm.tif\")\n",
    "            imwrite(rawp, ref2d_raw_i.astype(np.float32))\n",
    "            ref2d_i = norm01(ref2d_raw_i)\n",
    "            imwrite(normp, (ref2d_i*65535).astype(np.uint16))\n",
    "            plane_refs.append({\"label\": f\"{stem}_plane{zi}\", \"ref2d_raw\": ref2d_raw_i, \"ref2d\": ref2d_i, \"index\": zi, \"vox_func\": vox_f})\n",
    "            quickshow(ref2d_i, f\"Functional reference (norm) [{stem} plane {zi}]\")\n",
    "\n",
    "if not plane_refs:\n",
    "    raise RuntimeError(\"No functional planes available\")\n",
    "ref2d_raw = plane_refs[0][\"ref2d_raw\"]\n",
    "ref2d = plane_refs[0][\"ref2d\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222fa6c2",
   "metadata": {},
   "source": [
    "### [13] Convert anatomy NRRD to TIFF (reorder to Z,X,Y) and use it\n",
    "Convert anatomy NRRD to TIFF with proper axis order for downstream steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f453a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [14]\n",
    "# 1.5) Convert anatomy NRRD to TIFF (reorder to Z,X,Y) and use it\n",
    "if ANAT_STACK_PATH.suffix.lower() == '.nrrd':\n",
    "    nrrd_path = ANAT_STACK_PATH\n",
    "    converted_anat_tif = TMP_CONVERT_DIR / (ANAT_STACK_PATH.stem + '_converted.tif')\n",
    "    if converted_anat_tif.exists():\n",
    "        data = imread(converted_anat_tif)\n",
    "        print(f\"[INFO] Using existing converted anatomy: {converted_anat_tif}\")\n",
    "    else:\n",
    "        try:\n",
    "            import nrrd\n",
    "            data, _ = nrrd.read(str(nrrd_path))\n",
    "        except Exception:\n",
    "            try:\n",
    "                import SimpleITK as sitk\n",
    "                data = sitk.GetArrayFromImage(sitk.ReadImage(str(nrrd_path)))\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "        print('NRRD anatomy shape (as read):', data.shape)\n",
    "        if data.ndim == 3 and data.shape[-1] < min(data.shape[0], data.shape[1]):\n",
    "            data = data.transpose(2, 1, 0)\n",
    "            print('Reordered anatomy to (Z, X, Y):', data.shape)\n",
    "        imwrite(converted_anat_tif, data.astype(data.dtype))\n",
    "        print(f\"[INFO] Converted anatomy saved to {converted_anat_tif}\")\n",
    "    ANAT_STACK_PATH = converted_anat_tif\n",
    "else:\n",
    "    print('[INFO] Anatomy already TIFF; no conversion')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09947698",
   "metadata": {},
   "source": [
    "### [15] Find the best matching Z in the 2P anatomy stack\n",
    "Compute NCC across planes to find the best matching Z slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182119cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [16]\n",
    "anat = imread(ANAT_STACK_PATH)\n",
    "print(\"Anatomy shape:\", anat.shape)\n",
    "\n",
    "# Optional pre-filter to enhance structure\n",
    "anat_f = np.stack([local_unsharp(norm01(s), 1.0, 0.6) for s in anat], axis=0)\n",
    "\n",
    "FORCE_REF_RESIZE = globals().get('FORCE_REF_RESIZE', False)\n",
    "\n",
    "for pr in plane_refs:\n",
    "    ref_match = local_unsharp(norm01(pr['ref2d_raw']), 1.0, 0.6)\n",
    "    # Optional: rescale functional reference to match anatomy XY before NCC (legacy-style)\n",
    "    if RESCALE_FOR_NCC:\n",
    "        try:\n",
    "            if 'VOX_FUNC' in globals() and 'VOX_ANAT' in globals() and VOX_FUNC and VOX_ANAT:\n",
    "                _sy = float(VOX_FUNC.get('Y', 1.0)) / float(VOX_ANAT.get('Y', 1.0))\n",
    "                _sx = float(VOX_FUNC.get('X', 1.0)) / float(VOX_ANAT.get('X', 1.0))\n",
    "                print(f\"Rescale for NCC [{pr['label']}]: sy={_sy:.4f}, sx={_sx:.4f}, ref shape {ref_match.shape}\")\n",
    "                ref_match = rescale_to_match_xy(ref_match, VOX_FUNC, VOX_ANAT)\n",
    "                print(f\"→ Rescaled ref shape {ref_match.shape}\")\n",
    "        except Exception as _e:\n",
    "            print('Warning: voxel-based rescale skipped:', _e)\n",
    "    if ref_match.shape != anat_f[0].shape:\n",
    "        if FORCE_REF_RESIZE:\n",
    "            print(f\"[WARN] Ref/anat shape mismatch for {pr['label']}: {ref_match.shape} vs {anat_f[0].shape}; resizing ref\")\n",
    "            ref_match = transform.resize(ref_match, anat_f[0].shape, order=1, mode='reflect', preserve_range=True, anti_aliasing=True).astype(np.float32)\n",
    "        else:\n",
    "            print(f\"[INFO] Using ref shape {ref_match.shape} vs anat {anat_f[0].shape}; matcher will slide template\")\n",
    "    best_z, scores = best_z_by_ncc(ref_match, anat_f, use_cv2=True)\n",
    "    pr['ref_match'] = ref_match\n",
    "    pr['ncc_scores'] = scores\n",
    "    pr['best_z'] = best_z\n",
    "    print(f\"[{pr['label']}] Best Z in anatomy: {best_z}\")\n",
    "    pd.Series(scores).to_csv(OUT_REG / (f\"bestZ_scores_{pr['label']}.csv\"), index=False)\n",
    "\n",
    "# Use first plane for downstream convenience\n",
    "best_z = plane_refs[0]['best_z']\n",
    "ref_match = plane_refs[0]['ref_match']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd554b9",
   "metadata": {},
   "source": [
    "### [17] Plot NCC score curves for all planes\n",
    "Visualize NCC scores versus Z to inspect the chosen plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9d34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [18]\n",
    "# Plot NCC score curves for all planes\n",
    "if plane_refs:\n",
    "    import matplotlib.pyplot as _plt\n",
    "    _plt.figure(figsize=(10,5))\n",
    "    for pr in plane_refs:\n",
    "        scores = pr.get(\"ncc_scores\", None)\n",
    "        if scores is None:\n",
    "            continue\n",
    "        m = re.search(r\"plane(\\d+)\", pr.get(\"label\",\"\"))\n",
    "        plane_no = m.group(1) if m else \"?\"\n",
    "        lbl = f\"{FISH_ID} fish plane {plane_no}\"\n",
    "        line, = _plt.plot(scores, label=lbl)\n",
    "        if \"best_z\" in pr:\n",
    "            _plt.axvline(pr[\"best_z\"], color=line.get_color(), linestyle=\"--\", alpha=0.7)\n",
    "    _plt.xlabel(\"Z\")\n",
    "    _plt.ylabel(\"NCC score\")\n",
    "    _plt.title(\"Best-Z scores per plane\")\n",
    "    _plt.legend()\n",
    "    _plt.show()\n",
    "else:\n",
    "    print(\"No plane_refs available for NCC plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb325dd5",
   "metadata": {},
   "source": [
    "### [19] Estimate in-plane transform (functional → anatomy[best‑Z])\n",
    "Estimate 2D transform from functional reference to the selected anatomy plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-ref-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [20]\n",
    "tforms = []\n",
    "for pr in plane_refs:\n",
    "    bz = pr['best_z']\n",
    "    a_slice = anat_f[bz]\n",
    "    ref_match = pr['ref_match']\n",
    "    if ref_match.shape != a_slice.shape:\n",
    "        print(f\"[WARN] Ref/anat shape mismatch for transform {pr['label']}: {ref_match.shape} vs {a_slice.shape}\")\n",
    "    # Stage 1: Similarity (rigid+scale) via ORB+RANSAC\n",
    "    tform_sim = estimate_inplane_transform(ref_match, a_slice, method='similarity')\n",
    "    print(f\"[{pr['label']}] Similarity tform:\", tform_sim.params)\n",
    "\n",
    "    # Stage 2: Affine via ORB+RANSAC\n",
    "    tform_aff = estimate_inplane_transform(ref_match, a_slice, method='affine')\n",
    "    print(f\"[{pr['label']}] Affine (RANSAC) tform:\", tform_aff.params)\n",
    "\n",
    "    # Stage 3: Refine affine with ECC\n",
    "    tform = refine_affine_ecc(ref_match, a_slice, init_tform=tform_aff, max_iters=300, eps=1e-6, pyr_levels=3)\n",
    "    print(f\"[{pr['label']}] Affine (ECC refined) tform:\", tform.params)\n",
    "\n",
    "    pr['tform'] = tform\n",
    "    tforms.append(tform)\n",
    "    \n",
    "    # Warp the matched (normalized) reference into anatomy space for visual QA\n",
    "    ref_warped_raw = apply_transform_2d(pr['ref_match'], tform, output_shape=a_slice.shape, order=1)\n",
    "    pr['ref_warped_raw'] = ref_warped_raw\n",
    "\n",
    "# Persist per-plane transforms so QA cells can reuse without recomputing\n",
    "try:\n",
    "    tform_records = []\n",
    "    for idx, pr in enumerate(plane_refs):\n",
    "        tf = pr.get('tform', None)\n",
    "        if tf is None:\n",
    "            continue\n",
    "        params = tf.params\n",
    "        tform_records.append({\n",
    "            'plane_index': idx,\n",
    "            'label': pr.get('label', f'plane{idx}'),\n",
    "            'best_z': int(pr.get('best_z', best_z if 'best_z' in globals() else 0)),\n",
    "            'm00': float(params[0,0]), 'm01': float(params[0,1]), 'm02': float(params[0,2]),\n",
    "            'm10': float(params[1,0]), 'm11': float(params[1,1]), 'm12': float(params[1,2]),\n",
    "        })\n",
    "    if tform_records:\n",
    "        pd.DataFrame(tform_records).to_csv(OUT_REG/'tforms_by_plane.csv', index=False)\n",
    "except Exception as _e:\n",
    "    print('Could not save per-plane transforms:', _e)\n",
    "\n",
    "\n",
    "# Keep first-plane artifacts for downstream convenience (keep ref/tform in sync)\n",
    "FIRST_PLANE_LABEL = plane_refs[0].get('label', 'plane0')\n",
    "tform = plane_refs[0]['tform']\n",
    "ref_match = plane_refs[0].get('ref_match', plane_refs[0].get('ref2d_raw', plane_refs[0].get('ref2d')))\n",
    "ref_warped_raw = plane_refs[0]['ref_warped_raw']\n",
    "ref2d_raw = plane_refs[0].get('ref2d_raw', ref2d_raw)\n",
    "ref2d = plane_refs[0].get('ref2d', ref2d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddbd7c2",
   "metadata": {},
   "source": [
    "### [21] Interactive overlay: toggle channels like FIJI\n",
    "Interactive viewer to overlay functional and anatomy images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa4dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [22]\n",
    "# Interactive overlay: toggle channels like FIJI\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "    _HAS_WIDGETS = True\n",
    "except Exception as _e:\n",
    "    _HAS_WIDGETS = False\n",
    "    print('ipywidgets not available; skipping interactive overlay. Install ipywidgets to enable.')\n",
    "\n",
    "# Prepare normalized views (may depend on earlier cells)\n",
    "__overlay_ready = True\n",
    "try:\n",
    "    # Prefer warped functional reference if available; else compute a preview using the current transform; else fall back.\n",
    "    if 'ref_warped' in globals() and ref_warped is not None:\n",
    "        f_src = ref_warped; F_SRC_LABEL = 'warped'\n",
    "    elif 'tform' in globals():\n",
    "        mov_src = ref_match if 'ref_match' in globals() else (ref2d_raw if 'ref2d_raw' in globals() else ref2d)\n",
    "        f_src = apply_transform_2d(mov_src, tform, output_shape=anat[best_z].shape, order=1); F_SRC_LABEL = 'tform-preview'\n",
    "    else:\n",
    "        f_src = ref2d; F_SRC_LABEL = 'raw'\n",
    "    f_vis = norm01(f_src)\n",
    "    a_vis = norm01(anat[best_z])\n",
    "    # Harmonize shapes for overlay — resize functional to anatomy slice shape if needed\n",
    "    if f_vis.shape != a_vis.shape:\n",
    "        f_vis = transform.resize(f_vis, a_vis.shape, order=1, mode='reflect', preserve_range=True, anti_aliasing=True).astype(np.float32)\n",
    "except Exception as _e:\n",
    "    print('Interactive overlay prerequisites missing (ref2d/anat/best_z). Run previous cells first.)')\n",
    "    __overlay_ready = False\n",
    "\n",
    "# Define simple LUTs\n",
    "_COLORS = {\n",
    "    'green':   (0.0, 1.0, 0.0),\n",
    "    'magenta': (1.0, 0.0, 1.0),\n",
    "    'red':     (1.0, 0.0, 0.0),\n",
    "    'blue':    (0.0, 0.0, 1.0),\n",
    "    'cyan':    (0.0, 1.0, 1.0),\n",
    "    'yellow':  (1.0, 1.0, 0.0),\n",
    "    'white':   (1.0, 1.0, 1.0)\n",
    "}\n",
    "\n",
    "def _apply_color(gray01, rgb):\n",
    "    r, g, b = rgb\n",
    "    return np.stack([gray01*r, gray01*g, gray01*b], axis=-1)\n",
    "\n",
    "def _render(show_func=True, show_anat=True, func_color='green', anat_color='magenta', func_alpha=1.0, anat_alpha=1.0):\n",
    "    out = np.zeros((f_vis.shape[0], f_vis.shape[1], 3), dtype=np.float32)\n",
    "    if show_anat:\n",
    "        out += _apply_color(a_vis, _COLORS[anat_color]) * float(anat_alpha)\n",
    "    if show_func:\n",
    "        out += _apply_color(f_vis, _COLORS[func_color]) * float(func_alpha)\n",
    "    out = np.clip(out, 0, 1)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(out)\n",
    "    plt.title(f'Overlay — func[{func_color}] alpha={func_alpha:.2f}, anat[{anat_color}] alpha={anat_alpha:.2f} | func src: {F_SRC_LABEL}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_func_cb = widgets.Checkbox(value=True, description='Show functional')\n",
    "show_anat_cb = widgets.Checkbox(value=True, description='Show anatomy')\n",
    "func_color_dd = widgets.Dropdown(options=list(_COLORS.keys()), value='green', description='Func LUT')\n",
    "anat_color_dd = widgets.Dropdown(options=list(_COLORS.keys()), value='magenta', description='Anat LUT')\n",
    "func_alpha_sl = widgets.FloatSlider(value=1.0, min=0.0, max=1.0, step=0.05, readout_format='.2f', description='Func α')\n",
    "anat_alpha_sl = widgets.FloatSlider(value=1.0, min=0.0, max=1.0, step=0.05, readout_format='.2f', description='Anat α')\n",
    "\n",
    "if _HAS_WIDGETS and __overlay_ready:\n",
    "    ui = widgets.VBox([\n",
    "        widgets.HBox([show_func_cb, func_color_dd, func_alpha_sl]),\n",
    "        widgets.HBox([show_anat_cb, anat_color_dd, anat_alpha_sl])\n",
    "    ])\n",
    "    out = widgets.interactive_output(_render, {\n",
    "        'show_func': show_func_cb,\n",
    "        'show_anat': show_anat_cb,\n",
    "        'func_color': func_color_dd,\n",
    "        'anat_color': anat_color_dd,\n",
    "        'func_alpha': func_alpha_sl,\n",
    "        'anat_alpha': anat_alpha_sl,\n",
    "    })\n",
    "    display(ui, out)\n",
    "elif _HAS_WIDGETS and not __overlay_ready:\n",
    "    print('Interactive overlay not shown: run best-Z cell first.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f67d88b",
   "metadata": {},
   "source": [
    "### [23a] Suite2p masks + traces (preferred over Cellpose)\n",
    "Load Suite2p outputs, build ROI label masks (iscell only), and compute dF/F from raw F.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a032ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [23a]\n",
    "# Suite2p: load outputs, build label masks, compute dF/F (iscell only)\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pathlib\n",
    "\n",
    "USE_SUITE2P_LABELS = True\n",
    "SUITE2P_ROOT = Path(globals().get('SUITE2P_ROOT', OUTDIR / 'suite2P'))\n",
    "SUITE2P_PLANE_GLOB = globals().get('SUITE2P_PLANE_GLOB', 'plane*')\n",
    "SUITE2P_FLIP_X = globals().get('SUITE2P_FLIP_X', None)  # None=auto\n",
    "DFOF_BASELINE_PCT = float(globals().get('DFOF_BASELINE_PCT', 10.0))\n",
    "DFOF_EPS = float(globals().get('DFOF_EPS', 1e-6))\n",
    "\n",
    "\n",
    "def _load_ops_npy(path):\n",
    "    try:\n",
    "        return np.load(path, allow_pickle=True).item()\n",
    "    except NotImplementedError:\n",
    "        _orig_win = pathlib.WindowsPath\n",
    "        _orig_pure = pathlib.PureWindowsPath\n",
    "        pathlib.WindowsPath = pathlib.PosixPath\n",
    "        pathlib.PureWindowsPath = pathlib.PurePosixPath\n",
    "        try:\n",
    "            return np.load(path, allow_pickle=True).item()\n",
    "        finally:\n",
    "            pathlib.WindowsPath = _orig_win\n",
    "            pathlib.PureWindowsPath = _orig_pure\n",
    "\n",
    "\n",
    "def _find_suite2p_file(plane_dir, key):\n",
    "    p = plane_dir / f\"{key}.npy\"\n",
    "    if p.exists():\n",
    "        return p\n",
    "    hits = sorted(plane_dir.glob(f\"*_{key}.npy\"))\n",
    "    if hits:\n",
    "        return hits[0]\n",
    "    return None\n",
    "\n",
    "\n",
    "def _plane_num_from_name(name):\n",
    "    m = re.search(r'plane(\\d+)', str(name))\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "\n",
    "def _ensure_float32(img):\n",
    "    return np.asarray(img, dtype=np.float32)\n",
    "\n",
    "\n",
    "def _resize_like(img, target_shape):\n",
    "    if img is None:\n",
    "        return None\n",
    "    if img.shape == target_shape:\n",
    "        return img\n",
    "    try:\n",
    "        from skimage.transform import resize\n",
    "        return resize(img, target_shape, order=1, preserve_range=True, anti_aliasing=True).astype(np.float32)\n",
    "    except Exception:\n",
    "        return img\n",
    "\n",
    "\n",
    "def _corr2(a, b):\n",
    "    try:\n",
    "        if 'corrcoef_img' in globals():\n",
    "            return float(corrcoef_img(a, b))\n",
    "    except Exception:\n",
    "        pass\n",
    "    a = a.astype(np.float32); b = b.astype(np.float32)\n",
    "    am = a.mean(); bm = b.mean()\n",
    "    num = ((a - am) * (b - bm)).sum()\n",
    "    den = np.sqrt(((a - am) ** 2).sum() * ((b - bm) ** 2).sum()) + 1e-8\n",
    "    return float(num / den)\n",
    "\n",
    "\n",
    "def _build_labels_from_stat(stat, iscell, ops):\n",
    "    Ly = int(ops.get('Ly', 0)); Lx = int(ops.get('Lx', 0))\n",
    "    labels = np.zeros((Ly, Lx), dtype=np.uint32)\n",
    "    keep = np.asarray(iscell)[:, 0].astype(bool)\n",
    "    idxs = np.where(keep)[0]\n",
    "    for roi_idx in idxs:\n",
    "        s = stat[roi_idx]\n",
    "        if isinstance(s, dict):\n",
    "            ypix = np.asarray(s.get('ypix', []), dtype=np.int64)\n",
    "            xpix = np.asarray(s.get('xpix', []), dtype=np.int64)\n",
    "            overlap = s.get('overlap', None)\n",
    "        else:\n",
    "            ypix = np.asarray(s['ypix'], dtype=np.int64)\n",
    "            xpix = np.asarray(s['xpix'], dtype=np.int64)\n",
    "            dtype_names = getattr(getattr(s, 'dtype', None), 'names', None)\n",
    "            overlap = s['overlap'] if dtype_names and 'overlap' in dtype_names else None\n",
    "        if overlap is not None:\n",
    "            ok = ~np.asarray(overlap, dtype=bool)\n",
    "            ypix = ypix[ok]; xpix = xpix[ok]\n",
    "        if ypix.size and xpix.size:\n",
    "            labels[ypix, xpix] = roi_idx + 1\n",
    "    return labels, keep\n",
    "\n",
    "\n",
    "if 'plane_refs' not in globals() or not plane_refs:\n",
    "    raise RuntimeError('plane_refs missing; run the functional reference cell first.')\n",
    "\n",
    "plane_dirs = [p for p in Path(SUITE2P_ROOT).glob(SUITE2P_PLANE_GLOB) if p.is_dir()]\n",
    "plane_dirs = sorted(plane_dirs, key=lambda p: (_plane_num_from_name(p.name) if _plane_num_from_name(p.name) is not None else p.name))\n",
    "\n",
    "plane_ref_map = {}\n",
    "for i, pr in enumerate(plane_refs):\n",
    "    label = str(pr.get('label', ''))\n",
    "    m = re.search(r'plane(\\d+)', label)\n",
    "    if m:\n",
    "        plane_ref_map[int(m.group(1))] = i\n",
    "    elif pr.get('index') is not None:\n",
    "        idx = int(pr.get('index'))\n",
    "        if idx not in plane_ref_map:\n",
    "            plane_ref_map[idx] = i\n",
    "\n",
    "suite2p_planes = []\n",
    "suite2p_by_ref_idx = {}\n",
    "func_labels = [None] * len(plane_refs)\n",
    "\n",
    "if not plane_dirs:\n",
    "    print(f'[Suite2p] No plane dirs found under {SUITE2P_ROOT} (glob={SUITE2P_PLANE_GLOB})')\n",
    "else:\n",
    "    for pd_i, plane_dir in enumerate(plane_dirs):\n",
    "        plane_num = _plane_num_from_name(plane_dir.name)\n",
    "        ref_idx = plane_ref_map.get(plane_num, None)\n",
    "        if ref_idx is None:\n",
    "            ref_idx = pd_i if pd_i < len(plane_refs) else None\n",
    "\n",
    "        paths = {k: _find_suite2p_file(plane_dir, k) for k in ('F', 'Fneu', 'spks', 'stat', 'ops', 'iscell')}\n",
    "        missing = [k for k, v in paths.items() if v is None]\n",
    "        if missing:\n",
    "            print(f'[Suite2p] Missing {missing} in {plane_dir}; skipping')\n",
    "            continue\n",
    "\n",
    "        F = np.load(paths['F'], allow_pickle=True)\n",
    "        Fneu = np.load(paths['Fneu'], allow_pickle=True)\n",
    "        spks = np.load(paths['spks'], allow_pickle=True)\n",
    "        stat = np.load(paths['stat'], allow_pickle=True)\n",
    "        ops = _load_ops_npy(paths['ops'])\n",
    "        iscell = np.load(paths['iscell'], allow_pickle=True)\n",
    "\n",
    "        labels, keep = _build_labels_from_stat(stat, iscell, ops)\n",
    "\n",
    "        flip_x = SUITE2P_FLIP_X\n",
    "        if flip_x is None and ref_idx is not None:\n",
    "            ref_img = plane_refs[ref_idx].get('ref2d_raw', plane_refs[ref_idx].get('ref2d'))\n",
    "            mean_img = ops.get('meanImg', None)\n",
    "            if ref_img is not None and mean_img is not None:\n",
    "                ref_img = _ensure_float32(ref_img)\n",
    "                mean_img = _ensure_float32(mean_img)\n",
    "                mean_img = _resize_like(mean_img, ref_img.shape)\n",
    "                try:\n",
    "                    c0 = _corr2(mean_img, ref_img)\n",
    "                    c1 = _corr2(mean_img[:, ::-1], ref_img)\n",
    "                    flip_x = c1 > (c0 + 1e-3)\n",
    "                except Exception:\n",
    "                    flip_x = False\n",
    "            else:\n",
    "                flip_x = False\n",
    "        if flip_x:\n",
    "            labels = labels[:, ::-1]\n",
    "\n",
    "        F_raw = np.asarray(F, dtype=np.float32)\n",
    "        F0 = np.percentile(F_raw, DFOF_BASELINE_PCT, axis=1, keepdims=True)\n",
    "        dff = (F_raw - F0) / (F0 + DFOF_EPS)\n",
    "\n",
    "        plane_info = {\n",
    "            'plane_dir': plane_dir,\n",
    "            'plane_num': plane_num,\n",
    "            'ref_idx': ref_idx,\n",
    "            'labels': labels,\n",
    "            'iscell_keep': keep,\n",
    "            'F': F,\n",
    "            'Fneu': Fneu,\n",
    "            'spks': spks,\n",
    "            'stat': stat,\n",
    "            'ops': ops,\n",
    "            'iscell': iscell,\n",
    "            'dff': dff,\n",
    "            'flip_x': bool(flip_x),\n",
    "        }\n",
    "        suite2p_planes.append(plane_info)\n",
    "\n",
    "        if ref_idx is not None:\n",
    "            func_labels[ref_idx] = labels\n",
    "            suite2p_by_ref_idx[ref_idx] = plane_info\n",
    "            plane_refs[ref_idx]['suite2p'] = plane_info\n",
    "\n",
    "        n_cells = int(keep.sum())\n",
    "        print(f\"[Suite2p] {plane_dir.name}: rois={len(keep)} cells={n_cells} ref_idx={ref_idx} flip_x={plane_info['flip_x']}\")\n",
    "\n",
    "globals()['suite2p_planes'] = suite2p_planes\n",
    "globals()['suite2p_by_ref_idx'] = suite2p_by_ref_idx\n",
    "if USE_SUITE2P_LABELS:\n",
    "    globals()['func_labels'] = func_labels\n",
    "    print(f\"[Suite2p] Loaded labels for {sum(m is not None for m in func_labels)} plane(s) into func_labels.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816bd380",
   "metadata": {},
   "source": [
    "### [23] Cellpose segmentation on functional reference (8-bit input)\n",
    "Run Cellpose segmentation on the functional reference image.\n",
    "\n",
    "If you are using Suite2p labels, skip this section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de683458",
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool(globals().get('USE_SUITE2P_LABELS', False)):\n",
    "    print('[Cellpose] USE_SUITE2P_LABELS is True; skipping Cellpose segmentation.')\n",
    "else:\n",
    "    # [24]\n",
    "\n",
    "    # Cellpose segmentation on functional reference (8-bit input)\n",
    "    # Segments all functional planes unless skipped.\n",
    "    CP_SAVE_PNG = False\n",
    "    CP_SAVE_TIF = True\n",
    "    CP_SKIP_IF_EXISTS = True\n",
    "    CP_PLANE_INDICES = None  # set to list of indices to subset, or None for all\n",
    "\n",
    "    from cellpose import models, io\n",
    "    io.logger_setup()\n",
    "    MODEL_PATH = '/Volumes/jlarsch/default/D2c/07_Data/Danin/Cellpose/models/2pf_cpsam_20250915_115140'\n",
    "    model = models.CellposeModel(gpu=True, pretrained_model=MODEL_PATH)\n",
    "\n",
    "    if not plane_refs:\n",
    "        raise RuntimeError('No functional planes available for Cellpose')\n",
    "\n",
    "    plane_indices = CP_PLANE_INDICES if CP_PLANE_INDICES is not None else range(len(plane_refs))\n",
    "    for pi in plane_indices:\n",
    "        pi_int = int(pi)\n",
    "        if pi_int < 0 or pi_int >= len(plane_refs):\n",
    "            print(f\"[SKIP] plane index {pi_int} out of range\")\n",
    "            continue\n",
    "        pr = plane_refs[pi_int]\n",
    "        label = pr.get('label', f'plane{pi_int}')\n",
    "        mask_path = OUT_SEG / f\"{label}_cellpose_masks.tif\"\n",
    "        if CP_SKIP_IF_EXISTS and mask_path.exists():\n",
    "            print(f\"[SKIP] existing masks: {mask_path}\")\n",
    "            continue\n",
    "        ref_for_cp = pr.get('ref2d_raw') if pr.get('ref2d_raw') is not None else pr.get('ref_match') or pr.get('ref2d')\n",
    "        if ref_for_cp is None:\n",
    "            print(f\"[SKIP] no reference image for {label}\")\n",
    "            continue\n",
    "        cp_img = (norm01(ref_for_cp) * 255).astype(np.uint8)\n",
    "        cp_input_path = OUT_SEG / f\"{label}_cellpose_input.tif\"\n",
    "        imwrite(cp_input_path, cp_img)\n",
    "        result = model.eval([cp_img], channels=[0,0], channel_axis=None, normalize=True)\n",
    "        try:\n",
    "            masks, flows, styles, diams = result\n",
    "        except ValueError:\n",
    "            masks, flows, styles = result\n",
    "            diams = None\n",
    "        mask_arr = masks[0].astype(np.uint16)\n",
    "        if CP_SAVE_TIF:\n",
    "            imwrite(mask_path, mask_arr)\n",
    "        if CP_SAVE_PNG:\n",
    "            try:\n",
    "                from PIL import Image\n",
    "                Image.fromarray(mask_arr.astype('uint16')).save(mask_path.with_suffix('.png'))\n",
    "            except Exception as e:\n",
    "                print('[WARN] PNG save failed:', e)\n",
    "        print(f\"[CP] {label}: input={cp_input_path}, masks={mask_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8abad5",
   "metadata": {},
   "source": [
    "### [25] Functional labels on references\n",
    "Plot functional label masks over each functional reference to verify the mapping before downstream QC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee037e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [26]\n",
    "# Plot functional labels on top of each functional reference (per-plane)\n",
    "from skimage import color as _color\n",
    "\n",
    "def _get_labels_for_plane(pr, p_idx):\n",
    "    lbl = pr.get('label', f'plane{p_idx}')\n",
    "    use_s2p = bool(globals().get('USE_SUITE2P_LABELS', False))\n",
    "    if use_s2p:\n",
    "        fl = globals().get('func_labels', None)\n",
    "        if fl is not None:\n",
    "            if isinstance(fl, list):\n",
    "                if p_idx < len(fl):\n",
    "                    arr_i = fl[p_idx]\n",
    "                    if arr_i is not None:\n",
    "                        arr_i = _ensure_uint_labels(arr_i)\n",
    "                        return arr_i, f\"Suite2p labels list[{p_idx}]\"\n",
    "            else:\n",
    "                arr_np = _ensure_uint_labels(np.asarray(fl))\n",
    "                if arr_np.ndim == 3:\n",
    "                    src_idx = pr.get('index', p_idx)\n",
    "                    if src_idx < arr_np.shape[0]:\n",
    "                        return arr_np[src_idx], f\"Suite2p labels stack[{src_idx}]\"\n",
    "                elif arr_np.ndim == 2:\n",
    "                    return arr_np, \"Suite2p labels (2D)\"\n",
    "    cp_path = OUT_SEG / f\"{lbl}_cellpose_masks.tif\"\n",
    "    if cp_path.exists():\n",
    "        arr = _ensure_uint_labels(imread(cp_path))\n",
    "        if arr.ndim == 3 and arr.shape[-1] in (3, 4):\n",
    "            arr = arr[..., 0]\n",
    "        if arr.ndim == 3 and arr.shape[0] == 1:\n",
    "            arr = arr[0]\n",
    "        return arr, f\"Cellpose masks: {cp_path}\"\n",
    "    fl = globals().get('func_labels', None)\n",
    "    if fl is not None:\n",
    "        arr = fl\n",
    "        if isinstance(arr, list):\n",
    "            if p_idx < len(arr):\n",
    "                arr_i = _ensure_uint_labels(arr[p_idx])\n",
    "                return arr_i, f\"func_labels list[{p_idx}]\"\n",
    "        else:\n",
    "            arr_np = _ensure_uint_labels(np.asarray(arr))\n",
    "            if arr_np.ndim == 3:\n",
    "                src_idx = pr.get('index', p_idx)\n",
    "                if src_idx < arr_np.shape[0]:\n",
    "                    return arr_np[src_idx], f\"func_labels stack[{src_idx}]\"\n",
    "            elif arr_np.ndim == 2:\n",
    "                return arr_np, \"func_labels (2D)\"\n",
    "    if FUNC_LABELS_PATH and os.path.exists(FUNC_LABELS_PATH):\n",
    "        arr = _ensure_uint_labels(imread(FUNC_LABELS_PATH))\n",
    "        if arr.ndim == 3:\n",
    "            src_idx = pr.get('index', p_idx)\n",
    "            if src_idx < arr.shape[0]:\n",
    "                return arr[src_idx], f\"FUNC_LABELS_PATH[{src_idx}]: {Path(FUNC_LABELS_PATH).name}\"\n",
    "        elif arr.ndim == 2:\n",
    "            return arr, f\"FUNC_LABELS_PATH: {Path(FUNC_LABELS_PATH).name}\"\n",
    "    return None, None\n",
    "\n",
    "if 'plane_refs' not in globals() or not plane_refs:\n",
    "    print('No functional references to plot.')\n",
    "else:\n",
    "    import matplotlib.pyplot as _plt\n",
    "    for p_idx, pr in enumerate(plane_refs):\n",
    "        lbl = pr.get('label', f'plane{p_idx}')\n",
    "        labels, src_desc = _get_labels_for_plane(pr, p_idx)\n",
    "        if labels is None:\n",
    "            print(f\"[SKIP] No labels found for {lbl}\")\n",
    "            continue\n",
    "        if labels.ndim == 3 and labels.shape[-1] in (3, 4):\n",
    "            labels = labels[..., 0]\n",
    "        if labels.ndim != 2:\n",
    "            print(f\"[SKIP] Labels for {lbl} have unsupported shape {labels.shape}\")\n",
    "            continue\n",
    "        ref_img = pr.get('ref2d_raw', None)\n",
    "        if ref_img is None:\n",
    "            ref_img = pr.get('ref2d', None)\n",
    "        if ref_img is None:\n",
    "            print(f\"[SKIP] No functional reference for {lbl}\")\n",
    "            continue\n",
    "        if ref_img.ndim == 3 and ref_img.shape[-1] in (3, 4):\n",
    "            ref_img = ref_img[..., 0]\n",
    "        if labels.shape != ref_img.shape:\n",
    "            print(f\"[SKIP] Label/ref shape mismatch for {lbl}: labels {labels.shape}, ref {ref_img.shape}\")\n",
    "            continue\n",
    "        ref_vis = norm01(ref_img)\n",
    "        overlay = _color.label2rgb(labels, image=ref_vis, bg_label=0, alpha=0.35, image_alpha=1.0)\n",
    "        _plt.figure(figsize=(6, 6))\n",
    "        _plt.imshow(overlay)\n",
    "        title_src = src_desc if src_desc else 'labels'\n",
    "        _plt.title(f\"Functional labels on reference — {lbl}\\n{title_src}\")\n",
    "        _plt.axis('off')\n",
    "        _plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mask_diameters_hdr",
   "metadata": {},
   "source": [
    "### [27] Mask diameters by axis (µm)\n",
    "Measure per-axis diameters of segmentation masks in microns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mask_diameters_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [28]\n",
    "\n",
    "# Mask diameters (µm) for anatomy, functional, HCR (if available)\n",
    "import numpy as _np, matplotlib.pyplot as _plt\n",
    "\n",
    "def _load_labels_or_none(path):\n",
    "    if path is None:\n",
    "        return None\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            return None\n",
    "        arr = _ensure_uint_labels(imread(path))\n",
    "        if arr.ndim == 3 and arr.shape[-1] in (3,4):\n",
    "            arr = arr[...,0]\n",
    "        return arr\n",
    "    except Exception as _e:\n",
    "        print('Could not load labels from', path, ':', _e)\n",
    "        return None\n",
    "\n",
    "dfs = []\n",
    "anat_labels = _load_labels_or_none(ANAT_LABELS_PATH)\n",
    "if anat_labels is not None:\n",
    "    anat_3d = anat_labels if anat_labels.ndim == 3 else anat_labels[None, ...]\n",
    "    vox_a = {\n",
    "        'Z': float(VOX_ANAT.get('Z',1.0)) if 'VOX_ANAT' in globals() and VOX_ANAT else 1.0,\n",
    "        'Y': float(VOX_ANAT.get('Y',1.0)) if 'VOX_ANAT' in globals() and VOX_ANAT else 1.0,\n",
    "        'X': float(VOX_ANAT.get('X',1.0)) if 'VOX_ANAT' in globals() and VOX_ANAT else 1.0,\n",
    "    }\n",
    "    df_anat_diam = diameters_um_from_array(anat_3d, vox_a)\n",
    "    df_anat_diam['dataset'] = 'Anatomy'\n",
    "    dfs.append(df_anat_diam)\n",
    "\n",
    "# Optional: 2P anatomy masks (Cellpose) fallback: if ANAT_LABELS_PATH missing, try structural/cp_masks\n",
    "if anat_labels is None:\n",
    "    anat2p_path = Path('/Volumes/jlarsch/default/D2c/07_Data/Danin/L395_f11/03_analysis/structural/cp_masks/L395_f11_anatomy_00001_8bit_cp_masks.tif')\n",
    "    anat2p_labels = _load_labels_or_none(anat2p_path)\n",
    "    if anat2p_labels is not None:\n",
    "        anat2p_3d = anat2p_labels if anat2p_labels.ndim == 3 else anat2p_labels[None, ...]\n",
    "        vox_a2p = {\n",
    "            'Z': float(VOX_ANAT.get('Z',1.0)) if 'VOX_ANAT' in globals() and VOX_ANAT else 1.0,\n",
    "            'Y': float(VOX_ANAT.get('Y',1.0)) if 'VOX_ANAT' in globals() and VOX_ANAT else 1.0,\n",
    "            'X': float(VOX_ANAT.get('X',1.0)) if 'VOX_ANAT' in globals() and VOX_ANAT else 1.0,\n",
    "        }\n",
    "        df_anat2p_diam = diameters_um_from_array(anat2p_3d, vox_a2p)\n",
    "        df_anat2p_diam['dataset'] = 'Anatomy2P'\n",
    "        dfs.append(df_anat2p_diam)\n",
    "\n",
    "\n",
    "func_labels = globals().get(\"func_labels\", None)\n",
    "if isinstance(func_labels, list) and not any(m is not None for m in func_labels):\n",
    "    func_labels = None\n",
    "_cp_masks = []\n",
    "if func_labels is None:\n",
    "    try:\n",
    "        for pr in plane_refs:\n",
    "            lbl = pr.get(\"label\", None)\n",
    "            if lbl is None:\n",
    "                continue\n",
    "            mp = OUT_SEG / f\"{lbl}_cellpose_masks.tif\"\n",
    "            if not mp.exists():\n",
    "                continue\n",
    "            m = imread(mp)\n",
    "            m = _np.asarray(m)\n",
    "            if m.ndim == 3 and m.shape[-1] in (3,4):\n",
    "                m = m[...,0]\n",
    "            m = _ensure_uint_labels(m)\n",
    "            _cp_masks.append(m)\n",
    "        if _cp_masks:\n",
    "            func_labels = _cp_masks  # list of per-plane masks with unique labels\n",
    "    except Exception as _e:\n",
    "        print(\"Warning: loading cellpose masks failed:\", _e)\n",
    "if func_labels is None:\n",
    "    func_labels = _load_labels_or_none(FUNC_LABELS_PATH)\n",
    "if func_labels is not None:\n",
    "    vox_f = {\n",
    "        'Z': 1.0,\n",
    "        'Y': float(VOX_FUNC.get('Y',1.0)) if 'VOX_FUNC' in globals() and VOX_FUNC else 1.0,\n",
    "        'X': float(VOX_FUNC.get('X',1.0)) if 'VOX_FUNC' in globals() and VOX_FUNC else 1.0,\n",
    "    }\n",
    "    func_dfs = []\n",
    "    if isinstance(func_labels, list):\n",
    "        for i, m in enumerate(func_labels):\n",
    "            func_3d = m if m.ndim == 3 else m[None, ...]\n",
    "            df_i = diameters_um_from_array(func_3d, vox_f)\n",
    "            df_i['plane_idx'] = i\n",
    "            func_dfs.append(df_i)\n",
    "        df_func_diam = pd.concat(func_dfs, ignore_index=True) if func_dfs else None\n",
    "    else:\n",
    "        func_3d = func_labels if func_labels.ndim == 3 else func_labels[None, ...]\n",
    "        df_func_diam = diameters_um_from_array(func_3d, vox_f)\n",
    "    if df_func_diam is not None:\n",
    "        df_func_diam['dataset'] = 'Functional'\n",
    "        dfs.append(df_func_diam)\n",
    "\n",
    "HCR_LABELS_PATHS = globals().get('HCR_LABELS_PATHS', []) if 'HCR_LABELS_PATHS' in globals() else []\n",
    "if (not HCR_LABELS_PATHS) and 'HCR_LABELS_PATH' in globals() and HCR_LABELS_PATH:\n",
    "    HCR_LABELS_PATHS = [HCR_LABELS_PATH]\n",
    "\n",
    "hcr_dfs = []\n",
    "for hp in HCR_LABELS_PATHS:\n",
    "    hl = _load_labels_or_none(hp)\n",
    "    if hl is None:\n",
    "        continue\n",
    "    hcr_3d = hl if hl.ndim == 3 else hl[None, ...]\n",
    "    vox_h = {\n",
    "        'Z': float(VOX_HCR.get('Z',1.0)) if 'VOX_HCR' in globals() and VOX_HCR else 1.0,\n",
    "        'Y': float(VOX_HCR.get('Y',1.0)) if 'VOX_HCR' in globals() and VOX_HCR else 1.0,\n",
    "        'X': float(VOX_HCR.get('X',1.0)) if 'VOX_HCR' in globals() and VOX_HCR else 1.0,\n",
    "    }\n",
    "    df_h = diameters_um_from_array(hcr_3d, vox_h)\n",
    "    df_h['dataset'] = 'HCR'\n",
    "    df_h['gene'] = gene_from_mask(hp)\n",
    "    df_h['file'] = Path(hp).name\n",
    "    hcr_dfs.append(df_h)\n",
    "if hcr_dfs:\n",
    "    dfs.append(pd.concat(hcr_dfs, ignore_index=True))\n",
    "\n",
    "if not dfs:\n",
    "    print('No label volumes available for diameter analysis.')\n",
    "else:\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "    try:\n",
    "        df_all.to_pickle(OUT_QA / 'diameters_df_all.pkl')\n",
    "    except Exception:\n",
    "        pass\n",
    "    cats_plot = ['Anatomy','Functional','HCR']\n",
    "    present = [c for c in cats_plot if c in df_all['dataset'].unique()]\n",
    "\n",
    "    def _series_for_axis(axis_col):\n",
    "        ser = []\n",
    "        for ds in present:\n",
    "            vals = df_all.loc[df_all['dataset']==ds, axis_col].to_numpy(dtype=float)\n",
    "            ser.append(vals)\n",
    "        return ser\n",
    "\n",
    "    series_x = _series_for_axis('x_um')\n",
    "    series_y = _series_for_axis('y_um')\n",
    "    series_z = _series_for_axis('z_um')\n",
    "\n",
    "    all_vals = _np.concatenate([a for a in (series_x + series_y + series_z) if a.size]) if any(\n",
    "        (a.size for a in (series_x + series_y + series_z))) else _np.array([])\n",
    "    y_max_data = float(_np.max(all_vals)) if all_vals.size else 10.0\n",
    "    from math import ceil as _ceil\n",
    "    y_max = max(10.0, 10.0 * _ceil(y_max_data / 10.0))\n",
    "    yticks = _np.arange(0.0, y_max + 0.1, 10.0)\n",
    "\n",
    "    color_map = {'Anatomy':'#bbbbbb', 'Functional':'#88ccee', 'HCR':'#cc88ff'}\n",
    "    colors = [color_map.get(ds, '#cccccc') for ds in present]\n",
    "\n",
    "    fig, axes = _plt.subplots(1, 3, figsize=(20, 4.5), sharey=True)\n",
    "    if not hasattr(axes, '__len__'):\n",
    "        axes = [axes]\n",
    "    axis_cols = ['x_um', 'y_um', 'z_um']\n",
    "    for ax, ser, title, axis_col in zip(axes, [series_x, series_y, series_z], ['X diameter (µm)', 'Y diameter (µm)', 'Z diameter (µm)'], axis_cols):\n",
    "        if not any(a.size for a in ser):\n",
    "            ax.set_visible(False)\n",
    "            continue\n",
    "        parts = ax.violinplot(ser, showmeans=False, showmedians=False, showextrema=False)\n",
    "        for i, pc in enumerate(parts['bodies']):\n",
    "            pc.set_facecolor(colors[i])\n",
    "            pc.set_edgecolor('black')\n",
    "            pc.set_alpha(0.7)\n",
    "        x_offset = 0.18\n",
    "        y_offset = 0.02 * y_max\n",
    "        bbox_style = dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1.0)\n",
    "        for i, vals in enumerate(ser, start=1):\n",
    "            if vals.size:\n",
    "                med = float(_np.median(vals))\n",
    "                ax.scatter([i], [med], color='crimson', zorder=3, s=26)\n",
    "                label_txt = 'median={:.2f} µm\\nn={:d}'.format(med, vals.size)\n",
    "                ax.text(i + x_offset, med + y_offset, label_txt,\n",
    "                        va='bottom', ha='left', fontsize=8, bbox=bbox_style, clip_on=False, zorder=4)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xticks(range(1, len(present)+1))\n",
    "        ax.set_xticklabels(present, rotation=0)\n",
    "        ax.set_ylim(0, y_max)\n",
    "        ax.set_yticks(yticks)\n",
    "        ax.grid(axis='y', alpha=0.2)\n",
    "    fig.suptitle('Mask diameters by axis — Anatomy, Functional, HCR')\n",
    "    _plt.tight_layout()\n",
    "    _plt.show()\n",
    "\n",
    "    # HCR per-file diameters (one violin per file, pooled by axis)\n",
    "    if 'HCR' in present and 'file' in df_all.columns and df_all.loc[df_all['dataset']=='HCR', 'file'].notna().any():\n",
    "        hcr_sub = df_all[df_all['dataset']=='HCR'].copy()\n",
    "        hcr_files = list(hcr_sub['file'].dropna().unique())\n",
    "        hcr_genes = [gene_from_mask(fn) for fn in hcr_files]\n",
    "        if hcr_files:\n",
    "            fig2, axes2 = _plt.subplots(1, 3, figsize=(20, 4.5), sharey=True)\n",
    "            if not hasattr(axes2, '__len__'):\n",
    "                axes2 = [axes2]\n",
    "            for ax, axis_col in zip(axes2, axis_cols):\n",
    "                ser = [hcr_sub.loc[hcr_sub['file']==fn, axis_col].to_numpy(dtype=float) for fn in hcr_files]\n",
    "                if not any(a.size for a in ser):\n",
    "                    ax.set_visible(False)\n",
    "                    continue\n",
    "                parts = ax.violinplot(ser, showmeans=False, showmedians=False, showextrema=False)\n",
    "                for pc in parts['bodies']:\n",
    "                    pc.set_facecolor('#cc88ff'); pc.set_edgecolor('black'); pc.set_alpha(0.7)\n",
    "                x_offset = 0.12; y_offset = 0.02 * y_max\n",
    "                for i, vals in enumerate(ser, start=1):\n",
    "                    if vals.size:\n",
    "                        med = float(_np.median(vals))\n",
    "                        ax.scatter([i], [med], color='crimson', zorder=3, s=24)\n",
    "                        ax.text(i + x_offset, med + y_offset, f\"median={med:.2f} µm\\nn={vals.size}\",\n",
    "                                va='bottom', ha='left', fontsize=7.5, bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1.0), clip_on=False, zorder=4)\n",
    "                ax.set_title(f\"HCR {axis_col.split('_')[0].upper()} per file\")\n",
    "                ax.set_xticks(range(1, len(hcr_files)+1))\n",
    "                ax.set_xticklabels(hcr_genes, rotation=30, ha='right')\n",
    "                ax.set_ylim(0, y_max)\n",
    "                ax.set_yticks(yticks)\n",
    "                ax.grid(axis='y', alpha=0.2)\n",
    "            _plt.tight_layout()\n",
    "            _plt.show()\n",
    "\n",
    "    axis_name_map = {'x_um':'X', 'y_um':'Y', 'z_um':'Z'}\n",
    "    outlier_rows = []\n",
    "    for ds in present:\n",
    "        sub = df_all[df_all['dataset']==ds]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        for axis_col in axis_cols:\n",
    "            vals = sub[axis_col].to_numpy(dtype=float)\n",
    "            if not len(vals):\n",
    "                continue\n",
    "            try:\n",
    "                q1, q3 = _np.percentile(vals, [25, 75])\n",
    "                iqr = q3 - q1\n",
    "                lo = q1 - 1.5 * iqr\n",
    "                hi = q3 + 1.5 * iqr\n",
    "            except Exception:\n",
    "                lo, hi = -_np.inf, _np.inf\n",
    "            outs = sub[(sub[axis_col] < lo) | (sub[axis_col] > hi)]\n",
    "            if outs.empty:\n",
    "                continue\n",
    "            for _, row in outs.iterrows():\n",
    "                entry = {'dataset': ds, 'axis': axis_name_map.get(axis_col, axis_col), 'diameter_um': float(row[axis_col])}\n",
    "                if 'label' in row and not pd.isna(row['label']):\n",
    "                    entry['label'] = int(row['label'])\n",
    "                if ds == 'Functional' and 'plane_idx' in row and not pd.isna(row['plane_idx']):\n",
    "                    entry['plane_idx'] = int(row['plane_idx'])\n",
    "                if 'file' in row and not pd.isna(row['file']):\n",
    "                    entry['file'] = row['file']\n",
    "                outlier_rows.append(entry)\n",
    "    if outlier_rows:\n",
    "        out_df = pd.DataFrame(outlier_rows).sort_values(['dataset','axis','diameter_um'], ascending=[True, True, False]).reset_index(drop=True)\n",
    "        print('Outlier diameters (IQR fence, pooled), sorted high→low (per-plane/per-file labels):')\n",
    "        try:\n",
    "            from IPython.display import HTML, display  # type: ignore\n",
    "            html = out_df.to_html(index=False)\n",
    "            display(HTML(f\"<div style='max-height:320px; overflow-y:auto'>{html}</div>\"))\n",
    "        except Exception:\n",
    "            print(out_df.to_string(index=False))\n",
    "    else:\n",
    "        print('No diameter outliers found.')\n",
    "\n",
    "\n",
    "    rows = []\n",
    "    for ds in present:\n",
    "        sub = df_all[df_all['dataset']==ds]\n",
    "        if sub.empty:\n",
    "            rows.append({'dataset': ds, 'x_median': _np.nan, 'y_median': _np.nan, 'z_median': _np.nan, 'n': 0})\n",
    "        else:\n",
    "            rows.append({\n",
    "                'dataset': ds,\n",
    "                'x_median': float(_np.median(sub['x_um'])) if len(sub['x_um']) else _np.nan,\n",
    "                'y_median': float(_np.median(sub['y_um'])) if len(sub['y_um']) else _np.nan,\n",
    "                'z_median': float(_np.median(sub['z_um'])) if len(sub['z_um']) else _np.nan,\n",
    "                'n': int(len(sub)),\n",
    "            })\n",
    "    med_table = pd.DataFrame(rows)\n",
    "    try:\n",
    "        display(med_table)\n",
    "    except Exception:\n",
    "        print(med_table.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee0a2b4",
   "metadata": {},
   "source": [
    "### [29] Noise-filtered diameters (HCR X/Y ≥ 3 µm)\n",
    "Filter mask diameter measurements to reduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67edbd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [30]\n",
    "# Noise-filtered diameters: drop HCR labels with X or Y diameter < 3 µm\n",
    "import numpy as _np, matplotlib.pyplot as _plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "MIN_XY_HCR_UM = 3.0\n",
    "\n",
    "# Try to reuse df_all from previous cell or cache\n",
    "df_all = globals().get('df_all', None)\n",
    "if df_all is None or getattr(df_all, 'empty', True):\n",
    "    _diam_paths = []\n",
    "    if 'OUT_QA' in globals():\n",
    "        _diam_paths.append(OUT_QA / 'diameters_df_all.pkl')\n",
    "    if 'OUTDIR' in globals():\n",
    "        _diam_paths.append(OUTDIR / 'diameters_df_all.pkl')\n",
    "    for _p in _diam_paths:\n",
    "        if os.path.exists(_p):\n",
    "            try:\n",
    "                df_all = pd.read_pickle(_p)\n",
    "                print(f\"[Info] Loaded df_all from cache pickle: {_p}\")\n",
    "                break\n",
    "            except Exception:\n",
    "                df_all = None\n",
    "\n",
    "if df_all is None or getattr(df_all, 'empty', True):\n",
    "    print('No label volumes available for noise-filtered diameter analysis. Run the previous diameters cell first.')\n",
    "else:\n",
    "    # Filter HCR by X/Y diameter >= threshold\n",
    "    df_f = df_all.copy()\n",
    "    if 'dataset' in df_f.columns:\n",
    "        hcr_mask = df_f['dataset'].eq('HCR') if 'HCR' in df_f['dataset'].unique() else None\n",
    "        if hcr_mask is not None:\n",
    "            drop_mask = hcr_mask & ((df_f['x_um'] < MIN_XY_HCR_UM) | (df_f['y_um'] < MIN_XY_HCR_UM))\n",
    "            if drop_mask.any():\n",
    "                print(f\"[Filter] Dropping {int(drop_mask.sum())} HCR labels with X or Y < {MIN_XY_HCR_UM} µm\")\n",
    "                df_f = df_f.loc[~drop_mask].reset_index(drop=True)\n",
    "    df_all_filt = df_f\n",
    "    cats_plot = ['Anatomy','Functional','HCR']\n",
    "    present = [c for c in cats_plot if c in df_all_filt['dataset'].unique()]\n",
    "\n",
    "    def _series_for_axis(axis_col, df):\n",
    "        ser = []\n",
    "        for ds in present:\n",
    "            vals = df.loc[df['dataset']==ds, axis_col].to_numpy(dtype=float)\n",
    "            ser.append(vals)\n",
    "        return ser\n",
    "\n",
    "    series_x = _series_for_axis('x_um', df_all_filt)\n",
    "    series_y = _series_for_axis('y_um', df_all_filt)\n",
    "    series_z = _series_for_axis('z_um', df_all_filt)\n",
    "\n",
    "    all_vals = _np.concatenate([a for a in (series_x + series_y + series_z) if a.size]) if any((a.size for a in (series_x + series_y + series_z))) else _np.array([])\n",
    "    y_max_data = float(_np.max(all_vals)) if all_vals.size else 10.0\n",
    "    from math import ceil as _ceil\n",
    "    y_max = max(10.0, 10.0 * _ceil(y_max_data / 10.0))\n",
    "    yticks = _np.arange(0.0, y_max + 0.1, 10.0)\n",
    "\n",
    "    color_map = {'Anatomy':'#bbbbbb', 'Functional':'#88ccee', 'HCR':'#cc88ff'}\n",
    "    colors = [color_map.get(ds, '#cccccc') for ds in present]\n",
    "\n",
    "    fig, axes = _plt.subplots(1, 3, figsize=(20, 4.5), sharey=True)\n",
    "    if not hasattr(axes, '__len__'):\n",
    "        axes = [axes]\n",
    "    axis_cols = ['x_um', 'y_um', 'z_um']\n",
    "    for ax, ser, title, axis_col in zip(axes, [series_x, series_y, series_z], ['X diameter (µm)', 'Y diameter (µm)', 'Z diameter (µm)'], axis_cols):\n",
    "        if not any(a.size for a in ser):\n",
    "            ax.set_visible(False)\n",
    "            continue\n",
    "        parts = ax.violinplot(ser, showmeans=False, showmedians=False, showextrema=False)\n",
    "        for i, pc in enumerate(parts['bodies']):\n",
    "            pc.set_facecolor(colors[i])\n",
    "            pc.set_edgecolor('black')\n",
    "            pc.set_alpha(0.7)\n",
    "        x_offset = 0.18\n",
    "        y_offset = 0.02 * y_max\n",
    "        bbox_style = dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1.0)\n",
    "        for i, vals in enumerate(ser, start=1):\n",
    "            if vals.size:\n",
    "                med = float(_np.median(vals))\n",
    "                sd = float(_np.std(vals))\n",
    "                ax.scatter([i], [med], color='crimson', zorder=3, s=26)\n",
    "                label_txt = \"median={:.2f} µm\\nsd={:.2f} µm\\nn={:d}\".format(med, sd, vals.size)\n",
    "                ax.text(i + x_offset, med + y_offset, label_txt,\n",
    "                        va='bottom', ha='left', fontsize=8, bbox=bbox_style, clip_on=False, zorder=4)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xticks(range(1, len(present)+1))\n",
    "        ax.set_xticklabels(present, rotation=0)\n",
    "        ax.set_ylim(0, y_max)\n",
    "        ax.set_yticks(yticks)\n",
    "        ax.grid(axis='y', alpha=0.2)\n",
    "    fig.suptitle(f\"Mask diameters by axis (HCR filtered: X/Y ≥ {MIN_XY_HCR_UM} µm)\")\n",
    "    _plt.tight_layout()\n",
    "    _plt.show()\n",
    "\n",
    "    # HCR per-file diameters (filtered)\n",
    "    if 'HCR' in present and 'file' in df_all_filt.columns and df_all_filt.loc[df_all_filt['dataset']=='HCR', 'file'].notna().any():\n",
    "        hcr_sub = df_all_filt[df_all_filt['dataset']=='HCR'].copy()\n",
    "        if 'gene' not in hcr_sub.columns:\n",
    "            hcr_sub['gene'] = hcr_sub['file'].apply(gene_from_mask)\n",
    "        files_genes = hcr_sub[['file','gene']].dropna().drop_duplicates()\n",
    "        hcr_files = files_genes['file'].tolist()\n",
    "        hcr_genes = files_genes['gene'].tolist()\n",
    "        if hcr_files:\n",
    "            fig2, axes2 = _plt.subplots(1, 3, figsize=(20, 4.5), sharey=True)\n",
    "            if not hasattr(axes2, '__len__'):\n",
    "                axes2 = [axes2]\n",
    "            for ax, axis_col in zip(axes2, axis_cols):\n",
    "                ser = [hcr_sub.loc[hcr_sub['file']==fn, axis_col].to_numpy(dtype=float) for fn in hcr_files]\n",
    "                if not any(a.size for a in ser):\n",
    "                    ax.set_visible(False)\n",
    "                    continue\n",
    "                parts = ax.violinplot(ser, showmeans=False, showmedians=False, showextrema=False)\n",
    "                for pc in parts['bodies']:\n",
    "                    pc.set_facecolor('#cc88ff')\n",
    "                    pc.set_edgecolor('black')\n",
    "                    pc.set_alpha(0.7)\n",
    "                x_offset = 0.12\n",
    "                y_offset = 0.02 * y_max\n",
    "                for i, vals in enumerate(ser, start=1):\n",
    "                    if vals.size:\n",
    "                        med = float(_np.median(vals))\n",
    "                        sd = float(_np.std(vals))\n",
    "                        ax.scatter([i], [med], color='crimson', zorder=3, s=24)\n",
    "                        ax.text(\n",
    "                            i + x_offset, med + y_offset,\n",
    "                            f\"median={med:.2f} µm\\nsd={sd:.2f} µm\\nn={vals.size}\",\n",
    "                            va='bottom', ha='left', fontsize=7.5,\n",
    "                            bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1.0),\n",
    "                            clip_on=False, zorder=4\n",
    "                        )\n",
    "                ax.set_title(f\"HCR {axis_col.split('_')[0].upper()} per file (filtered)\")\n",
    "                ax.set_xticks(range(1, len(hcr_files)+1))\n",
    "                ax.set_xticklabels(hcr_genes, rotation=30, ha='right')\n",
    "                ax.set_ylim(0, y_max)\n",
    "                ax.set_yticks(yticks)\n",
    "                ax.grid(axis='y', alpha=0.2)\n",
    "            _plt.tight_layout()\n",
    "            _plt.show()\n",
    "\n",
    "    axis_name_map = {'x_um':'X', 'y_um':'Y', 'z_um':'Z'}\n",
    "    outlier_rows = []\n",
    "    for ds in present:\n",
    "        sub = df_all_filt[df_all_filt['dataset']==ds]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        for axis_col in axis_cols:\n",
    "            vals = sub[axis_col].to_numpy(dtype=float)\n",
    "            if not len(vals):\n",
    "                continue\n",
    "            try:\n",
    "                q1, q3 = _np.percentile(vals, [25, 75])\n",
    "                iqr = q3 - q1\n",
    "                lo = q1 - 1.5*iqr\n",
    "                hi = q3 + 1.5*iqr\n",
    "            except Exception:\n",
    "                lo, hi = -_np.inf, _np.inf\n",
    "            outs = sub[(sub[axis_col] < lo) | (sub[axis_col] > hi)]\n",
    "            if outs.empty:\n",
    "                continue\n",
    "            for _, row in outs.iterrows():\n",
    "                entry = {\n",
    "                    'dataset': ds,\n",
    "                    'axis': axis_name_map.get(axis_col, axis_col),\n",
    "                    'diameter_um': float(row[axis_col])\n",
    "                }\n",
    "                if 'label' in row and not pd.isna(row['label']):\n",
    "                    entry['label'] = int(row['label'])\n",
    "                if ds == 'Functional' and 'plane_idx' in row and not pd.isna(row['plane_idx']):\n",
    "                    entry['plane_idx'] = int(row['plane_idx'])\n",
    "                if 'file' in row and not pd.isna(row['file']):\n",
    "                    entry['file'] = row['file']\n",
    "                outlier_rows.append(entry)\n",
    "\n",
    "    if outlier_rows:\n",
    "        out_df = pd.DataFrame(outlier_rows).sort_values(\n",
    "            ['dataset','axis','diameter_um'],\n",
    "            ascending=[True, True, False]\n",
    "        ).reset_index(drop=True)\n",
    "        print('Outlier diameters after HCR filter (IQR fence, pooled), sorted high→low (per-plane/per-file labels):')\n",
    "        try:\n",
    "            from IPython.display import HTML, display\n",
    "            html = out_df.to_html(index=False)\n",
    "            display(HTML(f\"<div style='max-height:320px; overflow-y:auto'>{html}</div>\"))\n",
    "        except Exception:\n",
    "            print(out_df.to_string(index=False))\n",
    "    else:\n",
    "        print('No diameter outliers found after filter.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad8761c",
   "metadata": {},
   "source": [
    "### [31] Diagnostic: functional mask voxel assumptions vs pixel-scale diameters\n",
    "Compare voxel-based and pixel-based mask diameter assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761b88a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [32]\n",
    "\n",
    "# Diagnostic: functional mask voxel assumptions vs pixel-scale diameters\n",
    "try:\n",
    "    func_labels = globals().get(\"func_labels\", None)\n",
    "    if isinstance(func_labels, list) and not any(m is not None for m in func_labels):\n",
    "        func_labels = None\n",
    "    if func_labels is None:\n",
    "        # Try cellpose masks first\n",
    "        _cp_masks = []\n",
    "        for pr in plane_refs:\n",
    "            lbl = pr.get(\"label\", None)\n",
    "            if not lbl:\n",
    "                continue\n",
    "            mp = OUT_SEG / f\"{lbl}_cellpose_masks.tif\"\n",
    "            if mp.exists():\n",
    "                _cp_masks.append(imread(mp))\n",
    "        if _cp_masks:\n",
    "            func_labels = _np.stack(_cp_masks, axis=0)\n",
    "            source = \"cellpose masks\"\n",
    "        elif FUNC_LABELS_PATH and os.path.exists(FUNC_LABELS_PATH):\n",
    "            func_labels = imread(FUNC_LABELS_PATH)\n",
    "            source = f\"FUNC_LABELS_PATH: {FUNC_LABELS_PATH}\"\n",
    "        else:\n",
    "            source = \"none\"\n",
    "    else:\n",
    "        source = \"suite2p labels\"\n",
    "        if isinstance(func_labels, list):\n",
    "            try:\n",
    "                func_labels = _np.stack([m for m in func_labels if m is not None], axis=0)\n",
    "            except Exception:\n",
    "                func_labels = None\n",
    "                source = \"suite2p labels (stack failed)\"\n",
    "    print('Functional labels source:', source)\n",
    "    if func_labels is None:\n",
    "        print('No functional labels available for diagnostic.')\n",
    "    else:\n",
    "        func_3d = func_labels if func_labels.ndim == 3 else func_labels[None, ...]\n",
    "        vox_f = {\n",
    "            'Z': 1.0,\n",
    "            'Y': float(VOX_FUNC.get('Y',1.0)) if 'VOX_FUNC' in globals() and VOX_FUNC else 1.0,\n",
    "            'X': float(VOX_FUNC.get('X',1.0)) if 'VOX_FUNC' in globals() and VOX_FUNC else 1.0,\n",
    "        }\n",
    "        print('VOX_FUNC used:', vox_f)\n",
    "        df_um = diameters_um_from_array(func_3d, vox_f)\n",
    "        df_px = diameters_um_from_array(func_3d, {'Z':1.0,'Y':1.0,'X':1.0})\n",
    "        print('Median diameters (µm): X={:.2f}, Y={:.2f}, Z={:.2f}'.format(df_um['x_um'].median(), df_um['y_um'].median(), df_um['z_um'].median()))\n",
    "        print('Median diameters (pixels): X={:.2f}, Y={:.2f}, Z={:.2f}'.format(df_px['x_um'].median(), df_px['y_um'].median(), df_px['z_um'].median()))\n",
    "except Exception as e:\n",
    "    import traceback; traceback.print_exc()\n",
    "    print('Diagnostic failed:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a_dist_violin_hdr",
   "metadata": {},
   "source": [
    "### [33] Centroid distance QA (labels already in anatomy space)\n",
    "Quantify centroid distances between functional labels and anatomy labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e372973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [34]\n",
    "# 3.1a) Centroid distance QA\n",
    "_has_planes = ('plane_refs' in globals() and plane_refs)\n",
    "if not _has_planes:\n",
    "    print('No planes available for centroid QA.')\n",
    "\n",
    "# Preload anatomy labels if available\n",
    "anat_labels_all = None\n",
    "try:\n",
    "    if ANAT_LABELS_PATH and os.path.exists(ANAT_LABELS_PATH):\n",
    "        anat_labels_all = _ensure_uint_labels(imread(ANAT_LABELS_PATH))\n",
    "except Exception as _e:\n",
    "    print('Could not load anatomy labels for centroid QA:', _e)\n",
    "\n",
    "# Cache per-plane link tables\n",
    "_links_cache = {}\n",
    "_plane_data_cache = {}\n",
    "_global_data = None\n",
    "\n",
    "# Font sizes for this QA plot\n",
    "QA_LEGEND_FONTSIZE = 15\n",
    "QA_YTICK_FONTSIZE = 15\n",
    "MIN_OVERLAP_FUNC_ANAT = int(globals().get('MIN_OVERLAP_FUNC_ANAT', 1))\n",
    "REQUIRE_OVERLAP_FUNC_ANAT = bool(globals().get('REQUIRE_OVERLAP_FUNC_ANAT', True))\n",
    "\n",
    "\n",
    "def _load_func_labels_for_plane(p_idx):\n",
    "    if _has_planes:\n",
    "        lbl = plane_refs[p_idx].get('label', f'plane{p_idx}')\n",
    "        use_s2p = bool(globals().get('USE_SUITE2P_LABELS', False))\n",
    "        if use_s2p:\n",
    "            fl = globals().get('func_labels', None)\n",
    "            if fl is not None:\n",
    "                if isinstance(fl, list):\n",
    "                    if p_idx < len(fl):\n",
    "                        arr_i = fl[p_idx]\n",
    "                        if arr_i is not None:\n",
    "                            return _ensure_uint_labels(arr_i), lbl, f\"Suite2p labels list[{p_idx}]\"\n",
    "                else:\n",
    "                    arr_np = _ensure_uint_labels(np.asarray(fl))\n",
    "                    if arr_np.ndim == 3 and p_idx < arr_np.shape[0]:\n",
    "                        return arr_np[p_idx], lbl, f\"Suite2p labels stack[{p_idx}]\"\n",
    "                    if arr_np.ndim == 2:\n",
    "                        return arr_np, lbl, \"Suite2p labels (2D)\"\n",
    "        cp_path = OUT_SEG / f\"{lbl}_cellpose_masks.tif\"\n",
    "        legacy_cp_path = OUTDIR / f\"{lbl}_cellpose_masks.tif\"\n",
    "        if cp_path.exists():\n",
    "            return _ensure_uint_labels(imread(cp_path)), lbl, f\"Cellpose masks: {cp_path}\"\n",
    "        if legacy_cp_path.exists():\n",
    "            return _ensure_uint_labels(imread(legacy_cp_path)), lbl, f\"Cellpose masks (legacy): {legacy_cp_path}\"\n",
    "    if FUNC_LABELS_PATH and os.path.exists(FUNC_LABELS_PATH):\n",
    "        arr = _ensure_uint_labels(imread(FUNC_LABELS_PATH))\n",
    "        if arr.ndim == 3 and p_idx < arr.shape[0]:\n",
    "            return arr[p_idx], f\"FUNC_LABELS_PATH[{p_idx}]\", FUNC_LABELS_PATH\n",
    "        if arr.ndim == 2:\n",
    "            return arr, 'FUNC_LABELS_PATH', FUNC_LABELS_PATH\n",
    "        if arr.ndim > 2:\n",
    "            return arr[...,0], 'FUNC_LABELS_PATH[...,0]', FUNC_LABELS_PATH\n",
    "    return None, None, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _compute_links_for_plane(p_idx):\n",
    "    def _apply_overlap_gate(df):\n",
    "        if not REQUIRE_OVERLAP_FUNC_ANAT:\n",
    "            return df\n",
    "        if \"overlap_px\" not in df.columns:\n",
    "            return None\n",
    "        return df[df[\"overlap_px\"] >= int(MIN_OVERLAP_FUNC_ANAT)].reset_index(drop=True)\n",
    "\n",
    "    if p_idx in _links_cache:\n",
    "        return _links_cache[p_idx]\n",
    "    if not _has_planes:\n",
    "        return None\n",
    "    p_idx = int(max(0, min(len(plane_refs)-1, p_idx)))\n",
    "    pr = plane_refs[p_idx]\n",
    "    plabel = pr.get(\"label\", f\"plane{p_idx}\")\n",
    "    cache_paths = []\n",
    "    write_paths = []\n",
    "    if plabel:\n",
    "        if 'OUT_REG' in globals():\n",
    "            p_out = OUT_REG / f\"f2a_centroid_matches_{plabel}.csv\"\n",
    "            cache_paths.append(p_out)\n",
    "            write_paths.append(p_out)\n",
    "        if 'OUTDIR' in globals():\n",
    "            cache_paths.append(OUTDIR / f\"f2a_centroid_matches_{plabel}.csv\")\n",
    "    if p_idx == 0:\n",
    "        if 'OUT_REG' in globals():\n",
    "            p_out = OUT_REG / \"f2a_centroid_matches.csv\"\n",
    "            cache_paths.append(p_out)\n",
    "            write_paths.append(p_out)\n",
    "        if 'OUTDIR' in globals():\n",
    "            cache_paths.append(OUTDIR / \"f2a_centroid_matches.csv\")\n",
    "    for cp in cache_paths:\n",
    "        if os.path.exists(cp):\n",
    "            try:\n",
    "                df_cached = pd.read_csv(cp)\n",
    "                df_cached = _apply_overlap_gate(df_cached)\n",
    "                if df_cached is not None:\n",
    "                    _links_cache[p_idx] = df_cached\n",
    "                    return df_cached\n",
    "            except Exception:\n",
    "                pass\n",
    "    func_labels, func_label_name, func_src = _load_func_labels_for_plane(p_idx)\n",
    "    if func_labels is None:\n",
    "        print(f\"[3.1a] Missing functional labels for plane {plabel}\")\n",
    "        return None\n",
    "    if func_labels.ndim == 3:\n",
    "        func_labels = func_labels[...,0]\n",
    "    if func_labels.ndim != 2:\n",
    "        print(f\"[3.1a] Unexpected func label shape for plane {plabel}: {func_labels.shape}\")\n",
    "        return None\n",
    "    tform_for_qa = pr.get(\"tform\", tform)\n",
    "    best_z_for_qa = int(pr.get(\"best_z\", best_z if \"best_z\" in globals() else 0))\n",
    "    anat_labels_z = None\n",
    "    if anat_labels_all is not None:\n",
    "        anat_labels_z = anat_labels_all[best_z_for_qa] if anat_labels_all.ndim == 3 else anat_labels_all\n",
    "    if anat_labels_z is None:\n",
    "        print(f\"[3.1a] Missing anatomy labels for plane {plabel}\")\n",
    "        return None\n",
    "    func_warped = func_labels\n",
    "    try:\n",
    "        if tform_for_qa is not None:\n",
    "            func_warped = resample_labels_nn(func_labels, tform_for_qa, output_shape=anat_labels_z.shape)\n",
    "        elif func_warped.shape != anat_labels_z.shape:\n",
    "            from skimage.transform import AffineTransform\n",
    "            func_warped = resample_labels_nn(func_labels, AffineTransform(), output_shape=anat_labels_z.shape)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if func_warped.shape != anat_labels_z.shape:\n",
    "        print(f\"[3.1a] Func/anat shape mismatch for plane {plabel}: {func_warped.shape} vs {anat_labels_z.shape}\")\n",
    "        return None\n",
    "    fdf = _regionprops_centroids_2d(func_warped)\n",
    "    adf = _regionprops_centroids_2d(anat_labels_z)\n",
    "    F = fdf[[\"cx\",\"cy\"]].to_numpy(); A = adf[[\"cx\",\"cy\"]].to_numpy()\n",
    "    if F.size == 0 or A.size == 0:\n",
    "        _links_cache[p_idx] = pd.DataFrame(columns=[\"fx_anat_px\",\"fy_anat_px\",\"ax_px\",\"ay_px\",\"dist_px\",\"dist_um\",\"func_label\",\"anat_label\",\"overlap_px\"])\n",
    "        return _links_cache[p_idx]\n",
    "    d2 = ((F[:,None,:] - A[None,:,:])**2).sum(axis=2); D = np.sqrt(d2)\n",
    "    from scipy.optimize import linear_sum_assignment as _lsa\n",
    "    row_ind, col_ind = _lsa(D)\n",
    "    MAX_LINK_DIST_PX = 50.0\n",
    "    keep = D[row_ind, col_ind] <= MAX_LINK_DIST_PX\n",
    "    row_ind = row_ind[keep]; col_ind = col_ind[keep]\n",
    "    try:\n",
    "        vox_x = float(VOX_ANAT.get(\"X\", VOX_ANAT.get(2, VOX_ANAT.get(\"2\", 1.0))))\n",
    "        vox_y = float(VOX_ANAT.get(\"Y\", VOX_ANAT.get(1, VOX_ANAT.get(\"1\", 1.0))))\n",
    "    except Exception:\n",
    "        vox_x, vox_y = 1.0, 1.0\n",
    "    links = []\n",
    "    for r, c in zip(row_ind, col_ind):\n",
    "        fxp, fyp = float(F[r,0]), float(F[r,1]); axp, ayp = float(A[c,0]), float(A[c,1])\n",
    "        dist_px = float(D[r,c])\n",
    "        dx_um = (fxp - axp) * vox_x; dy_um = (fyp - ayp) * vox_y\n",
    "        dist_um = float(np.sqrt(dx_um*dx_um + dy_um*dy_um))\n",
    "        links.append({\"fx_anat_px\": fxp, \"fy_anat_px\": fyp, \"ax_px\": axp, \"ay_px\": ayp, \"dist_px\": dist_px, \"dist_um\": dist_um, \"func_label\": int(fdf.iloc[r][\"label\"]), \"anat_label\": int(adf.iloc[c][\"label\"])} )\n",
    "    df = pd.DataFrame(links)\n",
    "    overlap_df = compute_label_overlap(func_warped, anat_labels_z, min_overlap_voxels=1)\n",
    "    overlap_df = overlap_df.rename(columns={\"conf_label\":\"func_label\",\"twoP_label\":\"anat_label\",\"overlap_voxels\":\"overlap_px\"})\n",
    "    df = df.merge(overlap_df, on=[\"func_label\",\"anat_label\"], how=\"left\") if not overlap_df.empty else df.assign(overlap_px=0)\n",
    "    df[\"overlap_px\"] = df[\"overlap_px\"].fillna(0).astype(int)\n",
    "    if REQUIRE_OVERLAP_FUNC_ANAT:\n",
    "        df = df[df[\"overlap_px\"] >= int(MIN_OVERLAP_FUNC_ANAT)]\n",
    "    df = df.reset_index(drop=True)\n",
    "    _links_cache[p_idx] = df\n",
    "    if len(df):\n",
    "        for cp in (write_paths or cache_paths):\n",
    "            try:\n",
    "                df.to_csv(cp, index=False)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return df\n",
    "def _prepare_plane_data(p_idx):\n",
    "    if p_idx in _plane_data_cache:\n",
    "        return _plane_data_cache[p_idx]\n",
    "    df = _compute_links_for_plane(p_idx)\n",
    "    if df is None or not len(df):\n",
    "        _plane_data_cache[p_idx] = None\n",
    "        return None\n",
    "    try:\n",
    "        import numpy as _np\n",
    "        d = df['dist_um'].to_numpy().astype(float)\n",
    "        med = float(_np.median(d))\n",
    "    except Exception:\n",
    "        d = df['dist_um'].to_numpy().astype(float)\n",
    "        med = float(np.median(d))\n",
    "    pr = plane_refs[int(p_idx)] if _has_planes else {}\n",
    "    label = pr.get('label', f'plane{p_idx}') if pr else f'plane{p_idx}'\n",
    "    best_z_for_qa = int(pr.get('best_z', best_z if 'best_z' in globals() else 0)) if pr else int(best_z if 'best_z' in globals() else 0)\n",
    "    try:\n",
    "        bg = norm01(anat[int(best_z_for_qa)])\n",
    "    except Exception:\n",
    "        bg = None\n",
    "    _plane_data_cache[p_idx] = {\n",
    "        'df': df,\n",
    "        'd': d,\n",
    "        'med': med,\n",
    "        'label': label,\n",
    "        'best_z': best_z_for_qa,\n",
    "        'bg': bg,\n",
    "    }\n",
    "    return _plane_data_cache[p_idx]\n",
    "\n",
    "\n",
    "def _collect_global_data():\n",
    "    global _global_data\n",
    "    if _global_data is not None:\n",
    "        return _global_data\n",
    "    if not _has_planes:\n",
    "        return None\n",
    "    import numpy as _np\n",
    "    all_d = []\n",
    "    for p in range(len(plane_refs)):\n",
    "        pdp = _prepare_plane_data(p)\n",
    "        if pdp is not None and pdp.get('d') is not None and pdp['d'].size:\n",
    "            all_d.append(pdp['d'])\n",
    "    if not all_d:\n",
    "        _global_data = None\n",
    "        return None\n",
    "    dcat = _np.concatenate(all_d).astype(float)\n",
    "    _global_data = {'d': dcat, 'med': float(_np.median(dcat)), 'N': int(dcat.size)}\n",
    "    return _global_data\n",
    "\n",
    "\n",
    "if _has_planes:\n",
    "    import numpy as _np, matplotlib.pyplot as _plt, ipywidgets as widgets\n",
    "    plane_sl = widgets.IntSlider(value=0, min=0, max=max(0, len(plane_refs)-1), step=1, description='Plane', continuous_update=False)\n",
    "    thr_sl = widgets.FloatSlider(value=0.0, min=0.0, max=10.0, step=0.1, description='Threshold (µm)', style={'description_width': '120px'}, layout=widgets.Layout(width='420px'))\n",
    "    out = widgets.Output(); count_html = widgets.HTML()\n",
    "    _state = {'plane': int(plane_sl.value)}\n",
    "\n",
    "    def _render(threshold):\n",
    "        pdata = _plane_data_cache.get(_state['plane'], None)\n",
    "        if pdata is None:\n",
    "            with out:\n",
    "                out.clear_output(wait=True)\n",
    "                print('No centroid matches to plot for this plane.')\n",
    "            count_html.value = ''\n",
    "            return\n",
    "        d = pdata['d']; df = pdata['df']; med = pdata['med']; bg = pdata['bg']; label = pdata['label']; bz = pdata['best_z']\n",
    "        gdata = _collect_global_data()\n",
    "        dg = gdata['d'] if gdata is not None else None\n",
    "        gmed = gdata['med'] if gdata is not None else None\n",
    "        gN = int(gdata.get('N', dg.size if dg is not None else 0)) if gdata is not None else None\n",
    "        N = int(d.size)\n",
    "        with out:\n",
    "            out.clear_output(wait=True)\n",
    "            fig, (ax1, ax2, ax3) = _plt.subplots(1,3, figsize=(20,7), gridspec_kw={'width_ratios':[1,3,1]})\n",
    "            vp = ax1.violinplot(d, showmeans=False, showmedians=False, showextrema=False)\n",
    "            for pc in vp['bodies']:\n",
    "                pc.set_facecolor('#88ccee'); pc.set_edgecolor('black'); pc.set_alpha(0.7)\n",
    "            ax1.axhline(med, color='crimson', linestyle='--', linewidth=1.5, label=f'Median {med:.2f} µm')\n",
    "            ax1.axhline(float(threshold), color='orange', linestyle=':', linewidth=1.5, label=f'Thresh {float(threshold):.2f} µm')\n",
    "            ax1.set_xticks([]); ax1.set_ylabel('Centroid distance (µm)'); ax1.set_title(f'Centroid distances (µm) — {label}'); ax1.legend(loc='lower right', fontsize=QA_LEGEND_FONTSIZE)\n",
    "            ax1.tick_params(axis='y', labelsize=QA_YTICK_FONTSIZE)\n",
    "            if bg is not None:\n",
    "                ax2.imshow(bg, cmap='gray')\n",
    "            keep_mask = d <= float(threshold); keep = int(keep_mask.sum())\n",
    "            if keep and all(col in df.columns for col in ('ax_px','ay_px','fx_anat_px','fy_anat_px')):\n",
    "                kept = df[keep_mask] if keep_mask.shape[0] == len(df) else df\n",
    "                ax2.scatter(kept['ax_px'], kept['ay_px'], s=30, c='magenta', label='Anat centroids')\n",
    "                ax2.scatter(kept['fx_anat_px'], kept['fy_anat_px'], s=30, facecolors='none', edgecolors='lime', label='Func→Anat centroids')\n",
    "                for _, row in kept.iterrows():\n",
    "                    ax2.plot([row['ax_px'], row['fx_anat_px']], [row['ay_px'], row['fy_anat_px']], 'y-', alpha=0.5)\n",
    "                ax2.legend(loc='lower right', fontsize=QA_LEGEND_FONTSIZE)\n",
    "            ax2.set_title(f'Kept links (≤ threshold) on anatomy — {label} @ Z={bz}'); ax2.axis('off')\n",
    "            if dg is not None and dg.size:\n",
    "                vp_g = ax3.violinplot(dg, showmeans=False, showmedians=False, showextrema=False)\n",
    "                for pc in vp_g['bodies']:\n",
    "                    pc.set_facecolor('#b3d9ff'); pc.set_edgecolor('black'); pc.set_alpha(0.7)\n",
    "                if gmed is not None:\n",
    "                    ax3.axhline(gmed, color='crimson', linestyle='--', linewidth=1.5, label=f'Global median {gmed:.2f} µm')\n",
    "                ax3.axhline(float(threshold), color='orange', linestyle=':', linewidth=1.5, label=f'Thresh {float(threshold):.2f} µm')\n",
    "                ax3.set_xticks([])\n",
    "                ax3.set_ylabel('Centroid distance (µm)')\n",
    "                ttl = 'All planes' if gN is None else f'All planes (N={gN})'\n",
    "                ax3.set_title(ttl)\n",
    "                ax3.legend(loc='lower right', fontsize=QA_LEGEND_FONTSIZE)\n",
    "            ax3.set_ylim(ax1.get_ylim())\n",
    "            ax3.tick_params(axis='y', labelsize=QA_YTICK_FONTSIZE)\n",
    "            _plt.tight_layout(); _plt.show()\n",
    "            count_html.value = f\"<b>Plane:</b> {label} | <b>Best Z:</b> {bz} | <b>Keep:</b> {keep} / {N} cells (≤ {float(threshold):.2f} µm)\"\n",
    "\n",
    "    def _on_thr(change):\n",
    "        if change.get('name') == 'value':\n",
    "            _render(change['new'])\n",
    "\n",
    "    def _ensure_plane_settings(p_idx):\n",
    "        pdata = _prepare_plane_data(p_idx)\n",
    "        if pdata is None:\n",
    "            return None\n",
    "        d = pdata['d']\n",
    "        if d.size:\n",
    "            thr_max = float(_np.percentile(d, 99))\n",
    "            thr_val = float(_np.percentile(d, 90))\n",
    "        else:\n",
    "            thr_max = 10.0; thr_val = 0.0\n",
    "        try:\n",
    "            thr_sl.unobserve(_on_thr, names='value')\n",
    "        except Exception:\n",
    "            pass\n",
    "        thr_sl.min = 0.0\n",
    "        thr_sl.max = max(1.0, thr_max)\n",
    "        thr_sl.value = min(thr_sl.max, max(thr_sl.min, thr_val))\n",
    "        thr_sl.observe(_on_thr, names='value')\n",
    "        return pdata\n",
    "\n",
    "    def _on_plane(change):\n",
    "        if change.get('name') == 'value':\n",
    "            _state['plane'] = int(change['new'])\n",
    "            # reset global cache when switching plane list may change\n",
    "            _render_reset_global()\n",
    "\n",
    "    def _render_reset_global():\n",
    "        global _global_data\n",
    "        _global_data = None\n",
    "        pdata = _ensure_plane_settings(_state['plane'])\n",
    "        _render(thr_sl.value if pdata is not None else 0.0)\n",
    "\n",
    "    plane_sl.observe(_on_plane, names='value')\n",
    "    thr_sl.observe(_on_thr, names='value')\n",
    "\n",
    "    _render_reset_global()\n",
    "    ui = widgets.VBox([widgets.HBox([plane_sl, thr_sl]), count_html, out])\n",
    "    display(ui)\n",
    "else:\n",
    "    print('No plane_refs for centroid QA.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22535d0e",
   "metadata": {},
   "source": [
    "### [35] All-plane centroid distances (no threshold)\n",
    "Plot the combined centroid distance distribution across all planes with the median indicated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4293c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [36]\n",
    "# Aggregate centroid distances across all planes (no distance gating) as a violin plot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as _plt\n",
    "\n",
    "# Tuneable font sizes and figure size\n",
    "VIOLIN_LEGEND_FONTSIZE = 5  # kept for compatibility (legend removed)\n",
    "VIOLIN_YTICK_FONTSIZE = 10\n",
    "VIOLIN_FIGSIZE = (5, 5)\n",
    "VIOLIN_ANNOT_FONTSIZE = 10\n",
    "\n",
    "\n",
    "def _collect_all_plane_centroid_dists():\n",
    "    if 'plane_refs' not in globals() or not plane_refs:\n",
    "        return None, 'No plane_refs available.'\n",
    "    if '_collect_global_data' in globals():\n",
    "        try:\n",
    "            gd = _collect_global_data()\n",
    "            if gd and gd.get('d') is not None and gd['d'].size:\n",
    "                return np.asarray(gd['d'], dtype=float), None\n",
    "        except Exception:\n",
    "            pass\n",
    "    if '_prepare_plane_data' not in globals():\n",
    "        return None, 'Centroid QA helpers not initialized; run the previous QA cell first.'\n",
    "    all_d = []\n",
    "    for p in range(len(plane_refs)):\n",
    "        pdp = _prepare_plane_data(p)\n",
    "        if pdp and pdp.get('d') is not None and pdp['d'].size:\n",
    "            all_d.append(np.asarray(pdp['d'], dtype=float))\n",
    "    if not all_d:\n",
    "        return None, 'No centroid matches available to plot.'\n",
    "    return np.concatenate(all_d).astype(float), None\n",
    "\n",
    "\n",
    "def _median_centroid_diameter_um():\n",
    "    df_src = globals().get('df_all_filt', None)\n",
    "    if df_src is None or getattr(df_src, 'empty', True):\n",
    "        df_src = globals().get('df_all', None)\n",
    "        if df_src is None or getattr(df_src, 'empty', True):\n",
    "            _diam_paths = []\n",
    "            if 'OUT_QA' in globals():\n",
    "                _diam_paths.append(OUT_QA / 'diameters_df_all.pkl')\n",
    "            if 'OUTDIR' in globals():\n",
    "                _diam_paths.append(OUTDIR / 'diameters_df_all.pkl')\n",
    "            for _p in _diam_paths:\n",
    "                if os.path.exists(_p):\n",
    "                    try:\n",
    "                        df_src = pd.read_pickle(_p)\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        df_src = None\n",
    "    if df_src is None or getattr(df_src, 'empty', True):\n",
    "        return None, 'No diameter dataframe available (run diameters cell [13]/[14]).'\n",
    "    if 'dataset' not in df_src.columns:\n",
    "        return None, 'Diameter table missing dataset column.'\n",
    "    df_func = df_src[df_src['dataset'] == 'Functional']\n",
    "    if df_func.empty or not {'x_um', 'y_um'}.issubset(df_func.columns):\n",
    "        return None, 'Functional XY diameters unavailable.'\n",
    "    diam_xy = df_func[['x_um', 'y_um']].mean(axis=1).to_numpy(dtype=float)\n",
    "    if not diam_xy.size:\n",
    "        return None, 'Functional XY diameters unavailable.'\n",
    "    return float(np.median(diam_xy)), None\n",
    "\n",
    "\n",
    "dists, err = _collect_all_plane_centroid_dists()\n",
    "if err:\n",
    "    print(err)\n",
    "elif dists is None or not dists.size:\n",
    "    print('No centroid distances to plot.')\n",
    "else:\n",
    "    med = float(np.median(dists))\n",
    "    mean = float(np.mean(dists))\n",
    "    std = float(np.std(dists))\n",
    "    med_diam, diam_err = _median_centroid_diameter_um()\n",
    "    if diam_err:\n",
    "        print(diam_err)\n",
    "    fig, ax = _plt.subplots(figsize=VIOLIN_FIGSIZE)\n",
    "    vp = ax.violinplot([dists], showmeans=False, showmedians=False, showextrema=False)\n",
    "    for pc in vp['bodies']:\n",
    "        pc.set_facecolor('#88ccee')\n",
    "        pc.set_edgecolor('black')\n",
    "        pc.set_alpha(0.8)\n",
    "    ax.axhline(med, color='crimson', linestyle='--', linewidth=1.6)\n",
    "    ax.axhline(mean, color='#2ca02c', linestyle=':', linewidth=1.5)\n",
    "    ax.axhspan(mean - std, mean + std, color='#2ca02c', alpha=0.08)\n",
    "    if med_diam is not None:\n",
    "        ax.axhline(med_diam, color='orange', linestyle='-.', linewidth=1.6)\n",
    "    ax.set_xticks([])\n",
    "    ax.grid(axis='y', alpha=0.25)\n",
    "    ax.tick_params(axis='y', labelsize=VIOLIN_YTICK_FONTSIZE)\n",
    "    ax.set_yticklabels(ax.get_yticks(), fontsize=VIOLIN_YTICK_FONTSIZE)\n",
    "    ax.text(\n",
    "        0.98,\n",
    "        0.98,\n",
    "        f'Median: {med:.2f}\\nMean: {mean:.2f}\\nStd: {std:.2f}',\n",
    "        transform=ax.transAxes,\n",
    "        ha='right',\n",
    "        va='top',\n",
    "        fontsize=VIOLIN_ANNOT_FONTSIZE,\n",
    "        bbox=dict(boxstyle='round,pad=0.35', facecolor='white', edgecolor='#2ca02c', alpha=0.85),\n",
    "    )\n",
    "    _plt.tight_layout()\n",
    "    _plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1db945",
   "metadata": {},
   "source": [
    "### [37] Confocal masks via `best_rounds.csv` (ANTs transforms)\n",
    "Build a manifest of confocal masks and transform chains from metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [38]\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BEST_ROUNDS_CSV = Path(\"/Volumes/jlarsch/default/D2c/07_Data/Danin/best_rounds.csv\")\n",
    "NAS_ROOT = Path(\"/Volumes/jlarsch/default/D2c/07_Data\")\n",
    "MANIFEST_OUT = Path(\"/Volumes/jlarsch/default/D2c/07_Data/Danin/confocal_mask_manifest.csv\")\n",
    "MASK_GLOB = \"03_analysis/confocal/raw/cp_masks/*round{round_idx}*_cp_masks.tif\"\n",
    "# Override per fish/round if filenames differ (e.g., Cellpose). Example structure:\n",
    "# HARDCODED_MASKS = {\n",
    "#     \"L395_f11\": {\n",
    "#         1: [\"/path/to/L395_f11_round1_mask.tif\"],\n",
    "#         2: [\"/path/to/L395_f11_round2_mask.nrrd\"],\n",
    "#     }\n",
    "# }\n",
    "HARDCODED_MASKS = {}\n",
    "\n",
    "def owner_root(nas_root, owner):\n",
    "    base = Path(nas_root) / owner\n",
    "    mic = base / \"Microscopy\"\n",
    "    return mic if mic.exists() else base\n",
    "\n",
    "def find_transform_pair(fish_dir, src_round, target_tags):\n",
    "    tm_root = fish_dir / \"02_reg\"\n",
    "    candidates = sorted(tm_root.glob(\"**/transMatrices/*\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    src_token = f\"round{src_round}\"\n",
    "    warp = affine = None\n",
    "    for f in candidates:\n",
    "        name = f.name.lower()\n",
    "        if src_token not in name:\n",
    "            continue\n",
    "        if not any(tag in name for tag in target_tags):\n",
    "            continue\n",
    "        if \"inverse\" in name:\n",
    "            continue\n",
    "        if (\"warp\" in name) and name.endswith(\".nii.gz\") and warp is None:\n",
    "            warp = f\n",
    "        if (\"affine\" in name) and f.suffix == \".mat\" and affine is None:\n",
    "            affine = f\n",
    "        if warp and affine:\n",
    "            break\n",
    "    return warp, affine\n",
    "\n",
    "def build_manifest_rows(best_row):\n",
    "    fish_id = str(best_row.fish_id)\n",
    "    best_round = str(best_row.best_round).lower()\n",
    "    best_idx = int(best_round.lstrip(\"r\"))\n",
    "    num_rounds = int(best_row.num_rounds)\n",
    "    owner = str(best_row.owner)\n",
    "    fish_dir = owner_root(NAS_ROOT, owner) / fish_id\n",
    "    ref = fish_dir / \"02_reg\" / \"00_preprocessing\" / \"2p_anatomy\" / f\"{fish_id}_anatomy_2P_GCaMP.nrrd\"\n",
    "    if not ref.exists():\n",
    "        print(f\"[WARN] Missing 2P anatomy for {fish_id}\")\n",
    "        return []\n",
    "    best2p_warp, best2p_aff = find_transform_pair(fish_dir, best_idx, target_tags=[\"2p\", \"ref\"])\n",
    "    if not (best2p_warp and best2p_aff):\n",
    "        print(f\"[WARN] Missing best->2p transforms for {fish_id} ({best_round})\")\n",
    "        return []\n",
    "    rows = []\n",
    "    overrides = HARDCODED_MASKS.get(fish_id, {})\n",
    "    for round_idx in range(1, num_rounds + 1):\n",
    "        if round_idx in overrides:\n",
    "            masks = []\n",
    "            for p in overrides[round_idx]:\n",
    "                p = Path(p)\n",
    "                if p.exists():\n",
    "                    masks.append(p)\n",
    "                else:\n",
    "                    print(f\"[WARN] Override mask missing for {fish_id} round {round_idx}: {p}\")\n",
    "        else:\n",
    "            masks = list(fish_dir.glob(MASK_GLOB.format(round_idx=round_idx)))\n",
    "        if not masks:\n",
    "            print(f\"[WARN] No masks found for {fish_id} round {round_idx}\")\n",
    "            continue\n",
    "        if round_idx != best_idx:\n",
    "            r_to_best = find_transform_pair(fish_dir, round_idx, target_tags=[f\"r{best_idx}\", f\"round{best_idx}\"])\n",
    "            if not all(r_to_best):\n",
    "                print(f\"[WARN] Missing r{round_idx}->r{best_idx} transforms for {fish_id}\")\n",
    "                continue\n",
    "        for mask in masks:\n",
    "            out_dir = mask.parent / \"aligned\"\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "            out_path = out_dir / f\"{mask.stem}_in_2p{mask.suffix}\"\n",
    "            transform_chain = [best2p_warp, best2p_aff]\n",
    "            if round_idx != best_idx:\n",
    "                transform_chain += [r_to_best[0], r_to_best[1]]\n",
    "            rows.append({\n",
    "                \"moving\": str(mask),\n",
    "                \"reference\": str(ref),\n",
    "                \"transforms\": \"; \".join(str(t) for t in transform_chain if t),\n",
    "                \"output\": str(out_path),\n",
    "                \"fish_id\": fish_id,\n",
    "                \"label\": f\"r{round_idx}_to_2p_via_r{best_idx}\"\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "best_df = pd.read_csv(BEST_ROUNDS_CSV)\n",
    "required = {\"fish_id\", \"best_round\", \"num_rounds\", \"owner\"}\n",
    "missing = required - set(best_df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"best_rounds.csv missing columns: {missing}\")\n",
    "\n",
    "manifest_rows = []\n",
    "for row in best_df.itertuples():\n",
    "    manifest_rows.extend(build_manifest_rows(row))\n",
    "\n",
    "manifest_df = pd.DataFrame(manifest_rows)\n",
    "print(f\"[INFO] built {len(manifest_df)} manifest rows\")\n",
    "if not manifest_df.empty:\n",
    "    MANIFEST_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "    manifest_df.to_csv(MANIFEST_OUT, index=False)\n",
    "    display(manifest_df.head())\n",
    "else:\n",
    "    print(\"[WARN] Manifest is empty; check warnings above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da94f635",
   "metadata": {},
   "source": [
    "### [39] Save a small JSON with parameters and results\n",
    "Persist run metadata and parameters to JSON for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a3df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [40]\n",
    "\n",
    "meta = {\n",
    "    \"FUNC_STACK_PATH\": str(FUNC_STACK_PATH) if 'FUNC_STACK_PATH' in globals() and FUNC_STACK_PATH else None,\n",
    "    \"ANAT_STACK_PATH\": str(ANAT_STACK_PATH) if 'ANAT_STACK_PATH' in globals() and ANAT_STACK_PATH else None,\n",
    "    \"HCR_STACK_PATH\": str(HCR_STACK_PATH) if 'HCR_STACK_PATH' in globals() and HCR_STACK_PATH else None,\n",
    "    \"best_z\": int(best_z),\n",
    "    \"transform_params_2x3\": tform.params.tolist(),\n",
    "    \"ref_build_strategy\": str(REF_BUILD_STRATEGY) if 'REF_BUILD_STRATEGY' in globals() else None,\n",
    "    \"voxels\": {\"func\": VOX_FUNC, \"anat\": VOX_ANAT, \"hcr\": VOX_HCR},\n",
    "    \"voxel_cache\": str(VOX_CACHE_PATH) if 'VOX_CACHE_PATH' in globals() else None,\n",
    "    \"rng_seed\": RNG_SEED,\n",
    "}\n",
    "\n",
    "with open(OUT_REG/'run_metadata.json', 'w') as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"Wrote\", OUT_REG/'run_metadata.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f886197",
   "metadata": {},
   "source": [
    "### [41] [HCR→2P] Config and transform discovery (ANTsPy)\n",
    "Load best-round metadata and locate transforms/paths for HCR→2P warps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cfe30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HCR→2P configuration (uses ANTsPy; mirrors antsQC defaults)\n",
    "import re, json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BEST_ROUNDS_CSV = Path(\"/Volumes/jlarsch/default/D2c/07_Data/Danin/best_rounds.csv\")\n",
    "BEST_ROUND_OVERRIDE = globals().get(\"BEST_ROUND_OVERRIDE\", None)\n",
    "\n",
    "HCR_CP_MASK_DIR = FISH_DIR / \"03_analysis\" / \"confocal\" / \"raw\" / \"cp_masks\"\n",
    "HCR_ALIGNED_DIR = FISH_DIR / \"03_analysis\" / \"confocal\" / \"aligned\"\n",
    "HCR_ALIGNED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PREPROC_DIR = FISH_DIR / \"02_reg\" / \"00_preprocessing\"\n",
    "RBEST_TM_DIR = FISH_DIR / \"02_reg\" / \"01_rbest-2p\" / \"transMatrices\"\n",
    "RN_TO_RBEST_TM_DIR = FISH_DIR / \"02_reg\" / \"02_rn-rbest\" / \"transMatrices\"\n",
    "ANAT_INT_PATH = PREPROC_DIR / \"2p_anatomy\" / f\"{FISH_ID}_anatomy_2P_GCaMP.nrrd\"\n",
    "\n",
    "MATCH_METHOD = globals().get(\"MATCH_METHOD\", \"nn\")\n",
    "MAX_DISTANCE_UM = float(globals().get(\"MAX_DISTANCE_UM\", 10))\n",
    "REQUIRE_OVERLAP = bool(globals().get(\"REQUIRE_OVERLAP\", True))\n",
    "MIN_OVERLAP_VOXELS = int(globals().get(\"MIN_OVERLAP_VOXELS\", 1))\n",
    "DEDUP_BY_TWOP = globals().get(\"DEDUP_BY_TWOP\", \"max_overlap\")\n",
    "IOU_MIN = float(globals().get(\"IOU_MIN\", 0.05))\n",
    "USE_FRAC_FILTERS = bool(globals().get(\"USE_FRAC_FILTERS\", False))\n",
    "MIN_OVERLAP_FRAC_CONF = float(globals().get(\"MIN_OVERLAP_FRAC_CONF\", 0.0))\n",
    "MIN_OVERLAP_FRAC_TWOP = float(globals().get(\"MIN_OVERLAP_FRAC_TWOP\", 0.0))\n",
    "PLOT_USE_FINAL_1TO1 = bool(globals().get(\"PLOT_USE_FINAL_1TO1\", True))\n",
    "INVERT_AFFINE = bool(globals().get(\"INVERT_AFFINE\", False))\n",
    "FORCE_RECOMPUTE_WARP = bool(globals().get(\"FORCE_RECOMPUTE_WARP\", False))\n",
    "\n",
    "\n",
    "def _parse_round_from_name(path: Path):\n",
    "    m = re.search(r\"round(\\d+)\", path.name.lower())\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "\n",
    "def _load_best_round(fish_id: str):\n",
    "    if BEST_ROUND_OVERRIDE is not None:\n",
    "        val = str(BEST_ROUND_OVERRIDE).lower().lstrip(\"r\")\n",
    "        return int(val), None\n",
    "    df = pd.read_csv(BEST_ROUNDS_CSV)\n",
    "    row = df.loc[df[\"fish_id\"] == fish_id]\n",
    "    if row.empty:\n",
    "        raise ValueError(f\"No best round found for fish_id={fish_id} in {BEST_ROUNDS_CSV}\")\n",
    "    r = row.iloc[0]\n",
    "    best_round = int(str(r[\"best_round\"]).lower().lstrip(\"r\"))\n",
    "    num_rounds = int(r[\"num_rounds\"])\n",
    "    return best_round, num_rounds\n",
    "\n",
    "\n",
    "def _find_intensity_for_round(round_idx: int):\n",
    "    sub = PREPROC_DIR / (\"rbest\" if round_idx == best_round_idx else \"rn\")\n",
    "    candidates = sorted(sub.glob(\"*\"))\n",
    "    chosen = None\n",
    "    for p in candidates:\n",
    "        name = p.name.lower()\n",
    "        if f\"round{round_idx}\" in name and \"channel1\" in name and \"gcamp\" in name and p.suffix.lower() == \".nrrd\":\n",
    "            chosen = p\n",
    "            break\n",
    "    if chosen is None:\n",
    "        hits = [p for p in candidates if f\"round{round_idx}\" in p.name.lower() and p.suffix.lower() == \".nrrd\"]\n",
    "        if hits:\n",
    "            chosen = hits[0]\n",
    "    return chosen\n",
    "\n",
    "\n",
    "def _find_transform(trans_dir: Path, round_idx: int, target_tag: str, kind: str):\n",
    "    if trans_dir is None or not trans_dir.exists():\n",
    "        return None\n",
    "    hits = []\n",
    "    for p in sorted(trans_dir.glob(\"*\")):\n",
    "        name = p.name.lower()\n",
    "        if f\"round{round_idx}\" not in name:\n",
    "            continue\n",
    "        if target_tag not in name:\n",
    "            continue\n",
    "        if \"inverse\" in name:\n",
    "            continue\n",
    "        if kind == \"warp\" and name.endswith(\"warp.nii.gz\"):\n",
    "            hits.append(p)\n",
    "        if kind == \"affine\" and name.endswith(\".mat\"):\n",
    "            hits.append(p)\n",
    "    return hits[0] if hits else None\n",
    "\n",
    "\n",
    "best_round_idx, num_rounds = _load_best_round(FISH_ID)\n",
    "print(f\"[HCR] fish={FISH_ID} best_round=r{best_round_idx} num_rounds={num_rounds}\")\n",
    "print(f\"[HCR] ANAT_INT_PATH: {ANAT_INT_PATH}\")\n",
    "print(f\"[HCR] RBEST_TM_DIR: {RBEST_TM_DIR}\")\n",
    "print(f\"[HCR] RN_TO_RBEST_TM_DIR: {RN_TO_RBEST_TM_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed96fcdf",
   "metadata": {},
   "source": [
    "### [42] [HCR→2P] Warp confocal masks to 2P (cached ANTsPy)\n",
    "Warp HCR label masks (and optional intensities) into 2P space with caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083b82ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warp confocal masks (one TIFF per input) into 2P anatomy space\n",
    "import numpy as np, tifffile as tiff\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from skimage.measure import regionprops_table\n",
    "try:\n",
    "    import ants  # type: ignore\n",
    "    HAVE_ANTSPY = True\n",
    "except Exception as e:\n",
    "    HAVE_ANTSPY = False\n",
    "    print(\"ANTsPy not available; HCR warps disabled.\", e)\n",
    "\n",
    "if HAVE_ANTSPY and not ANAT_INT_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Fixed 2P anatomy intensity not found: {ANAT_INT_PATH}\")\n",
    "MIN_XY_HCR_UM = float(globals().get(\"MIN_XY_HCR_UM\", 3.0))\n",
    "\n",
    "def _ants_clone_geometry(dst_img, like_img):\n",
    "    dst_img.set_spacing(like_img.spacing); dst_img.set_origin(like_img.origin); dst_img.set_direction(like_img.direction); return dst_img\n",
    "\n",
    "def warp_cache_candidates(save_basename: Path):\n",
    "    return [Path(f\"{save_basename}_labels_int32.npy\"), Path(f\"{save_basename}_labels_uint16.tif\")]\n",
    "\n",
    "def load_cached_warp(save_basename: Path):\n",
    "    for c in warp_cache_candidates(save_basename):\n",
    "        if c.exists():\n",
    "            if c.suffix == \".npy\": return np.load(c, mmap_mode=\"r\"), c\n",
    "            if c.suffix == \".tif\": return tiff.imread(str(c)), c\n",
    "    return None, None\n",
    "\n",
    "def warp_metadata_path(save_basename: Path):\n",
    "    return Path(f\"{save_basename}_warp_meta.json\")\n",
    "\n",
    "def write_warp_metadata(save_basename: Path, metadata: dict):\n",
    "    warp_metadata_path(save_basename).write_text(json.dumps(metadata, indent=2))\n",
    "\n",
    "def warp_label_tiff_chain(tiff_path, mov_img_int, fix_img_int, transformlist, whichtoinvert, save_basename: Path):\n",
    "    lab_zyx = tiff.imread(str(tiff_path))\n",
    "    if lab_zyx.ndim != 3: raise ValueError(f\"Expected 3D TIFF (ZYX); got shape {lab_zyx.shape}\")\n",
    "    # Filter small labels in native HCR space (XY diameter) before warping\n",
    "    if MIN_XY_HCR_UM > 0:\n",
    "        try:\n",
    "            props = regionprops_table(lab_zyx, properties=('label','bbox'))\n",
    "            df_size = pd.DataFrame(props)\n",
    "            df_size = df_size[df_size['label'] != 0]\n",
    "            if not df_size.empty:\n",
    "                df_size = df_size.rename(columns={'bbox-0':'zmin','bbox-1':'ymin','bbox-2':'xmin','bbox-3':'zmax','bbox-4':'ymax','bbox-5':'xmax'})\n",
    "                dx = float(VOX_HCR.get('X', 1.0)) if 'VOX_HCR' in globals() else 1.0\n",
    "                dy = float(VOX_HCR.get('Y', 1.0)) if 'VOX_HCR' in globals() else 1.0\n",
    "                df_size['x_um'] = (df_size['xmax'] - df_size['xmin']) * dx\n",
    "                df_size['y_um'] = (df_size['ymax'] - df_size['ymin']) * dy\n",
    "                drop_labels = df_size.loc[(df_size['x_um'] < MIN_XY_HCR_UM) | (df_size['y_um'] < MIN_XY_HCR_UM), 'label'].to_numpy(dtype=np.int64)\n",
    "                if drop_labels.size:\n",
    "                    lab_zyx = np.where(np.isin(lab_zyx, drop_labels), 0, lab_zyx)\n",
    "                    print(f\"[HCR] {Path(tiff_path).name}: dropped {len(drop_labels)} labels with native XY < {MIN_XY_HCR_UM} µm before warp\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    lab_xyz = np.transpose(lab_zyx, (2,1,0)).astype(np.int32, copy=False)\n",
    "    mov_label_img = ants.from_numpy(lab_xyz); _ants_clone_geometry(mov_label_img, mov_img_int)\n",
    "    if mov_label_img.shape != mov_img_int.shape: raise RuntimeError(f\"Label XYZ shape {mov_label_img.shape} != moving intensity shape {mov_img_int.shape}.\")\n",
    "    warped_xyz = ants.apply_transforms(fixed=fix_img_int, moving=mov_label_img, transformlist=[str(t) for t in transformlist], whichtoinvert=list(whichtoinvert), interpolator=\"nearestNeighbor\").numpy().astype(np.int32, copy=False)\n",
    "    warped_zyx = np.transpose(warped_xyz, (2,1,0))\n",
    "    out_paths = {}\n",
    "    if int(warped_zyx.max()) <= 65535:\n",
    "        tif_path = Path(f\"{save_basename}_labels_uint16.tif\"); tiff.imwrite(tif_path, warped_zyx.astype(np.uint16)); out_paths[\"tif\"] = tif_path\n",
    "    else:\n",
    "        npy_path = Path(f\"{save_basename}_labels_int32.npy\"); np.save(npy_path, warped_zyx); out_paths[\"npy\"] = npy_path\n",
    "    return warped_zyx, out_paths\n",
    "\n",
    "def _invert_flags_for_chain(chain):\n",
    "    return [bool(INVERT_AFFINE) if str(p).lower().endswith(\".mat\") else False for p in chain]\n",
    "\n",
    "warp_results = []\n",
    "if not HAVE_ANTSPY:\n",
    "    print(\"ANTsPy missing; skip HCR warps.\")\n",
    "else:\n",
    "    fix_img_int = ants.image_read(str(ANAT_INT_PATH))\n",
    "    best_warp = _find_transform(RBEST_TM_DIR, best_round_idx, \"to_2p\", \"warp\")\n",
    "    best_aff  = _find_transform(RBEST_TM_DIR, best_round_idx, \"to_2p\", \"affine\")\n",
    "    if best_warp is None or best_aff is None:\n",
    "        raise FileNotFoundError(f\"Missing r{best_round_idx}->2P transforms in {RBEST_TM_DIR}\")\n",
    "\n",
    "    mask_paths = sorted(HCR_CP_MASK_DIR.glob(\"*round*_cp_masks*.tif\"))\n",
    "    if not mask_paths: print(f\"[HCR] No confocal masks found in {HCR_CP_MASK_DIR}\")\n",
    "    intensity_paths = []\n",
    "    if bool(globals().get(\"WARP_HCR_INTENSITY_QC\", False)):\n",
    "        manual_intensity = globals().get(\"HCR_INTENSITY_PATHS\", None)\n",
    "        if manual_intensity:\n",
    "            intensity_paths = [Path(p) for p in manual_intensity]\n",
    "        else:\n",
    "            intensity_paths = sorted(list((PREPROC_DIR/\"rbest\").glob(\"*round*_channel*.nrrd\")) + list((PREPROC_DIR/\"rn\").glob(\"*round*_channel*.nrrd\")))\n",
    "        manual_int_path = globals().get(\"MANUAL_INTENSITY_WARP_PATH\", None)\n",
    "        manual_int_round = globals().get(\"MANUAL_INTENSITY_WARP_ROUND\", None)\n",
    "        if manual_int_path:\n",
    "            intensity_paths.append(Path(manual_int_path))\n",
    "            print(f\"[HCR] Manual intensity path requested (round {manual_int_round}): {manual_int_path}\")\n",
    "        print(f\"[HCR] Intensity QC enabled; {len(intensity_paths)} stack(s) found:\")\n",
    "        for ip in intensity_paths: print(\"   \", ip)\n",
    "\n",
    "    for mp in mask_paths:\n",
    "        rnd = _parse_round_from_name(mp)\n",
    "        if rnd is None: print(\"[HCR] Skip (no round in name):\", mp); continue\n",
    "        mov_int_path = _find_intensity_for_round(rnd)\n",
    "        if mov_int_path is None or not mov_int_path.exists(): print(f\"[HCR] Missing intensity NRRD for round {rnd}: {mov_int_path}\"); continue\n",
    "        mov_img_int = ants.image_read(str(mov_int_path))\n",
    "        if rnd == best_round_idx:\n",
    "            chain = [best_warp, best_aff]\n",
    "        else:\n",
    "            r_to_best_warp = _find_transform(RN_TO_RBEST_TM_DIR, rnd, f\"to_r{best_round_idx}\", \"warp\")\n",
    "            r_to_best_aff  = _find_transform(RN_TO_RBEST_TM_DIR, rnd, f\"to_r{best_round_idx}\", \"affine\")\n",
    "            chain = [best_warp, best_aff, r_to_best_warp, r_to_best_aff]  # warp before affine so ANTs applies affine then warp\n",
    "        if any(c is None for c in chain): print(f\"[HCR] Incomplete transform chain for round {rnd}; chain={chain}\"); continue\n",
    "        chain_flags = _invert_flags_for_chain(chain)\n",
    "        save_base = HCR_ALIGNED_DIR / f\"{mp.stem}_in_2p\"\n",
    "        cached, cached_src = load_cached_warp(save_base)\n",
    "        use_cache = cached is not None and not FORCE_RECOMPUTE_WARP and MIN_XY_HCR_UM <= 0\n",
    "        if use_cache:\n",
    "            warped = cached; out_paths = {\"cache\": cached_src}\n",
    "        else:\n",
    "            warped, out_paths = warp_label_tiff_chain(mp, mov_img_int, fix_img_int, chain, chain_flags, save_base)\n",
    "            write_warp_metadata(save_base, {\"mask\": str(mp), \"moving_intensity\": str(mov_int_path), \"fixed_intensity\": str(ANAT_INT_PATH), \"transforms\": [str(t) for t in chain], \"whichtoinvert\": chain_flags, \"best_round\": int(best_round_idx), \"round\": int(rnd)})\n",
    "        warp_results.append({\"round\": rnd, \"mask_path\": mp, \"warped\": warped, \"warped_paths\": out_paths, \"save_base\": save_base})\n",
    "        print(f\"[HCR] warped {mp.name} (round {rnd}) -> shape {warped.shape}\")\n",
    "\n",
    "    if intensity_paths:\n",
    "        for ip in intensity_paths:\n",
    "            ip_path = Path(ip)\n",
    "            if not ip_path.exists(): print(f\"[HCR] Intensity path missing, skip: {ip_path}\"); continue\n",
    "            rnd = None\n",
    "            if manual_int_path and ip_path == Path(manual_int_path) and manual_int_round is not None:\n",
    "                try: rnd = int(str(manual_int_round).lstrip('r'))\n",
    "                except Exception: rnd = None\n",
    "            if rnd is None: rnd = _parse_round_from_name(ip_path)\n",
    "            if rnd is None: print(f\"[HCR] Could not parse round from intensity path: {ip_path}\"); continue\n",
    "            if rnd == best_round_idx:\n",
    "                chain = [best_warp, best_aff]\n",
    "            else:\n",
    "                r_to_best_warp = _find_transform(RN_TO_RBEST_TM_DIR, rnd, f\"to_r{best_round_idx}\", \"warp\")\n",
    "                r_to_best_aff  = _find_transform(RN_TO_RBEST_TM_DIR, rnd, f\"to_r{best_round_idx}\", \"affine\")\n",
    "                chain = [best_warp, best_aff, r_to_best_warp, r_to_best_aff]  # warp before affine so ANTs applies affine then warp\n",
    "            if any(c is None for c in chain): print(f\"[HCR] Incomplete chain for intensity {ip_path}\"); continue\n",
    "            chain_flags = _invert_flags_for_chain(chain)\n",
    "            mov_img_int = ants.image_read(str(ip_path))\n",
    "            warped_img = ants.apply_transforms(fixed=fix_img_int, moving=mov_img_int, transformlist=[str(t) for t in chain], whichtoinvert=list(chain_flags), interpolator=\"linear\")\n",
    "            out_path = HCR_ALIGNED_DIR / f\"{ip_path.stem}_in_2p.nrrd\"\n",
    "            ants.image_write(warped_img, str(out_path))\n",
    "            print(f\"[HCR] warped intensity {ip_path.name} -> {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a7ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### [43a] HCR warp sanity check (post native XY filter)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "if not globals().get('warp_results'):\n",
    "    print('[43a] warp_results not available; run the warp cell first.')\n",
    "else:\n",
    "    rows = []\n",
    "    total = 0\n",
    "    for res in warp_results:\n",
    "        arr = np.asarray(res.get('warped'))\n",
    "        name = Path(res.get('mask_path','unknown')).name\n",
    "        labels = np.unique(arr)\n",
    "        labels = labels[labels != 0]\n",
    "        n = int(labels.size)\n",
    "        rows.append({'file': name, 'labels_after_filter': n})\n",
    "        total += n\n",
    "    df_counts = pd.DataFrame(rows).sort_values('file').reset_index(drop=True)\n",
    "    print(f\"[43a] Total HCR labels after native-space XY filter: {total}\")\n",
    "    try:\n",
    "        display(df_counts)\n",
    "    except Exception:\n",
    "        print(df_counts.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406a63df",
   "metadata": {},
   "source": [
    "### [43] Matching + QC vs 2P anatomy labels (antsQC style)\n",
    "Description pending for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8a684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### [44] Matching and QC for each warped mask (overlap-first; NN by default)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tifffile as tiff\n",
    "import json\n",
    "from pathlib import Path\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from skimage.measure import regionprops_table\n",
    "\n",
    "if not warp_results:\n",
    "    print(\"[HCR] No warped masks to match.\")\n",
    "else:\n",
    "    if ANAT_LABELS_PATH is None or not Path(ANAT_LABELS_PATH).exists():\n",
    "        raise RuntimeError(\"ANAT_LABELS_PATH is missing; cannot run QC.\")\n",
    "    anat_labels = imread_any(ANAT_LABELS_PATH)\n",
    "    anat_labels = _ensure_uint_labels(anat_labels)\n",
    "    vox_anat_um = {\n",
    "        \"dz\": float(VOX_ANAT.get(\"Z\", 1.0)) if \"VOX_ANAT\" in globals() else 1.0,\n",
    "        \"dy\": float(VOX_ANAT.get(\"Y\", 1.0)) if \"VOX_ANAT\" in globals() else 1.0,\n",
    "        \"dx\": float(VOX_ANAT.get(\"X\", 1.0)) if \"VOX_ANAT\" in globals() else 1.0,\n",
    "    }\n",
    "\n",
    "    hcr_match_results = []\n",
    "    total_conf_masks = 0\n",
    "    total_after_overlap = 0\n",
    "    total_after_twop = 0\n",
    "    total_after_conf = 0\n",
    "    masks_processed = 0\n",
    "\n",
    "    # Parameters with fallbacks\n",
    "    REQUIRE_OVERLAP = bool(globals().get('REQUIRE_OVERLAP', True))\n",
    "    MIN_OVERLAP_VOXELS = int(globals().get('MIN_OVERLAP_VOXELS', 1))\n",
    "    MAX_DISTANCE_UM = float(globals().get('MAX_DISTANCE_UM', np.inf))\n",
    "    MATCH_METHOD = globals().get('MATCH_METHOD', 'nn')\n",
    "    DEDUP_BY_TWOP = globals().get('DEDUP_BY_TWOP', 'closest')\n",
    "    DEDUP_BY_CONF = globals().get('DEDUP_BY_CONF', 'closest')\n",
    "    IOU_MIN = float(globals().get('IOU_MIN', 0.05))\n",
    "    USE_FRAC_FILTERS = bool(globals().get('USE_FRAC_FILTERS', False))\n",
    "    MIN_OVERLAP_FRAC_CONF = float(globals().get('MIN_OVERLAP_FRAC_CONF', 0.0))\n",
    "    MIN_OVERLAP_FRAC_TWOP = float(globals().get('MIN_OVERLAP_FRAC_TWOP', 0.0))\n",
    "    PLOT_USE_FINAL_1TO1 = bool(globals().get('PLOT_USE_FINAL_1TO1', False))\n",
    "    MIN_XY_HCR_UM = float(globals().get('MIN_XY_HCR_UM', 3.0))\n",
    "    for res in warp_results:\n",
    "        conf_labels_2p = np.array(res[\"warped\"], copy=True)\n",
    "        if conf_labels_2p.shape != anat_labels.shape:\n",
    "            print(f\"[HCR] Shape mismatch confocal vs anat: {conf_labels_2p.shape} vs {anat_labels.shape}\")\n",
    "            continue\n",
    "\n",
    "        df_conf = compute_centroids(conf_labels_2p)\n",
    "        df_2p = compute_centroids(anat_labels)\n",
    "        P_conf_um = idx_to_um(df_conf, vox_anat_um)\n",
    "        P_2p_um = idx_to_um(df_2p, vox_anat_um)\n",
    "\n",
    "        labels_conf = df_conf[\"label\"].to_numpy()\n",
    "        labels_2p = df_2p[\"label\"].to_numpy()\n",
    "\n",
    "        overlap_df = None\n",
    "        matches = None\n",
    "\n",
    "        if REQUIRE_OVERLAP:\n",
    "            overlap_df = compute_label_overlap(conf_labels_2p, anat_labels, min_overlap_voxels=int(MIN_OVERLAP_VOXELS))\n",
    "            if overlap_df.empty:\n",
    "                print(f\"[HCR] No overlapping labels found for {res['mask_path']}; relax MIN_OVERLAP_VOXELS or check inputs.\")\n",
    "                continue\n",
    "\n",
    "            conf_coords = dict(zip(labels_conf, P_conf_um))\n",
    "            twoP_coords = dict(zip(labels_2p, P_2p_um))\n",
    "            overlap_df['distance_um'] = overlap_df.apply(\n",
    "                lambda r: float(np.linalg.norm(conf_coords[int(r['conf_label'])] - twoP_coords[int(r['twoP_label'])])),\n",
    "                axis=1,\n",
    "            )\n",
    "\n",
    "            if MATCH_METHOD == 'hungarian':\n",
    "                conf_ids = sorted(overlap_df['conf_label'].unique())\n",
    "                twoP_ids = sorted(overlap_df['twoP_label'].unique())\n",
    "                cost_fill = 1e9\n",
    "                cost = np.full((len(conf_ids), len(twoP_ids)), cost_fill, dtype=float)\n",
    "                conf_idx = {c: i for i, c in enumerate(conf_ids)}\n",
    "                twoP_idx = {t: i for i, t in enumerate(twoP_ids)}\n",
    "                for _, r in overlap_df.iterrows():\n",
    "                    cost[conf_idx[int(r['conf_label'])], twoP_idx[int(r['twoP_label'])]] = float(r['distance_um'])\n",
    "                row_ind, col_ind = linear_sum_assignment(cost)\n",
    "                keep = cost[row_ind, col_ind] < cost_fill\n",
    "                row_ind, col_ind = row_ind[keep], col_ind[keep]\n",
    "                matches = pd.DataFrame({\n",
    "                    'conf_label': np.array(conf_ids, dtype=int)[row_ind],\n",
    "                    'twoP_label': np.array(twoP_ids, dtype=int)[col_ind],\n",
    "                    'distance_um': cost[row_ind, col_ind],\n",
    "                })\n",
    "                matches = matches.merge(overlap_df[['conf_label','twoP_label','overlap_voxels']], on=['conf_label','twoP_label'], how='left')\n",
    "            else:  # default to NN within overlap\n",
    "                best_idx = overlap_df.groupby('conf_label')['distance_um'].idxmin()\n",
    "                matches = overlap_df.loc[best_idx, ['conf_label','twoP_label','distance_um','overlap_voxels']].copy()\n",
    "\n",
    "            matches['within_gate'] = matches['distance_um'] <= float(MAX_DISTANCE_UM)\n",
    "\n",
    "        else:\n",
    "            if MATCH_METHOD == \"nn\":\n",
    "                dists, nn = nearest_neighbor_match(P_conf_um, P_2p_um)\n",
    "                matched_twoP_labels = labels_2p[nn]\n",
    "                matched_conf_labels = labels_conf\n",
    "            elif MATCH_METHOD == \"hungarian\":\n",
    "                dists, col_ind, row_ind = hungarian_match(P_conf_um, P_2p_um, max_cost=np.inf)\n",
    "                matched_conf_labels = labels_conf[row_ind]\n",
    "                matched_twoP_labels = labels_2p[col_ind]\n",
    "            else:\n",
    "                raise ValueError('MATCH_METHOD must be \"nn\" or \"hungarian\"')\n",
    "\n",
    "            valid = dists <= float(MAX_DISTANCE_UM)\n",
    "            matches = pd.DataFrame({\n",
    "                \"conf_label\": matched_conf_labels,\n",
    "                \"twoP_label\": matched_twoP_labels,\n",
    "                \"distance_um\": dists,\n",
    "                \"within_gate\": valid\n",
    "            }).sort_values(\"distance_um\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "            overlap_df = compute_label_overlap(conf_labels_2p, anat_labels, min_overlap_voxels=int(MIN_OVERLAP_VOXELS))\n",
    "            matches = matches.merge(overlap_df[[\"conf_label\", \"twoP_label\", \"overlap_voxels\"]], on=[\"conf_label\", \"twoP_label\"], how=\"left\")\n",
    "            matches[\"overlap_voxels\"] = matches[\"overlap_voxels\"].fillna(0).astype(int)\n",
    "            if REQUIRE_OVERLAP:\n",
    "                matches[\"within_gate\"] = matches[\"within_gate\"] & (matches[\"overlap_voxels\"] >= int(MIN_OVERLAP_VOXELS))\n",
    "\n",
    "        if 'overlap_voxels' not in matches.columns:\n",
    "            matches['overlap_voxels'] = 0\n",
    "\n",
    "        matches = matches.sort_values(\"distance_um\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "        # Counts at each step\n",
    "        after_overlap = int(matches['conf_label'].nunique())\n",
    "        after_twop = None\n",
    "        if DEDUP_BY_TWOP in {'closest','max_overlap'}:\n",
    "            m = matches['within_gate'].to_numpy()\n",
    "            if m.any():\n",
    "                sub = matches.loc[m].copy()\n",
    "                if DEDUP_BY_TWOP == 'closest':\n",
    "                    sub = sub.sort_values(['twoP_label','distance_um'], ascending=[True, True])\n",
    "                else:\n",
    "                    sub = sub.sort_values(['twoP_label','overlap_voxels','distance_um'], ascending=[True, False, True])\n",
    "                keep_idx = sub.drop_duplicates(subset=['twoP_label'], keep='first').index\n",
    "                drop_idx = sub.index.difference(keep_idx)\n",
    "                if len(drop_idx) > 0:\n",
    "                    matches.loc[drop_idx, 'within_gate'] = False\n",
    "            after_twop = int(matches['within_gate'].sum())\n",
    "        if after_twop is None:\n",
    "            after_twop = int(matches['within_gate'].sum())\n",
    "\n",
    "        after_conf = None\n",
    "        if DEDUP_BY_CONF in {'closest','max_overlap'}:\n",
    "            m = matches['within_gate'].to_numpy()\n",
    "            if m.any():\n",
    "                sub = matches.loc[m].copy()\n",
    "                if DEDUP_BY_CONF == 'closest':\n",
    "                    sub = sub.sort_values(['conf_label','distance_um'], ascending=[True, True])\n",
    "                else:\n",
    "                    sub = sub.sort_values(['conf_label','overlap_voxels','distance_um'], ascending=[True, False, True])\n",
    "                keep_idx = sub.drop_duplicates(subset=['conf_label'], keep='first').index\n",
    "                drop_idx = sub.index.difference(keep_idx)\n",
    "                if len(drop_idx) > 0:\n",
    "                    matches.loc[drop_idx, 'within_gate'] = False\n",
    "            after_conf = int(matches['within_gate'].sum())\n",
    "        if after_conf is None:\n",
    "            after_conf = int(matches['within_gate'].sum())\n",
    "\n",
    "        total_masks = int(len(labels_conf))\n",
    "        after_dedup = int(matches.loc[matches['within_gate'], 'conf_label'].nunique())\n",
    "\n",
    "        def label_volumes(arr):\n",
    "            labels, counts = np.unique(arr, return_counts=True)\n",
    "            s = pd.Series(counts, index=labels)\n",
    "            return s.drop(index=0, errors='ignore').astype(int)\n",
    "\n",
    "        conf_warped_labels = conf_labels_2p\n",
    "        twoP_labels = anat_labels\n",
    "\n",
    "        conf_vol_s = label_volumes(conf_warped_labels)\n",
    "        twoP_vol_s = label_volumes(twoP_labels)\n",
    "\n",
    "        matches['conf_vol'] = matches['conf_label'].map(conf_vol_s).fillna(0).astype(int)\n",
    "        matches['twoP_vol'] = matches['twoP_label'].map(twoP_vol_s).fillna(0).astype(int)\n",
    "\n",
    "        den = matches['conf_vol'] + matches['twoP_vol'] - matches['overlap_voxels']\n",
    "        matches['iou'] = np.divide(matches['overlap_voxels'], den, out=np.zeros_like(den, dtype=float), where=(den > 0))\n",
    "        matches['overlap_frac_conf'] = np.divide(matches['overlap_voxels'], matches['conf_vol'], out=np.zeros_like(matches['conf_vol'], dtype=float), where=(matches['conf_vol'] > 0))\n",
    "        matches['overlap_frac_twoP'] = np.divide(matches['overlap_voxels'], matches['twoP_vol'], out=np.zeros_like(matches['twoP_vol'], dtype=float), where=(matches['twoP_vol'] > 0))\n",
    "\n",
    "        acc = matches.loc[matches['within_gate']].copy()\n",
    "        conf_counts = acc['conf_label'].value_counts()\n",
    "        twop_counts = acc['twoP_label'].value_counts()\n",
    "\n",
    "        def _pair_type(row):\n",
    "            if not row['within_gate']:\n",
    "                return 'rejected'\n",
    "            cm = int(conf_counts.get(row['conf_label'], 0))\n",
    "            tm = int(twop_counts.get(row['twoP_label'], 0))\n",
    "            if cm == 1 and tm == 1: return '1-1'\n",
    "            if cm > 1 and tm == 1:  return 'merge'\n",
    "            if cm == 1 and tm > 1:  return 'split'\n",
    "            return 'complex'\n",
    "\n",
    "        matches['pair_type'] = matches.apply(_pair_type, axis=1)\n",
    "\n",
    "        if USE_FRAC_FILTERS:\n",
    "            ok_frac = (matches['overlap_frac_conf'] >= float(MIN_OVERLAP_FRAC_CONF)) & (matches['overlap_frac_twoP'] >= float(MIN_OVERLAP_FRAC_TWOP))\n",
    "        else:\n",
    "            ok_frac = True\n",
    "\n",
    "        matches['quality'] = np.where(\n",
    "            matches['within_gate'] & (matches['iou'] >= float(IOU_MIN)) & ok_frac, 'good',\n",
    "            np.where(matches['within_gate'], 'iffy', 'rejected')\n",
    "        )\n",
    "\n",
    "        final_pairs = matches[(matches['pair_type'] == '1-1') & (matches['quality'] == 'good')].copy().sort_values(['distance_um', 'twoP_label'])\n",
    "        review = matches[((matches['pair_type'].isin(['split','merge','complex'])) & matches['within_gate']) | ((matches['pair_type'] == '1-1') & (matches['quality'] != 'good'))].sort_values(['pair_type','iou','distance_um'], ascending=[True, False, True]).copy()\n",
    "\n",
    "        gate_mask = (matches['distance_um'] <= float(MAX_DISTANCE_UM))\n",
    "        if bool(REQUIRE_OVERLAP):\n",
    "            gate_mask = gate_mask & (matches['overlap_voxels'] >= int(MIN_OVERLAP_VOXELS))\n",
    "\n",
    "        qc = pd.Series({\n",
    "            'gate_only_pairs': int(gate_mask.sum()),\n",
    "            'accepted_pairs_after_dedup': int(matches['within_gate'].sum()),\n",
    "            'final_1to1_good': int(final_pairs.shape[0]),\n",
    "            'splits_among_accepted': int((matches['pair_type'] == 'split').sum()),\n",
    "            'merges_among_accepted': int((matches['pair_type'] == 'merge').sum()),\n",
    "            'complex_among_accepted': int((matches['pair_type'] == 'complex').sum()),\n",
    "        }, name='QC Summary')\n",
    "\n",
    "        save_base = res['save_base']\n",
    "        matches_csv = Path(f\"{save_base}_matches.csv\")\n",
    "        matches.to_csv(matches_csv, index=False)\n",
    "        final_pairs.to_csv(Path(f\"{save_base}_final_pairs.csv\"), index=False)\n",
    "        review.to_csv(Path(f\"{save_base}_review.csv\"), index=False)\n",
    "\n",
    "        if PLOT_USE_FINAL_1TO1 and not final_pairs.empty:\n",
    "            matches_for_plots = final_pairs.copy()\n",
    "            key_final = set(map(tuple, matches_for_plots[['conf_label','twoP_label']].to_numpy()))\n",
    "            accepted_mask = matches[['conf_label','twoP_label']].apply(tuple, axis=1).isin(key_final).to_numpy()\n",
    "        else:\n",
    "            matches_for_plots = matches.loc[matches['within_gate']].copy()\n",
    "            accepted_mask = matches['within_gate'].to_numpy()\n",
    "\n",
    "        dz_um = float(vox_anat_um['dz'])\n",
    "        scope_df = matches_for_plots\n",
    "        conf_ids = np.unique(scope_df['conf_label'].to_numpy(dtype=int)); conf_ids = conf_ids[conf_ids != 0]\n",
    "        twoP_ids = np.unique(scope_df['twoP_label'].to_numpy(dtype=int)); twoP_ids = twoP_ids[twoP_ids != 0]\n",
    "\n",
    "        conf_warped_labels = conf_labels_2p\n",
    "        twoP_labels = anat_labels\n",
    "\n",
    "        conf_within_mask = np.isin(conf_warped_labels, conf_ids)\n",
    "        twoP_within_mask = np.isin(twoP_labels, twoP_ids)\n",
    "\n",
    "        tiff.imwrite(Path(f\"{save_base}_conf_within_mask.tif\"), (conf_within_mask.astype(np.uint8) * 255), imagej=True, metadata={'axes': 'ZYX', 'spacing': dz_um, 'unit': 'um'}, compression='deflate')\n",
    "        tiff.imwrite(Path(f\"{save_base}_twoP_within_mask.tif\"), (twoP_within_mask.astype(np.uint8) * 255), imagej=True, metadata={'axes': 'ZYX', 'spacing': dz_um, 'unit': 'um'}, compression='deflate')\n",
    "\n",
    "        conf_within_labels = np.where(conf_within_mask, conf_warped_labels, 0)\n",
    "        twoP_within_labels = np.where(twoP_within_mask, twoP_labels, 0)\n",
    "\n",
    "        def _min_unsigned_dtype(max_val: int):\n",
    "            import numpy as _np\n",
    "            if max_val <= _np.iinfo(_np.uint16).max:\n",
    "                return _np.uint16\n",
    "            elif max_val <= _np.iinfo(_np.uint32).max:\n",
    "                return _np.uint32\n",
    "            return _np.uint64\n",
    "\n",
    "        conf_dtype = _min_unsigned_dtype(int(conf_within_labels.max()))\n",
    "        twoP_dtype = _min_unsigned_dtype(int(twoP_within_labels.max()))\n",
    "        conf_within_labels = conf_within_labels.astype(conf_dtype, copy=False)\n",
    "        twoP_within_labels = twoP_within_labels.astype(twoP_dtype, copy=False)\n",
    "\n",
    "        def _imwrite_with_meta(path, arr):\n",
    "            import numpy as _np\n",
    "            if arr.dtype in (_np.uint8, _np.uint16):\n",
    "                tiff.imwrite(path, arr, imagej=True, metadata={'axes': 'ZYX', 'spacing': dz_um, 'unit': 'um'}, compression='deflate')\n",
    "            else:\n",
    "                tiff.imwrite(path, arr, compression='deflate')\n",
    "\n",
    "        _imwrite_with_meta(Path(f\"{save_base}_conf_within_labels.tif\"), conf_within_labels)\n",
    "        _imwrite_with_meta(Path(f\"{save_base}_twoP_within_labels.tif\"), twoP_within_labels)\n",
    "\n",
    "        rgb = np.zeros(conf_within_mask.shape + (3,), dtype=np.uint8)\n",
    "        rgb[..., 0] = np.where(conf_within_mask, 242, 0)\n",
    "        rgb[..., 1] = np.where(conf_within_mask,  84, 0)\n",
    "        rgb[..., 2] = np.where(conf_within_mask, 166, 0)\n",
    "        rgb[..., 0] = np.clip(rgb[..., 0] + np.where(twoP_within_mask,  51, 0), 0, 255)\n",
    "        rgb[..., 1] = np.clip(rgb[..., 1] + np.where(twoP_within_mask, 166, 0), 0, 255)\n",
    "        rgb[..., 2] = np.clip(rgb[..., 2] + np.where(twoP_within_mask, 255, 0), 0, 255)\n",
    "        tiff.imwrite(Path(f\"{save_base}_within_gate_overlay_rgb.tif\"), rgb, photometric='rgb', compression='deflate')\n",
    "\n",
    "        summary = summarize_distances(matches['distance_um'].to_numpy(), matches['within_gate'].to_numpy())\n",
    "        print(f\"Counts — total conf masks: {total_masks} | after overlap filter: {after_overlap} | after dedup by twoP: {after_twop} | after dedup by conf: {after_conf}\")\n",
    "        print('Summary:', json.dumps(summary, indent=2))\n",
    "\n",
    "        total_conf_masks += total_masks\n",
    "        total_after_overlap += after_overlap\n",
    "        total_after_twop += after_twop\n",
    "        total_after_conf += after_conf\n",
    "        masks_processed += 1\n",
    "\n",
    "        hcr_match_results.append({\n",
    "            'round': res['round'],\n",
    "            'mask_path': res['mask_path'],\n",
    "            'matches': matches,\n",
    "            'final_pairs': final_pairs,\n",
    "            'review': review,\n",
    "            'summary': summary,\n",
    "            'qc': qc,\n",
    "            'matches_csv': matches_csv,\n",
    "            'df_conf': df_conf,\n",
    "            'df_2p': df_2p,\n",
    "            'P_conf_um': P_conf_um,\n",
    "            'P_2p_um': P_2p_um,\n",
    "            'warped': conf_labels_2p,\n",
    "        })\n",
    "\n",
    "    if masks_processed > 0:\n",
    "        print(f\"[HCR] Global counts — conf masks: {total_conf_masks} | after overlap: {total_after_overlap} | after dedup twoP: {total_after_twop} | after dedup conf: {total_after_conf}\")\n",
    "    else:\n",
    "        print(\"[HCR] No warped masks processed.\")\n",
    "\n",
    "    best_round_matches = [r for r in hcr_match_results if r['round'] == best_round_idx]\n",
    "    if best_round_matches:\n",
    "        hcr_primary_matches = best_round_matches[0]['matches']\n",
    "        hcr_primary_final_pairs = best_round_matches[0]['final_pairs']\n",
    "        print(f\"[HCR] Primary results from round r{best_round_idx}: {len(hcr_primary_final_pairs)} final pairs\")\n",
    "    else:\n",
    "        hcr_primary_matches = None\n",
    "        hcr_primary_final_pairs = None\n",
    "        print(\"[HCR] No primary results (best round missing)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03807f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic printout of HCR matching results\n",
    "from pathlib import Path\n",
    "print(\"PLOT_USE_FINAL_1TO1 =\", PLOT_USE_FINAL_1TO1)\n",
    "for res in hcr_match_results:\n",
    "    fp = res.get('final_pairs')\n",
    "    m = res.get('matches')\n",
    "    print(Path(res['mask_path']).name,\n",
    "          len(fp) if fp is not None else None,\n",
    "          int(m['within_gate'].sum()) if m is not None and 'within_gate' in m.columns else None,\n",
    "          res.get('warped') is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a001fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "twoP_labels_all = _ensure_uint_labels(imread_any(ANAT_LABELS_PATH))\n",
    "for i, pr in enumerate(plane_refs):\n",
    "    bz = int(pr.get('best_z', -1))\n",
    "    func_labels_raw, func_label_name, func_src = _load_func_labels_for_plane(i)\n",
    "    print(\n",
    "        f\"plane {i} label={pr.get('label')} best_z={bz} \"\n",
    "        f\"func_src={func_src} func_ok={func_labels_raw is not None} \"\n",
    "        f\"anat_slice_ok={0 <= bz < twoP_labels_all.shape[0]}\"\n",
    "    )\n",
    "    if func_labels_raw is None or not (0 <= bz < twoP_labels_all.shape[0]):\n",
    "        continue\n",
    "    func_n = int((func_labels_raw > 0).sum())\n",
    "    anat_n = int((twoP_labels_all[bz] > 0).sum())\n",
    "    print(f\"  func voxels>0: {func_n}, anat voxels>0: {anat_n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860635ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [46] Functional↔Anatomy per-plane matching summary (viewer prep/debug)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.measure import regionprops_table\n",
    "from scipy.optimize import linear_sum_assignment as _lsa\n",
    "\n",
    "if not plane_refs:\n",
    "    print('No plane_refs available; skipping per-plane match summary.')\n",
    "else:\n",
    "    DZ = float(VOX_ANAT.get('Z', 1.0)) if 'VOX_ANAT' in globals() else 1.0\n",
    "    DY = float(VOX_ANAT.get('Y', 1.0)) if 'VOX_ANAT' in globals() else 1.0\n",
    "    DX = float(VOX_ANAT.get('X', 1.0)) if 'VOX_ANAT' in globals() else 1.0\n",
    "    MIN_OVERLAP_FUNC_ANAT = int(globals().get('MIN_OVERLAP_FUNC_ANAT', 1))\n",
    "    REQUIRE_OVERLAP_FUNC_ANAT = bool(globals().get('REQUIRE_OVERLAP_FUNC_ANAT', True))\n",
    "    MAX_DIST_FUNC_ANAT = float(globals().get('MAX_DIST_FUNC_ANAT', np.inf))\n",
    "\n",
    "    twoP_labels_all = _ensure_uint_labels(imread_any(ANAT_LABELS_PATH))\n",
    "    z_size = twoP_labels_all.shape[0]\n",
    "\n",
    "    def load_func_for_plane(pr, p_idx):\n",
    "        if '_get_labels_for_plane' in globals():\n",
    "            try:\n",
    "                arr, desc = _get_labels_for_plane(pr, p_idx)\n",
    "                return arr, desc\n",
    "            except Exception:\n",
    "                pass\n",
    "        if '_load_func_labels_for_plane' in globals():\n",
    "            try:\n",
    "                arr, _, desc = globals()['_load_func_labels_for_plane'](p_idx)\n",
    "                return arr, desc\n",
    "            except Exception:\n",
    "                pass\n",
    "        return None, None\n",
    "\n",
    "    def norm_func_labels(arr):\n",
    "        arr = _ensure_uint_labels(arr)\n",
    "        if arr.ndim == 3 and arr.shape[-1] in (3, 4):\n",
    "            arr = arr[..., 0]\n",
    "        if arr.ndim == 3 and arr.shape[0] == 1:\n",
    "            arr = arr[0]\n",
    "        return arr\n",
    "\n",
    "    plane_anat_ids = {}\n",
    "    plane_match_summary = []\n",
    "\n",
    "    for p_idx, pr in enumerate(plane_refs):\n",
    "        lbl = pr.get('label', f'plane{p_idx}')\n",
    "        bz = int(pr.get('best_z', 0))\n",
    "        if bz < 0 or bz >= z_size:\n",
    "            print(f\"[plane_match] skip {lbl}: best_z {bz} out of bounds\")\n",
    "            continue\n",
    "\n",
    "        anat_slice = _ensure_uint_labels(twoP_labels_all[bz])\n",
    "        func_raw, desc = load_func_for_plane(pr, p_idx)\n",
    "        if func_raw is None:\n",
    "            print(f\"[plane_match] skip {lbl}: no functional labels for plane\")\n",
    "            continue\n",
    "        func_raw = norm_func_labels(func_raw)\n",
    "        if func_raw.ndim != 2:\n",
    "            print(f\"[plane_match] skip {lbl}: functional labels shape {func_raw.shape} not 2D\")\n",
    "            continue\n",
    "\n",
    "        tform_use = pr.get('tform', globals().get('tform', None))\n",
    "        if tform_use is None:\n",
    "            func_warped = func_raw.copy()\n",
    "        else:\n",
    "            func_warped = resample_labels_nn(func_raw, tform_use, output_shape=anat_slice.shape)\n",
    "        func_warped = _ensure_uint_labels(func_warped)\n",
    "\n",
    "        f_props = regionprops_table(func_warped, properties=('label','centroid'))\n",
    "        a_props = regionprops_table(anat_slice, properties=('label','centroid'))\n",
    "        fdf = pd.DataFrame(f_props).rename(columns={'centroid-0':'cy','centroid-1':'cx'})\n",
    "        adf2d = pd.DataFrame(a_props).rename(columns={'centroid-0':'cy','centroid-1':'cx'})\n",
    "        fdf = fdf[fdf['label'] != 0].reset_index(drop=True)\n",
    "        adf2d = adf2d[adf2d['label'] != 0].reset_index(drop=True)\n",
    "        if fdf.empty:\n",
    "            print(f\"[plane_match] skip {lbl}: functional labels contain no nonzero regions\")\n",
    "            continue\n",
    "        if adf2d.empty:\n",
    "            print(f\"[plane_match] skip {lbl}: anatomy slice has no labels\")\n",
    "            continue\n",
    "\n",
    "        overlap_df = compute_label_overlap(func_warped, anat_slice, min_overlap_voxels=1)\n",
    "        if overlap_df.empty:\n",
    "            print(f\"[plane_match] skip {lbl}: no func↔anat overlap at this plane\")\n",
    "            continue\n",
    "        overlap_df = overlap_df.rename(columns={'conf_label':'func_label','twoP_label':'anat_label','overlap_voxels':'overlap_px'})\n",
    "\n",
    "        F = fdf[['cx','cy']].to_numpy(); A = adf2d[['cx','cy']].to_numpy()\n",
    "        D = np.sqrt(((F[:,None,:]-A[None,:,:])**2).sum(axis=2))\n",
    "        row_ind, col_ind = _lsa(D)\n",
    "        links = []\n",
    "        for r,c in zip(row_ind, col_ind):\n",
    "            fxp, fyp = float(F[r,0]), float(F[r,1]); axp, ayp = float(A[c,0]), float(A[c,1])\n",
    "            dist_px = float(D[r,c]); dx_um = (fxp-axp)*DX; dy_um=(fyp-ayp)*DY\n",
    "            dist_um = float(np.sqrt(dx_um*dx_um + dy_um*dy_um))\n",
    "            links.append({'func_label': int(fdf.iloc[r]['label']), 'anat_label': int(adf2d.iloc[c]['label']), 'dist_um': dist_um})\n",
    "        links_df = pd.DataFrame(links)\n",
    "        n_func_labels = len(fdf); n_anat_labels = len(adf2d); n_initial = len(links_df)\n",
    "\n",
    "        links_df = links_df.merge(overlap_df, on=['func_label','anat_label'], how='left')\n",
    "        links_df['overlap_px'] = links_df['overlap_px'].fillna(0).astype(int)\n",
    "        n_after_overlap = len(links_df)\n",
    "        if REQUIRE_OVERLAP_FUNC_ANAT:\n",
    "            links_df = links_df[links_df['overlap_px'] >= MIN_OVERLAP_FUNC_ANAT]\n",
    "        n_after_overlap_filter = len(links_df)\n",
    "\n",
    "        dist_gate = MAX_DIST_FUNC_ANAT if np.isfinite(MAX_DIST_FUNC_ANAT) and MAX_DIST_FUNC_ANAT > 0 else None\n",
    "        n_before_dist = len(links_df)\n",
    "        if dist_gate is not None:\n",
    "            links_df = links_df[links_df['dist_um'] <= dist_gate]\n",
    "        n_after_dist = len(links_df)\n",
    "\n",
    "        print(f\"[plane_match] {lbl}: func={n_func_labels}, anat={n_anat_labels}, matches={n_initial} -> overlap {n_after_overlap_filter}/{n_after_overlap} -> dist {n_after_dist}/{n_before_dist}\")\n",
    "        if links_df.empty:\n",
    "            continue\n",
    "\n",
    "        plane_anat_ids[lbl] = set(links_df['anat_label'].tolist())\n",
    "        plane_match_summary.append({\n",
    "            'plane': lbl,\n",
    "            'best_z': bz,\n",
    "            'n_func': n_func_labels,\n",
    "            'n_anat': n_anat_labels,\n",
    "            'matches_initial': n_initial,\n",
    "            'matches_overlap': n_after_overlap_filter,\n",
    "            'matches_dist': n_after_dist,\n",
    "        })\n",
    "\n",
    "    globals()['plane_anat_ids'] = plane_anat_ids\n",
    "    globals()['plane_match_summary'] = plane_match_summary\n",
    "    print('Stored plane_anat_ids and plane_match_summary (rerun viewer if you want 3D).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fae4db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [47]\n",
    "# 3D viewer: per-plane functional/anatomy with confocal overlays (simple opacity scheme, plane highlights)\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.measure import marching_cubes, regionprops_table\n",
    "from scipy.optimize import linear_sum_assignment as _lsa\n",
    "\n",
    "\n",
    "# Anatomy centroids\n",
    "anat_props3d = regionprops_table(twoP_labels_all, properties=('label', 'centroid', 'bbox'))\n",
    "adf3d = pd.DataFrame(anat_props3d).rename(columns={'centroid-0': 'cz', 'centroid-1': 'cy', 'centroid-2': 'cx'})\n",
    "adf3d = adf3d[adf3d['label'] != 0].reset_index(drop=True)\n",
    "anat_centroid_map = {int(r['label']): {'x': float(r['cx']), 'y': float(r['cy']), 'z': float(r['cz'])} for _, r in adf3d.iterrows()}\n",
    "\n",
    "# Median anatomy diameter and distance gate (defaults to 0.5× median diameter unless overridden)\n",
    "try:\n",
    "    adf3d = adf3d.rename(columns={'bbox-0':'zmin','bbox-1':'ymin','bbox-2':'xmin','bbox-3':'zmax','bbox-4':'ymax','bbox-5':'xmax'})\n",
    "    adf3d['diam_um'] = ((adf3d['xmax'] - adf3d['xmin'])*DX + (adf3d['ymax'] - adf3d['ymin'])*DY + (adf3d['zmax'] - adf3d['zmin'])*DZ)/3.0\n",
    "    median_diam_um = float(adf3d['diam_um'].median()) if adf3d['diam_um'].size else 0.0\n",
    "except Exception:\n",
    "    median_diam_um = 0.0\n",
    "\n",
    "MAX_DIST_FUNC_ANAT = float(globals().get('MAX_DIST_FUNC_ANAT', 0.5 * median_diam_um if median_diam_um > 0 else np.inf))\n",
    "\n",
    "# Tunables\n",
    "MIN_OVERLAP_FUNC_ANAT = int(globals().get('MIN_OVERLAP_FUNC_ANAT', 1))\n",
    "REQUIRE_OVERLAP_FUNC_ANAT = bool(globals().get('REQUIRE_OVERLAP_FUNC_ANAT', True))\n",
    "STEP_PL = int(globals().get('STEP_PL', 2))\n",
    "STEP_CONF = int(globals().get('STEP_CONF', STEP_PL))\n",
    "PAIR_LINE_WIDTH_FUNC = int(globals().get('PAIR_LINE_WIDTH_FUNC', 2))\n",
    "PAIR_LINE_COLOR_FUNC = globals().get('PAIR_LINE_COLOR_FUNC', 'gray')\n",
    "PAIR_LINE_WIDTH_CONF = int(globals().get('PAIR_LINE_WIDTH_CONF', 3))\n",
    "PAIR_LINE_COLOR_CONF = globals().get('PAIR_LINE_COLOR_CONF', 'magenta')\n",
    "OPACITY_FUNC_ALL = float(globals().get('OPACITY_FUNC_ALL', 0.05))\n",
    "OPACITY_FUNC_PAIRED = float(globals().get('OPACITY_FUNC_PAIRED', 0.05))\n",
    "OPACITY_ANAT_PAIRED = 0.10\n",
    "OPACITY_CONF_ALL = float(globals().get('OPACITY_CONF_ALL', 0.05))\n",
    "OPACITY_CONF_PLANE = float(globals().get('OPACITY_CONF_PLANE', 0.20))\n",
    "\n",
    "if not plane_refs:\n",
    "    print('No plane_refs available for per-plane viewer.')\n",
    "else:\n",
    "    DZ = float(VOX_ANAT.get('Z', 1.0)) if 'VOX_ANAT' in globals() else 1.0\n",
    "    DY = float(VOX_ANAT.get('Y', 1.0)) if 'VOX_ANAT' in globals() else 1.0\n",
    "    DX = float(VOX_ANAT.get('X', 1.0)) if 'VOX_ANAT' in globals() else 1.0\n",
    "\n",
    "    twoP_labels_all = _ensure_uint_labels(imread_any(ANAT_LABELS_PATH))\n",
    "    z_size = twoP_labels_all.shape[0]\n",
    "    y_size = twoP_labels_all.shape[1]\n",
    "    x_size = twoP_labels_all.shape[2]\n",
    "    z_max_um = (z_size - 1) * DZ\n",
    "    y_max_um = (y_size - 1) * DY\n",
    "    x_max_um = (x_size - 1) * DX\n",
    "\n",
    "    def load_func_for_plane(pr, p_idx):\n",
    "        if '_get_labels_for_plane' in globals():\n",
    "            try:\n",
    "                arr, desc = _get_labels_for_plane(pr, p_idx)\n",
    "                return arr, desc\n",
    "            except Exception:\n",
    "                pass\n",
    "        if '_load_func_labels_for_plane' in globals():\n",
    "            try:\n",
    "                arr, _, desc = globals()['_load_func_labels_for_plane'](p_idx)\n",
    "                return arr, desc\n",
    "            except Exception:\n",
    "                pass\n",
    "        return None, None\n",
    "\n",
    "    def norm_func_labels(arr):\n",
    "        arr = _ensure_uint_labels(arr)\n",
    "        if arr.ndim == 3 and arr.shape[-1] in (3, 4):\n",
    "            arr = arr[..., 0]\n",
    "        if arr.ndim == 3 and arr.shape[0] == 1:\n",
    "            arr = arr[0]\n",
    "        return arr\n",
    "\n",
    "    def build_surface(mask_bool, step):\n",
    "        if mask_bool is None or not np.any(mask_bool):\n",
    "            return (np.array([]),) * 6\n",
    "        try:\n",
    "            verts, faces, _, _ = marching_cubes(mask_bool.astype(np.uint8), level=0.5, spacing=(DZ, DY, DX), step_size=step)\n",
    "            i, j, k = faces.T.astype(np.int32, copy=False)\n",
    "            zc, yc, xc = verts[:, 0], verts[:, 1], verts[:, 2]\n",
    "            return (xc, yc, zc, i, j, k)\n",
    "        except RuntimeError:\n",
    "            return (np.array([]),) * 6\n",
    "\n",
    "    # Confocal overlays: renumber per mask to avoid label collisions\n",
    "    conf_union = None\n",
    "    conf_centroid_map = {}  # new_label -> dict\n",
    "    conf_pairs = []         # dict with new conf_label, anat_label\n",
    "    anat_to_conf = {}       # anat_label -> set(new conf labels)\n",
    "    next_conf = 1\n",
    "    if 'hcr_match_results' in globals() and hcr_match_results:\n",
    "        # build union with remapped labels\n",
    "        conf_union = np.zeros_like(twoP_labels_all, dtype=np.int32)\n",
    "        for mask_idx, res in enumerate(hcr_match_results):\n",
    "            warped = res.get('warped')\n",
    "            df_conf = res.get('df_conf'); df_2p = res.get('df_2p')\n",
    "            fp = res.get('final_pairs')\n",
    "            if warped is None:\n",
    "                continue\n",
    "            label_map = {}\n",
    "            if df_conf is not None:\n",
    "                for lbl, row in df_conf.set_index('label').iterrows():\n",
    "                    label_map[int(lbl)] = next_conf\n",
    "                    conf_centroid_map[next_conf] = {'x': float(row['x']), 'y': float(row['y']), 'z': float(row['z'])}\n",
    "                    next_conf += 1\n",
    "            # remap warped labels\n",
    "            remapped = np.zeros_like(warped, dtype=np.int32)\n",
    "            for orig, newlbl in label_map.items():\n",
    "                remapped = np.where(warped == orig, newlbl, remapped)\n",
    "            conf_union = np.where(remapped > 0, remapped, conf_union)\n",
    "            if fp is None or fp.empty or df_conf is None or df_2p is None:\n",
    "                continue\n",
    "            amap = df_2p.set_index('label')[['x','y','z']].to_dict('index')\n",
    "            for _, r in fp.iterrows():\n",
    "                al = int(r['twoP_label']); cl_orig = int(r['conf_label'])\n",
    "                new_cl = label_map.get(cl_orig)\n",
    "                c = conf_centroid_map.get(new_cl); a = amap.get(al)\n",
    "                if new_cl is None or c is None or a is None:\n",
    "                    continue\n",
    "                dx_um = (c['x'] - float(a['x'])) * DX\n",
    "                dy_um = (c['y'] - float(a['y'])) * DY\n",
    "                dz_um = (c['z'] - float(a['z'])) * DZ\n",
    "                conf_pairs.append({'conf_label': new_cl, 'anat_label': al, 'dist_um': float(np.sqrt(dx_um*dx_um + dy_um*dy_um + dz_um*dz_um))})\n",
    "                anat_to_conf.setdefault(al, set()).add(new_cl)\n",
    "\n",
    "    traces = []\n",
    "    plane_spans = []\n",
    "\n",
    "    # Global confocal mesh (always visible)\n",
    "    conf_trace_idx = None\n",
    "    if conf_union is not None and np.any(conf_union):\n",
    "        xC, yC, zC, iC, jC, kC = build_surface(conf_union > 0, STEP_CONF)\n",
    "        traces.append(go.Mesh3d(x=xC, y=yC, z=zC, i=iC, j=jC, k=kC, name='Confocal masks (all)', color='magenta', opacity=OPACITY_CONF_ALL, visible=True))\n",
    "        conf_trace_idx = 0\n",
    "\n",
    "    for p_idx, pr in enumerate(plane_refs):\n",
    "        lbl = pr.get('label', f'plane{p_idx}')\n",
    "        def _skip(reason):\n",
    "            print(f\"[viewer] skip {lbl}: {reason}\")\n",
    "        bz = int(pr.get('best_z', 0))\n",
    "        if bz < 0 or bz >= z_size:\n",
    "            _skip(f\"best_z {bz} out of bounds (0..{z_size-1})\")\n",
    "            continue\n",
    "\n",
    "        anat_slice = _ensure_uint_labels(twoP_labels_all[bz])\n",
    "\n",
    "        func_raw, desc = load_func_for_plane(pr, p_idx)\n",
    "        if func_raw is None:\n",
    "            _skip('no functional labels for plane')\n",
    "            continue\n",
    "\n",
    "        func_raw = norm_func_labels(func_raw)\n",
    "        if func_raw.ndim != 2:\n",
    "\n",
    "            _skip(f'functional labels shape {func_raw.shape} not 2D')\n",
    "            continue\n",
    "\n",
    "        tform_use = pr.get('tform', globals().get('tform', None))\n",
    "        if tform_use is None:\n",
    "            func_warped = func_raw.copy()\n",
    "        else:\n",
    "            func_warped = resample_labels_nn(func_raw, tform_use, output_shape=anat_slice.shape)\n",
    "        func_warped = _ensure_uint_labels(func_warped)\n",
    "\n",
    "        f_props = regionprops_table(func_warped, properties=('label', 'centroid'))\n",
    "        a_props = regionprops_table(anat_slice, properties=('label', 'centroid'))\n",
    "        fdf = pd.DataFrame(f_props).rename(columns={'centroid-0': 'cy', 'centroid-1': 'cx'})\n",
    "        adf2d = pd.DataFrame(a_props).rename(columns={'centroid-0': 'cy', 'centroid-1': 'cx'})\n",
    "        fdf = fdf[fdf['label'] != 0].reset_index(drop=True)\n",
    "        adf2d = adf2d[adf2d['label'] != 0].reset_index(drop=True)\n",
    "        if fdf.empty:\n",
    "            _skip('functional labels contain no nonzero regions')\n",
    "            continue\n",
    "        if adf2d.empty:\n",
    "            _skip('anatomy slice has no labels')\n",
    "            continue\n",
    "\n",
    "        overlap_df = compute_label_overlap(func_warped, anat_slice, min_overlap_voxels=1)\n",
    "        if overlap_df.empty:\n",
    "            _skip('no func↔anat overlap at this plane')\n",
    "            continue\n",
    "        overlap_df = overlap_df.rename(columns={'conf_label': 'func_label', 'twoP_label': 'anat_label', 'overlap_voxels': 'overlap_px'})\n",
    "\n",
    "        F = fdf[['cx', 'cy']].to_numpy()\n",
    "        A = adf2d[['cx', 'cy']].to_numpy()\n",
    "        D = np.sqrt(((F[:, None, :] - A[None, :, :]) ** 2).sum(axis=2))\n",
    "        row_ind, col_ind = _lsa(D)\n",
    "        links = []\n",
    "        for r, c in zip(row_ind, col_ind):\n",
    "            fxp, fyp = float(F[r, 0]), float(F[r, 1]); axp, ayp = float(A[c, 0]), float(A[c, 1])\n",
    "            dist_px = float(D[r, c])\n",
    "            dx_um = (fxp - axp) * DX; dy_um = (fyp - ayp) * DY\n",
    "            dist_um = float(np.sqrt(dx_um * dx_um + dy_um * dy_um))\n",
    "            links.append({'func_label': int(fdf.iloc[r]['label']), 'anat_label': int(adf2d.iloc[c]['label']), 'fx_px': fxp, 'fy_px': fyp, 'ax_px': axp, 'ay_px': ayp, 'dist_px': dist_px, 'dist_um': dist_um})\n",
    "        links_df = pd.DataFrame(links)\n",
    "        n_func_labels = len(fdf); n_anat_labels = len(adf2d); n_initial = len(links_df)\n",
    "\n",
    "        links_df = links_df.merge(overlap_df, on=['func_label', 'anat_label'], how='left')\n",
    "        links_df['overlap_px'] = links_df['overlap_px'].fillna(0).astype(int)\n",
    "        n_after_overlap = len(links_df)\n",
    "        if REQUIRE_OVERLAP_FUNC_ANAT:\n",
    "            links_df = links_df[links_df['overlap_px'] >= MIN_OVERLAP_FUNC_ANAT]\n",
    "        n_after_overlap_filter = len(links_df)\n",
    "\n",
    "        dist_gate = MAX_DIST_FUNC_ANAT if np.isfinite(MAX_DIST_FUNC_ANAT) and MAX_DIST_FUNC_ANAT > 0 else None\n",
    "        n_before_dist = len(links_df)\n",
    "        if dist_gate is not None:\n",
    "            links_df = links_df[links_df['dist_um'] <= dist_gate]\n",
    "        n_after_dist = len(links_df)\n",
    "\n",
    "        print(f\"[viewer] {lbl}: func={n_func_labels}, anat={n_anat_labels}, matches={n_initial} -> overlap {n_after_overlap_filter}/{n_after_overlap} -> dist {n_after_dist}/{n_before_dist}\")\n",
    "        if links_df.empty:\n",
    "            _skip('no matches after overlap/distance filters')\n",
    "            continue\n",
    "\n",
    "\n",
    "        func_ids = links_df['func_label'].unique()\n",
    "        anat_ids = links_df['anat_label'].unique()\n",
    "\n",
    "        func_mask = np.zeros_like(twoP_labels_all, dtype=bool); func_mask[bz] = func_warped > 0\n",
    "        func_mask_p = np.zeros_like(twoP_labels_all, dtype=bool); func_mask_p[bz] = np.isin(func_warped, func_ids)\n",
    "        anat_mask_p = np.isin(twoP_labels_all, anat_ids)\n",
    "\n",
    "        # Confocal masks paired with this plane's anatomy labels (3D volume) using remapped labels\n",
    "        conf_plane_mask = np.zeros_like(conf_union, dtype=bool)\n",
    "        conf_ids_plane = set()\n",
    "        if anat_to_conf and conf_union is not None:\n",
    "            for al in anat_ids.tolist():\n",
    "                conf_ids_plane.update(anat_to_conf.get(int(al), set()))\n",
    "            if conf_ids_plane:\n",
    "                conf_plane_mask = np.isin(conf_union, list(conf_ids_plane))\n",
    "        if not conf_ids_plane:\n",
    "            print(f\"No conf labels for plane {lbl} (anat_ids {anat_ids.tolist()})\")\n",
    "\n",
    "        xF, yF, zF, iF, jF, kF = build_surface(func_mask, STEP_PL)\n",
    "        xFP, yFP, zFP, iFP, jFP, kFP = build_surface(func_mask_p, STEP_PL)\n",
    "        xAP, yAP, zAP, iAP, jAP, kAP = build_surface(anat_mask_p, STEP_PL)\n",
    "        xCPm, yCPm, zCPm, iCPm, jCPm, kCPm = build_surface(conf_plane_mask, STEP_CONF)\n",
    "\n",
    "        fx_um = links_df['fx_px'].to_numpy() * DX\n",
    "        fy_um = links_df['fy_px'].to_numpy() * DY\n",
    "        fz_um = np.full_like(fx_um, bz * DZ)\n",
    "\n",
    "        ax_um = []\n",
    "        ay_um = []\n",
    "        az_um = []\n",
    "        for lbl in links_df['anat_label']:\n",
    "            c = anat_centroid_map.get(int(lbl))\n",
    "            if c is None:\n",
    "                ax_um.append(np.nan); ay_um.append(np.nan); az_um.append(np.nan)\n",
    "            else:\n",
    "                ax_um.append(c['x'] * DX); ay_um.append(c['y'] * DY); az_um.append(c['z'] * DZ)\n",
    "\n",
    "        fxp = fx_um.tolist(); fyp = fy_um.tolist(); fzp = fz_um.tolist()\n",
    "        axp = ax_um; ayp = ay_um; azp = az_um\n",
    "        lx = []; ly = []; lz = []; htext = []\n",
    "        for fxu, fyu, axu, ayu, azu, du, ov in zip(fx_um, fy_um, ax_um, ay_um, az_um, links_df['dist_um'].to_numpy(), links_df['overlap_px'].to_numpy()):\n",
    "            if np.isnan(axu) or np.isnan(ayu) or np.isnan(azu):\n",
    "                continue\n",
    "            lx += [fxu, axu, None]; ly += [fyu, ayu, None]; lz += [bz * DZ, azu, None]\n",
    "            htext.append(f\"d={du:.2f} µm | ov={int(ov)} px\")\n",
    "\n",
    "        # Confocal centroids/links for anatomy labels in this plane (final_pairs only, remapped labels)\n",
    "        conf_x = []; conf_y = []; conf_z = []; conf_lx = []; conf_ly = []; conf_lz = []; conf_text = []\n",
    "        if conf_pairs and conf_centroid_map:\n",
    "            anat_id_set = set(anat_ids.tolist())\n",
    "            for pair in conf_pairs:\n",
    "                if pair['anat_label'] not in anat_id_set:\n",
    "                    continue\n",
    "                ccent = conf_centroid_map.get(pair['conf_label'])\n",
    "                acent = anat_centroid_map.get(pair['anat_label'])\n",
    "                if ccent is None or acent is None:\n",
    "                    continue\n",
    "                cx_um = ccent['x'] * DX; cy_um = ccent['y'] * DY; cz_um = ccent['z'] * DZ\n",
    "                ax_um_c = acent['x'] * DX; ay_um_c = acent['y'] * DY; az_um_c = acent['z'] * DZ\n",
    "                conf_x.append(cx_um); conf_y.append(cy_um); conf_z.append(cz_um)\n",
    "                conf_lx += [cx_um, ax_um_c, None]; conf_ly += [cy_um, ay_um_c, None]; conf_lz += [cz_um, az_um_c, None]\n",
    "                conf_text.append(f\"conf {pair['conf_label']} ↔ anat {pair['anat_label']}\")\n",
    "\n",
    "        start = len(traces)\n",
    "        traces.extend([\n",
    "            go.Mesh3d(x=xF, y=yF, z=zF, i=iF, j=jF, k=kF, name=f\"Func {lbl} (all)\", color='#99ffcc', opacity=OPACITY_FUNC_ALL, visible=False),\n",
    "            go.Mesh3d(x=xFP, y=yFP, z=zFP, i=iFP, j=jFP, k=kFP, name=f\"Func {lbl} (paired)\", color='#00cc88', opacity=OPACITY_FUNC_PAIRED, visible=False),\n",
    "            go.Mesh3d(x=xAP, y=yAP, z=zAP, i=iAP, j=jAP, k=kAP, name=f\"Anat paired (3D)\", color='#00b7ff', opacity=OPACITY_ANAT_PAIRED, visible=False),\n",
    "            go.Scatter3d(x=fxp, y=fyp, z=fzp, mode='markers', name='Func centroids', marker=dict(size=3, color='#00cc88'), visible=False),\n",
    "            go.Scatter3d(x=axp, y=ayp, z=azp, mode='markers', name='Anat centroids (3D)', marker=dict(size=3, color='#00b7ff'), visible=False),\n",
    "            go.Scatter3d(x=lx, y=ly, z=lz, mode='lines', name='Pairs', line=dict(color=PAIR_LINE_COLOR_FUNC, width=PAIR_LINE_WIDTH_FUNC), hoverinfo='text', text=htext, visible=False),\n",
    "            go.Scatter3d(x=conf_x, y=conf_y, z=conf_z, mode='markers', name='Conf centroids', marker=dict(size=3, color='magenta'), visible=False),\n",
    "            go.Scatter3d(x=conf_lx, y=conf_ly, z=conf_lz, mode='lines', name='Conf↔Anat links', line=dict(color=PAIR_LINE_COLOR_CONF, width=PAIR_LINE_WIDTH_CONF), hoverinfo='text', text=conf_text, visible=False),\n",
    "            go.Mesh3d(x=xCPm, y=yCPm, z=zCPm, i=iCPm, j=jCPm, k=kCPm, name='Conf paired (plane)', color='red', opacity=OPACITY_CONF_PLANE, visible=False),\n",
    "        ])\n",
    "        plane_spans.append((lbl, start, bz))\n",
    "\n",
    "    if plane_spans:\n",
    "        total = len(traces)\n",
    "        init_vis = [False] * total\n",
    "        if conf_trace_idx is not None:\n",
    "            init_vis[conf_trace_idx] = True\n",
    "        lbl0, s0, bz0 = plane_spans[0]\n",
    "        init_vis[s0:s0 + 9] = [True] * 9\n",
    "        for i, tr in enumerate(traces):\n",
    "            tr.visible = init_vis[i]\n",
    "\n",
    "        buttons = []\n",
    "        for lbl, start, bz in plane_spans:\n",
    "            vis = [False] * total\n",
    "            if conf_trace_idx is not None:\n",
    "                vis[conf_trace_idx] = True\n",
    "            vis[start:start + 9] = [True] * 9\n",
    "            buttons.append(dict(label=lbl, method='update', args=[{'visible': vis}, {'title': f'{lbl} @ Z={bz}'}]))\n",
    "\n",
    "        fig = go.Figure(data=traces)\n",
    "        fig.update_layout(\n",
    "            width=1500, height=1250,\n",
    "            title=f'{lbl0} @ Z={bz0}',\n",
    "            scene=dict(xaxis=dict(title='X (µm)', range=[0, x_max_um]), yaxis=dict(title='Y (µm)', range=[0, y_max_um]), zaxis=dict(title='Z (µm)', range=[0, z_max_um]), aspectmode='data'),\n",
    "            updatemenus=[dict(buttons=buttons, direction='down')]\n",
    "        )\n",
    "        fig.show()\n",
    "    else:\n",
    "        print('No plane data to plot (missing masks or matches).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8958dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Verify anatomy label volume matches HCR matching\n",
    "import numpy as np\n",
    "anat = imread_any(ANAT_LABELS_PATH)\n",
    "print(\"viewer anat shape:\", anat.shape)\n",
    "print(\"HCR matched shape (first res):\", hcr_match_results[0]['warped'].shape)\n",
    "\n",
    "# 2) Recompute distances from stored centroids vs final_pairs distances\n",
    "res = hcr_match_results[0]\n",
    "df_conf = res['df_conf']; df_2p = res['df_2p']; fp = res['final_pairs']\n",
    "DX=float(VOX_ANAT['X']); DY=float(VOX_ANAT['Y']); DZ=float(VOX_ANAT['Z'])\n",
    "cmap = df_conf.set_index('label')[['x','y','z']].to_dict('index')\n",
    "amap = df_2p.set_index('label')[['x','y','z']].to_dict('index')\n",
    "d_re = []\n",
    "for _, r in fp.iterrows():\n",
    "    c = cmap.get(int(r.conf_label)); a = amap.get(int(r.twoP_label))\n",
    "    if c and a:\n",
    "        dx=(c['x']-a['x'])*DX; dy=(c['y']-a['y'])*DY; dz=(c['z']-a['z'])*DZ\n",
    "        d_re.append(np.sqrt(dx*dx+dy*dy+dz*dz))\n",
    "print(\"stored median dist_um:\", fp['distance_um'].median())\n",
    "print(\"recomputed median (vox*VOX_ANAT):\", np.median(d_re))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3ab018",
   "metadata": {},
   "outputs": [],
   "source": [
    "### [48] HCR→2P centroid distances (all final_pairs)\n",
    "# HCR → 2P anatomy centroid distances (final_pairs)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "if not globals().get('hcr_match_results'):\n",
    "    print('No hcr_match_results available.')\n",
    "else:\n",
    "    frames = []\n",
    "    for res in hcr_match_results:\n",
    "        fp = res.get('final_pairs')\n",
    "        if fp is None or fp.empty:\n",
    "            continue\n",
    "        name = Path(res.get('mask_path', 'unknown')).name\n",
    "        gene = gene_from_mask(res.get('mask_path', 'unknown'))\n",
    "        name = gene\n",
    "        d = fp['distance_um'].to_numpy(dtype=float)\n",
    "        frames.append(pd.DataFrame({'distance_um': d, 'mask': name}))\n",
    "    if not frames:\n",
    "        print('No final_pairs to plot.')\n",
    "    else:\n",
    "        df = pd.concat(frames, ignore_index=True)\n",
    "        stats = df['distance_um'].describe(percentiles=[0.5, 0.9, 0.95])\n",
    "        print(f\"n={len(df)} | median={stats['50%']:.2f} µm | mean={stats['mean']:.2f} µm | p90={stats['90%']:.2f} µm | max={stats['max']:.2f} µm\")\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        axes[0].hist(df['distance_um'], bins=50, color='gray', edgecolor='black')\n",
    "        axes[0].set_xlabel('distance (µm)'); axes[0].set_ylabel('count'); axes[0].set_title('All pairs')\n",
    "        mask_counts = df.groupby('mask')['distance_um'].size()\n",
    "        order = sorted(mask_counts.index)\n",
    "        df.boxplot(column='distance_um', by='mask', ax=axes[1], rot=90)\n",
    "        axes[1].set_ylabel('distance (µm)'); axes[1].set_title('By mask')\n",
    "        axes[1].set_xticklabels([f\"{m} (n={int(mask_counts.get(m,0))})\" for m in order], rotation=90)\n",
    "        plt.suptitle('')\n",
    "        plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a66046",
   "metadata": {},
   "outputs": [],
   "source": [
    "### [49] Confocal↔Anatomy offsets (anat labels intersect functional best_z)\n",
    "# Confocal↔Anatomy centroid offsets (XY plane and Z) for pairs whose anatomy labels match any functional plane; gene presence table\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from skimage.measure import regionprops_table\n",
    "from scipy.optimize import linear_sum_assignment as _lsa\n",
    "\n",
    "if not plane_refs or not globals().get('hcr_match_results'):\n",
    "    print('Need plane_refs and hcr_match_results; skipping.')\n",
    "else:\n",
    "    DZ = float(VOX_ANAT.get('Z', 1.0)) if 'VOX_ANAT' in globals() else 1.0\n",
    "    DY = float(VOX_ANAT.get('Y', 1.0)) if 'VOX_ANAT' in globals() else 1.0\n",
    "    DX = float(VOX_ANAT.get('X', 1.0)) if 'VOX_ANAT' in globals() else 1.0\n",
    "    MIN_OVERLAP_FUNC_ANAT = int(globals().get('MIN_OVERLAP_FUNC_ANAT', 1))\n",
    "    REQUIRE_OVERLAP_FUNC_ANAT = bool(globals().get('REQUIRE_OVERLAP_FUNC_ANAT', True))\n",
    "    MAX_DIST_FUNC_ANAT = float(globals().get('MAX_DIST_FUNC_ANAT', np.inf))\n",
    "\n",
    "    twoP_labels_all = _ensure_uint_labels(imread_any(ANAT_LABELS_PATH))\n",
    "    z_size = twoP_labels_all.shape[0]\n",
    "    try:\n",
    "        props3d = regionprops_table(twoP_labels_all, properties=('label','bbox'))\n",
    "        adf3d = pd.DataFrame(props3d)\n",
    "        adf3d = adf3d[adf3d['label'] != 0]\n",
    "        adf3d = adf3d.rename(columns={'bbox-0':'zmin','bbox-1':'ymin','bbox-2':'xmin','bbox-3':'zmax','bbox-4':'ymax','bbox-5':'xmax'})\n",
    "        med_dx_um = float(((adf3d['xmax'] - adf3d['xmin']) * DX).median()) if not adf3d.empty else 0.0\n",
    "        med_dy_um = float(((adf3d['ymax'] - adf3d['ymin']) * DY).median()) if not adf3d.empty else 0.0\n",
    "        med_dz_um = float(((adf3d['zmax'] - adf3d['zmin']) * DZ).median()) if not adf3d.empty else 0.0\n",
    "    except Exception:\n",
    "        med_dx_um = med_dy_um = med_dz_um = 0.0\n",
    "\n",
    "    def load_func_for_plane(pr, p_idx):\n",
    "        if '_get_labels_for_plane' in globals():\n",
    "            try:\n",
    "                arr, desc = _get_labels_for_plane(pr, p_idx)\n",
    "                return arr, desc\n",
    "            except Exception:\n",
    "                pass\n",
    "        if '_load_func_labels_for_plane' in globals():\n",
    "            try:\n",
    "                arr, _, desc = globals()['_load_func_labels_for_plane'](p_idx)\n",
    "                return arr, desc\n",
    "            except Exception:\n",
    "                pass\n",
    "        return None, None\n",
    "\n",
    "    def norm_func_labels(arr):\n",
    "        arr = _ensure_uint_labels(arr)\n",
    "        if arr.ndim == 3 and arr.shape[-1] in (3, 4):\n",
    "            arr = arr[..., 0]\n",
    "        if arr.ndim == 3 and arr.shape[0] == 1:\n",
    "            arr = arr[0]\n",
    "        return arr\n",
    "\n",
    "    anat_ids_matched = set()\n",
    "    plane_anat_ids = {}\n",
    "\n",
    "    # Collect anatomy labels that match functional planes (with overlap/distance gates)\n",
    "    for p_idx, pr in enumerate(plane_refs):\n",
    "        bz = int(pr.get('best_z', 0))\n",
    "        if bz < 0 or bz >= z_size:\n",
    "            continue\n",
    "        plabel = pr.get('label', f'plane{p_idx}')\n",
    "        anat_slice = _ensure_uint_labels(twoP_labels_all[bz])\n",
    "        func_raw, desc = load_func_for_plane(pr, p_idx)\n",
    "        if func_raw is None:\n",
    "            continue\n",
    "        func_raw = norm_func_labels(func_raw)\n",
    "        if func_raw.ndim != 2:\n",
    "            continue\n",
    "        tform_use = pr.get('tform', globals().get('tform', None))\n",
    "        if tform_use is None:\n",
    "            func_warped = func_raw.copy()\n",
    "        else:\n",
    "            func_warped = resample_labels_nn(func_raw, tform_use, output_shape=anat_slice.shape)\n",
    "        func_warped = _ensure_uint_labels(func_warped)\n",
    "\n",
    "        f_props = regionprops_table(func_warped, properties=('label','centroid'))\n",
    "        a_props = regionprops_table(anat_slice, properties=('label','centroid'))\n",
    "        fdf = pd.DataFrame(f_props).rename(columns={'centroid-0':'cy','centroid-1':'cx'})\n",
    "        adf2d = pd.DataFrame(a_props).rename(columns={'centroid-0':'cy','centroid-1':'cx'})\n",
    "        fdf = fdf[fdf['label'] != 0].reset_index(drop=True)\n",
    "        adf2d = adf2d[adf2d['label'] != 0].reset_index(drop=True)\n",
    "        if fdf.empty or adf2d.empty:\n",
    "            continue\n",
    "\n",
    "        overlap_df = compute_label_overlap(func_warped, anat_slice, min_overlap_voxels=1)\n",
    "        if overlap_df.empty:\n",
    "            continue\n",
    "        overlap_df = overlap_df.rename(columns={'conf_label':'func_label','twoP_label':'anat_label','overlap_voxels':'overlap_px'})\n",
    "\n",
    "        F = fdf[['cx','cy']].to_numpy(); A = adf2d[['cx','cy']].to_numpy()\n",
    "        D = np.sqrt(((F[:,None,:]-A[None,:,:])**2).sum(axis=2))\n",
    "        row_ind, col_ind = _lsa(D)\n",
    "        links = []\n",
    "        for r,c in zip(row_ind, col_ind):\n",
    "            fxp, fyp = float(F[r,0]), float(F[r,1]); axp, ayp = float(A[c,0]), float(A[c,1])\n",
    "            dist_px = float(D[r,c]); dx_um = (fxp-axp)*DX; dy_um=(fyp-ayp)*DY\n",
    "            dist_um = float(np.sqrt(dx_um*dx_um + dy_um*dy_um))\n",
    "            links.append({'func_label': int(fdf.iloc[r]['label']), 'anat_label': int(adf2d.iloc[c]['label']), 'dist_um': dist_um})\n",
    "        links_df = pd.DataFrame(links)\n",
    "        if links_df.empty:\n",
    "            continue\n",
    "        links_df = links_df.merge(overlap_df, on=['func_label','anat_label'], how='left')\n",
    "        links_df['overlap_px'] = links_df['overlap_px'].fillna(0).astype(int)\n",
    "        if REQUIRE_OVERLAP_FUNC_ANAT:\n",
    "            links_df = links_df[links_df['overlap_px'] >= MIN_OVERLAP_FUNC_ANAT]\n",
    "        if np.isfinite(MAX_DIST_FUNC_ANAT) and MAX_DIST_FUNC_ANAT > 0:\n",
    "            links_df = links_df[links_df['dist_um'] <= MAX_DIST_FUNC_ANAT]\n",
    "        if links_df.empty:\n",
    "            continue\n",
    "        anat_ids_matched.update(links_df['anat_label'].tolist())\n",
    "        plane_anat_ids[plabel] = set(links_df['anat_label'].tolist())\n",
    "\n",
    "    if not anat_ids_matched:\n",
    "        print('No anatomy labels matched to functional planes; nothing to filter.')\n",
    "    else:\n",
    "        rows = []\n",
    "        for res in hcr_match_results:\n",
    "            fp = res.get('final_pairs')\n",
    "            if fp is None or fp.empty:\n",
    "                continue\n",
    "            df_conf = res.get('df_conf'); df_2p = res.get('df_2p')\n",
    "            if df_conf is None or df_2p is None:\n",
    "                continue\n",
    "            cmap = df_conf.set_index('label')[['x','y','z']].to_dict('index')\n",
    "            amap = df_2p.set_index('label')[['x','y','z']].to_dict('index')\n",
    "            sub = fp[fp['twoP_label'].isin(anat_ids_matched)]\n",
    "            if sub.empty:\n",
    "                continue\n",
    "            name = Path(res.get('mask_path','unknown')).name\n",
    "            for _, r in sub.iterrows():\n",
    "                cl = int(r['conf_label']); al = int(r['twoP_label'])\n",
    "                c = cmap.get(cl); a = amap.get(al)\n",
    "                if c is None or a is None:\n",
    "                    continue\n",
    "                dx_um = (float(c['x']) - float(a['x'])) * DX\n",
    "                dy_um = (float(c['y']) - float(a['y'])) * DY\n",
    "                dz_um = (float(c['z']) - float(a['z'])) * DZ\n",
    "                rows.append({'mask': name, 'dx_um': dx_um, 'dy_um': dy_um, 'dz_um': dz_um, 'anat_label': al})\n",
    "        if not rows:\n",
    "            print('No confocal pairs intersecting functional-matched anatomy labels.')\n",
    "        else:\n",
    "            df = pd.DataFrame(rows)\n",
    "            df['|dx_um|'] = df['dx_um'].abs(); df['|dy_um|'] = df['dy_um'].abs(); df['|dz_um|'] = df['dz_um'].abs()\n",
    "            df['xy_um'] = np.sqrt(df['dx_um']*df['dx_um'] + df['dy_um']*df['dy_um'])\n",
    "            med_off = {'xy_um': df['xy_um'].median(), '|dz_um|': df['|dz_um|'].median()}\n",
    "            med_dxy_um = float(np.sqrt(med_dx_um*med_dx_um + med_dy_um*med_dy_um)) if (med_dx_um > 0 or med_dy_um > 0) else 0.0\n",
    "            n_rows = len(df)\n",
    "            print(f\"n={n_rows} | median XY={med_off['xy_um']:.2f} µm | median |dz|={med_off['|dz_um|']:.2f} µm\")\n",
    "            vals_xy = df['xy_um'].to_numpy(dtype=float)\n",
    "            vals_z = df['|dz_um|'].to_numpy(dtype=float)\n",
    "\n",
    "            all_vals = np.concatenate([a for a in (vals_xy, vals_z) if a.size]) if any((vals_xy.size, vals_z.size)) else np.array([])\n",
    "            y_max_data = float(np.max(all_vals)) if all_vals.size else 10.0\n",
    "            from math import ceil as _ceil\n",
    "            y_max = max(10.0, 10.0 * _ceil(y_max_data / 10.0))\n",
    "            yticks = np.arange(0.0, y_max + 0.1, 10.0)\n",
    "\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 4.2), sharey=True)\n",
    "            plots = [\n",
    "                ('XY plane offsets', vals_xy, med_off['xy_um'], med_dxy_um, 'XY plane'),\n",
    "                ('Z offsets', vals_z, med_off['|dz_um|'], med_dz_um, 'Z axis')\n",
    "            ]\n",
    "            for ax, (title, vals, med_val, diam_ref, tick_label) in zip(axes, plots):\n",
    "                if vals.size == 0:\n",
    "                    ax.set_visible(False)\n",
    "                    continue\n",
    "                parts = ax.violinplot([vals], showmeans=False, showmedians=False, showextrema=False)\n",
    "                for pc in parts['bodies']:\n",
    "                    pc.set_facecolor('#cccccc'); pc.set_edgecolor('black'); pc.set_alpha(0.7)\n",
    "                ax.scatter([1], [med_val], color='crimson', zorder=3, s=26)\n",
    "                ax.text(1.18, med_val + 0.02 * y_max, f\"median={med_val:.2f} µm\\nn={len(vals)}\",\n",
    "                        va='bottom', ha='left', fontsize=8,\n",
    "                        bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1.0),\n",
    "                        clip_on=False, zorder=4)\n",
    "                ax.axhline(med_val, color='blue', linestyle='-', linewidth=1.5, label=f\"Median offset {med_val:.2f} µm\")\n",
    "                if diam_ref > 0:\n",
    "                    ax.axhline(diam_ref, color='red', linestyle='--', linewidth=1.2, label=f\"Median 2P diam {diam_ref:.2f} µm\")\n",
    "                ax.set_ylabel('offset (µm)')\n",
    "                ax.set_title(title)\n",
    "                ax.set_xticks([1]); ax.set_xticklabels([tick_label])\n",
    "                ax.set_ylim(0, y_max); ax.set_yticks(yticks)\n",
    "                ax.grid(axis='y', alpha=0.2)\n",
    "                ax.legend(loc='upper right')\n",
    "            fig.suptitle('Confocal↔Anatomy centroid offsets — XY vs Z (anat labels matched to functional planes)')\n",
    "            plt.tight_layout(); plt.show()\n",
    "\n",
    "            # Gene/marker presence per plane\n",
    "            def gene_from_mask(path_str):\n",
    "                name = Path(path_str).name\n",
    "                import re\n",
    "                m = re.search(r\"channel\\d+_(.+?)_cp_masks\", name)\n",
    "                gene = m.group(1) if m else name\n",
    "                return gene.replace('sst1_', 'sst1.')\n",
    "\n",
    "            planes = []\n",
    "            for p_idx, pr in enumerate(plane_refs):\n",
    "                plabel = pr.get('label', f'plane{p_idx}')\n",
    "                anat_ids_plane = plane_anat_ids.get(plabel, set())\n",
    "                genes = set()\n",
    "                for res in hcr_match_results:\n",
    "                    fp = res.get('final_pairs')\n",
    "                    if fp is None or fp.empty:\n",
    "                        continue\n",
    "                    sub = fp[fp['twoP_label'].isin(anat_ids_plane)]\n",
    "                    if sub.empty:\n",
    "                        continue\n",
    "                    gene = gene_from_mask(res.get('mask_path','unknown'))\n",
    "                    genes.add(gene)\n",
    "                planes.append({'plane': plabel, 'genes': ', '.join(sorted(genes)) if genes else '-', 'anat_labels': len(anat_ids_plane)})\n",
    "            if planes:\n",
    "                plane_df = pd.DataFrame(planes)\n",
    "                print('Genes present per plane (anat labels that match functional planes):')\n",
    "                display(plane_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d9918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [50]\n",
    "# Save confocal↔functional pairings via anatomy labels (using functional matches per plane + HCR final_pairs)\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from skimage.measure import regionprops_table\n",
    "from scipy.optimize import linear_sum_assignment as _lsa\n",
    "\n",
    "OUT_PATH = OUT_REG / 'conf_to_func_pairs.csv'\n",
    "\n",
    "if not plane_refs or not globals().get('hcr_match_results'):\n",
    "    print('Need plane_refs and hcr_match_results; skipping save.')\n",
    "else:\n",
    "    DZ = float(VOX_ANAT.get('Z', 1.0)) if 'VOX_ANAT' in globals() else 1.0\n",
    "    DY = float(VOX_ANAT.get('Y', 1.0)) if 'VOX_ANAT' in globals() else 1.0\n",
    "    DX = float(VOX_ANAT.get('X', 1.0)) if 'VOX_ANAT' in globals() else 1.0\n",
    "    MIN_OVERLAP_FUNC_ANAT = int(globals().get('MIN_OVERLAP_FUNC_ANAT', 1))\n",
    "    REQUIRE_OVERLAP_FUNC_ANAT = bool(globals().get('REQUIRE_OVERLAP_FUNC_ANAT', True))\n",
    "    MAX_DIST_FUNC_ANAT = float(globals().get('MAX_DIST_FUNC_ANAT', np.inf))\n",
    "\n",
    "    twoP_labels_all = _ensure_uint_labels(imread_any(ANAT_LABELS_PATH))\n",
    "    z_size = twoP_labels_all.shape[0]\n",
    "\n",
    "    def load_func_for_plane(pr, p_idx):\n",
    "        if '_get_labels_for_plane' in globals():\n",
    "            try:\n",
    "                arr, desc = _get_labels_for_plane(pr, p_idx)\n",
    "                return arr, desc\n",
    "            except Exception:\n",
    "                pass\n",
    "        if '_load_func_labels_for_plane' in globals():\n",
    "            try:\n",
    "                arr, _, desc = globals()['_load_func_labels_for_plane'](p_idx)\n",
    "                return arr, desc\n",
    "            except Exception:\n",
    "                pass\n",
    "        return None, None\n",
    "\n",
    "    def norm_func_labels(arr):\n",
    "        arr = _ensure_uint_labels(arr)\n",
    "        if arr.ndim == 3 and arr.shape[-1] in (3, 4):\n",
    "            arr = arr[..., 0]\n",
    "        if arr.ndim == 3 and arr.shape[0] == 1:\n",
    "            arr = arr[0]\n",
    "        return arr\n",
    "\n",
    "    # Collect best functional→anatomy matches per plane\n",
    "    func_pairs = []  # rows with plane, plane_label, func_label, anat_label, dist_um\n",
    "    for p_idx, pr in enumerate(plane_refs):\n",
    "        bz = int(pr.get('best_z', 0))\n",
    "        if bz < 0 or bz >= z_size:\n",
    "            continue\n",
    "        plabel = pr.get('label', f'plane{p_idx}')\n",
    "        anat_slice = _ensure_uint_labels(twoP_labels_all[bz])\n",
    "        func_raw, desc = load_func_for_plane(pr, p_idx)\n",
    "        if func_raw is None:\n",
    "            continue\n",
    "        func_raw = norm_func_labels(func_raw)\n",
    "        if func_raw.ndim != 2:\n",
    "            continue\n",
    "        tform_use = pr.get('tform', globals().get('tform', None))\n",
    "        if tform_use is None:\n",
    "            func_warped = func_raw.copy()\n",
    "        else:\n",
    "            func_warped = resample_labels_nn(func_raw, tform_use, output_shape=anat_slice.shape)\n",
    "        func_warped = _ensure_uint_labels(func_warped)\n",
    "\n",
    "        f_props = regionprops_table(func_warped, properties=('label','centroid'))\n",
    "        a_props = regionprops_table(anat_slice, properties=('label','centroid'))\n",
    "        fdf = pd.DataFrame(f_props).rename(columns={'centroid-0':'cy','centroid-1':'cx'})\n",
    "        adf = pd.DataFrame(a_props).rename(columns={'centroid-0':'cy','centroid-1':'cx'})\n",
    "        fdf = fdf[fdf['label'] != 0].reset_index(drop=True)\n",
    "        adf = adf[adf['label'] != 0].reset_index(drop=True)\n",
    "        if fdf.empty or adf.empty:\n",
    "            continue\n",
    "        overlap_df = compute_label_overlap(func_warped, anat_slice, min_overlap_voxels=1)\n",
    "        if overlap_df.empty:\n",
    "            continue\n",
    "        overlap_df = overlap_df.rename(columns={'conf_label':'func_label','twoP_label':'anat_label','overlap_voxels':'overlap_px'})\n",
    "        F = fdf[['cx','cy']].to_numpy(); A = adf[['cx','cy']].to_numpy()\n",
    "        D = np.sqrt(((F[:,None,:]-A[None,:,:])**2).sum(axis=2))\n",
    "        row_ind, col_ind = _lsa(D)\n",
    "        links = []\n",
    "        for r,c in zip(row_ind, col_ind):\n",
    "            fxp, fyp = float(F[r,0]), float(F[r,1]); axp, ayp = float(A[c,0]), float(A[c,1])\n",
    "            dist_px = float(D[r,c]); dx_um = (fxp-axp)*DX; dy_um=(fyp-ayp)*DY\n",
    "            dist_um = float(np.sqrt(dx_um*dx_um + dy_um*dy_um))\n",
    "            links.append({'plane': p_idx, 'plane_label': plabel, 'func_label': int(fdf.iloc[r]['label']), 'anat_label': int(adf.iloc[c]['label']), 'dist_um_func_anat': dist_um})\n",
    "        links_df = pd.DataFrame(links)\n",
    "        if links_df.empty:\n",
    "            continue\n",
    "        links_df = links_df.merge(overlap_df, on=['func_label','anat_label'], how='left')\n",
    "        links_df['overlap_px'] = links_df['overlap_px'].fillna(0).astype(int)\n",
    "        if REQUIRE_OVERLAP_FUNC_ANAT:\n",
    "            links_df = links_df[links_df['overlap_px'] >= MIN_OVERLAP_FUNC_ANAT]\n",
    "        if np.isfinite(MAX_DIST_FUNC_ANAT) and MAX_DIST_FUNC_ANAT > 0:\n",
    "            links_df = links_df[links_df['dist_um_func_anat'] <= MAX_DIST_FUNC_ANAT]\n",
    "        if links_df.empty:\n",
    "            continue\n",
    "        func_pairs.append(links_df)\n",
    "\n",
    "    if not func_pairs:\n",
    "        print('No functional→anatomy pairs after gating; nothing to save.')\n",
    "    else:\n",
    "        func_pairs_df = pd.concat(func_pairs, ignore_index=True)\n",
    "        rows = []\n",
    "        for res in hcr_match_results:\n",
    "            fp = res.get('final_pairs')\n",
    "            if fp is None or fp.empty:\n",
    "                continue\n",
    "            mask_path = res.get('mask_path','unknown')\n",
    "            for _, r in fp.iterrows():\n",
    "                al = int(r['twoP_label']); cl = int(r['conf_label']); dist_conf_anat = float(r.get('distance_um', np.nan))\n",
    "                func_candidates = func_pairs_df[func_pairs_df['anat_label'] == al]\n",
    "                if func_candidates.empty:\n",
    "                    rows.append({'conf_label': cl, 'conf_mask': mask_path, 'anat_label': al, 'func_label': None, 'plane': None, 'plane_label': None, 'dist_conf_anat_um': dist_conf_anat, 'dist_func_anat_um': None})\n",
    "                else:\n",
    "                    for _, fr in func_candidates.iterrows():\n",
    "                        rows.append({'conf_label': cl, 'conf_mask': mask_path, 'anat_label': al, 'func_label': int(fr['func_label']), 'plane': int(fr['plane']), 'plane_label': fr['plane_label'], 'dist_conf_anat_um': dist_conf_anat, 'dist_func_anat_um': float(fr['dist_um_func_anat'])})\n",
    "        if not rows:\n",
    "            print('No confocal→functional mappings to save.')\n",
    "        else:\n",
    "            out_df = pd.DataFrame(rows)\n",
    "            out_df.to_csv(OUT_PATH, index=False)\n",
    "            print(f'Saved confocal→functional mapping to {OUT_PATH} (rows={len(out_df)})')\n",
    "            display(out_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3764594b",
   "metadata": {},
   "source": [
    "### [50a] Inspect gene-specific mapping counts (confocal → anatomy → functional)\n",
    "Identify whether extra functional ROIs come from many-to-one anatomy/confocal mappings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd8372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [50a]\n",
    "# Inspect gene-specific mapping counts (confocal → anatomy → functional)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "GENE_OF_INTEREST = globals().get('GENE_OF_INTEREST', 'pth2')\n",
    "CONF_FUNC_CSV = globals().get('CONF_FUNC_CSV', str(OUT_REG / 'conf_to_func_pairs.csv'))\n",
    "TOP_K = int(globals().get('TOP_K', 10))\n",
    "\n",
    "def _pick_plane_col(df):\n",
    "    for col in ('plane', 'plane_ref', 'plane_idx', 'plane_index'):\n",
    "        if col in df.columns:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def _uniq_sorted(vals):\n",
    "    vals = [v for v in pd.unique(vals) if pd.notna(v)]\n",
    "    try:\n",
    "        return sorted(int(v) for v in vals)\n",
    "    except Exception:\n",
    "        return sorted(vals)\n",
    "\n",
    "def _split_plane_diag(df, label_col, label_name, plane_col):\n",
    "    if plane_col is None:\n",
    "        print(f\"No plane column found; can't assess cross-plane splits for {label_name}.\")\n",
    "        return\n",
    "    tmp = df.dropna(subset=[label_col, 'func_key', plane_col]).copy()\n",
    "    if tmp.empty:\n",
    "        return\n",
    "    diag = (tmp.groupby(label_col)\n",
    "            .agg(n_func=('func_key', 'nunique'),\n",
    "                 n_planes=(plane_col, 'nunique'),\n",
    "                 planes=(plane_col, _uniq_sorted))\n",
    "            .reset_index())\n",
    "    diag = diag[diag['n_func'] > 1]\n",
    "    if diag.empty:\n",
    "        return\n",
    "    cross = diag[diag['n_planes'] > 1]\n",
    "    within = diag[diag['n_planes'] == 1]\n",
    "    print(f\"Split diagnostic for {label_name} (labels -> multiple functional ROIs):\")\n",
    "    print(f\"  total split labels: {len(diag)}\")\n",
    "    print(f\"  cross-plane splits: {len(cross)}\")\n",
    "    print(f\"  within-plane splits: {len(within)}\")\n",
    "    display(diag.sort_values(['n_planes', 'n_func'], ascending=[False, False]).head(TOP_K))\n",
    "\n",
    "if not Path(CONF_FUNC_CSV).exists():\n",
    "    print(f'Missing conf_to_func_pairs.csv: {CONF_FUNC_CSV} (run [50]).')\n",
    "else:\n",
    "    df = pd.read_csv(CONF_FUNC_CSV)\n",
    "    if df.empty:\n",
    "        print('conf_to_func_pairs.csv is empty.')\n",
    "    else:\n",
    "        if 'gene' not in df.columns:\n",
    "            if 'conf_mask' in df.columns:\n",
    "                if 'gene_from_mask' in globals():\n",
    "                    df['gene'] = df['conf_mask'].apply(gene_from_mask)\n",
    "                else:\n",
    "                    def _gene_from_mask(path_str):\n",
    "                        name = Path(str(path_str)).name\n",
    "                        m = re.search(r'channel\\d+_(.+?)_cp_masks', name)\n",
    "                        gene = m.group(1) if m else name\n",
    "                        return gene.replace('sst1_', 'sst1.')\n",
    "                    df['gene'] = df['conf_mask'].apply(_gene_from_mask)\n",
    "            else:\n",
    "                df['gene'] = 'unknown'\n",
    "\n",
    "        sub = df[df['gene'] == GENE_OF_INTEREST].copy()\n",
    "        if sub.empty:\n",
    "            print(f'No rows for gene: {GENE_OF_INTEREST}')\n",
    "        else:\n",
    "            plane_col = _pick_plane_col(sub)\n",
    "\n",
    "            def _func_key(row):\n",
    "                plane_val = row.get(plane_col) if plane_col else row.get('plane')\n",
    "                if pd.notna(plane_val) and pd.notna(row.get('func_label')):\n",
    "                    return f\"{int(plane_val)}:{int(row['func_label'])}\"\n",
    "                return None\n",
    "\n",
    "            # Count distinct entities\n",
    "            sub['func_key'] = sub.apply(_func_key, axis=1)\n",
    "            conf_key = None\n",
    "            if 'conf_label' in sub.columns and 'conf_mask' in sub.columns:\n",
    "                sub['conf_key'] = sub.apply(lambda r: f\"{Path(str(r['conf_mask'])).name}:{int(r['conf_label'])}\", axis=1)\n",
    "                conf_key = 'conf_key'\n",
    "            elif 'conf_label' in sub.columns:\n",
    "                sub['conf_key'] = sub['conf_label'].astype(str)\n",
    "                conf_key = 'conf_key'\n",
    "\n",
    "            n_func = sub['func_key'].nunique(dropna=True)\n",
    "            n_anat = sub['anat_label'].nunique(dropna=True) if 'anat_label' in sub.columns else None\n",
    "            n_conf = sub[conf_key].nunique(dropna=True) if conf_key is not None else None\n",
    "\n",
    "            print(f\"Gene {GENE_OF_INTEREST}:\")\n",
    "            print(f\"  unique functional ROIs: {n_func}\")\n",
    "            if n_anat is not None:\n",
    "                print(f\"  unique anatomy labels: {n_anat}\")\n",
    "            if n_conf is not None:\n",
    "                print(f\"  unique confocal labels: {n_conf}\")\n",
    "            print(f\"  total rows in mapping: {len(sub)}\")\n",
    "\n",
    "            # Many-to-one: anatomy → multiple functional ROIs\n",
    "            if 'anat_label' in sub.columns:\n",
    "                anat_map = sub.dropna(subset=['anat_label', 'func_key']).groupby('anat_label')['func_key'].nunique()\n",
    "                multi_anat = anat_map[anat_map > 1].sort_values(ascending=False)\n",
    "                if not multi_anat.empty:\n",
    "                    print(f\"Anatomy labels mapping to multiple functional ROIs (top {TOP_K}):\")\n",
    "                    display(multi_anat.head(TOP_K))\n",
    "                    _split_plane_diag(sub, 'anat_label', 'anatomy', plane_col)\n",
    "                else:\n",
    "                    print('No anatomy labels map to multiple functional ROIs.')\n",
    "\n",
    "            # Many-to-one: confocal → multiple functional ROIs\n",
    "            if conf_key is not None:\n",
    "                conf_map = sub.dropna(subset=[conf_key, 'func_key']).groupby(conf_key)['func_key'].nunique()\n",
    "                multi_conf = conf_map[conf_map > 1].sort_values(ascending=False)\n",
    "                if not multi_conf.empty:\n",
    "                    print(f\"Confocal labels mapping to multiple functional ROIs (top {TOP_K}):\")\n",
    "                    display(multi_conf.head(TOP_K))\n",
    "                    _split_plane_diag(sub, conf_key, 'confocal', plane_col)\n",
    "                else:\n",
    "                    print('No confocal labels map to multiple functional ROIs.')\n",
    "\n",
    "            # Functional ROIs linked to multiple anatomy labels\n",
    "            if 'anat_label' in sub.columns:\n",
    "                func_map = sub.dropna(subset=['anat_label', 'func_key']).groupby('func_key')['anat_label'].nunique()\n",
    "                multi_func = func_map[func_map > 1].sort_values(ascending=False)\n",
    "                if not multi_func.empty:\n",
    "                    print(f\"Functional ROIs linked to multiple anatomy labels (top {TOP_K}):\")\n",
    "                    display(multi_func.head(TOP_K))\n",
    "                else:\n",
    "                    print('No functional ROIs map to multiple anatomy labels.')\n",
    "\n",
    "# --- Explicit reconciliation for split labels vs trace set ---\n",
    "TRACE_META_CSV = globals().get('TRACE_META_CSV', str(OUT_DERIVED / 'suite2p_traces' / 'suite2p_dff_traces_meta.csv'))\n",
    "\n",
    "def _make_func_key_local(df, plane_col):\n",
    "    if plane_col is None or 'func_label' not in df.columns:\n",
    "        return pd.Series([None] * len(df))\n",
    "    def _f(row):\n",
    "        plane_val = row.get(plane_col)\n",
    "        func_val = row.get('func_label')\n",
    "        if pd.notna(plane_val) and pd.notna(func_val):\n",
    "            return f'{int(plane_val)}:{int(func_val)}'\n",
    "        return None\n",
    "    return df.apply(_f, axis=1)\n",
    "\n",
    "def _split_trace_table_local(df, label_col, label_name, plane_col, trace_keys):\n",
    "    if label_col not in df.columns:\n",
    "        return\n",
    "    splits = df.dropna(subset=[label_col, 'func_key']).groupby(label_col)['func_key'].nunique()\n",
    "    splits = splits[splits > 1]\n",
    "    if splits.empty:\n",
    "        return\n",
    "    detail = df[df[label_col].isin(splits.index)].copy()\n",
    "    detail = detail.dropna(subset=[label_col, 'func_key'])\n",
    "    detail['in_trace_meta'] = detail['func_key'].isin(trace_keys) if trace_keys is not None else np.nan\n",
    "    cols = [label_col]\n",
    "    if plane_col:\n",
    "        cols.append(plane_col)\n",
    "    if 'func_label' in detail.columns:\n",
    "        cols.append('func_label')\n",
    "    cols.extend(['func_key', 'in_trace_meta'])\n",
    "    detail = detail.drop_duplicates(subset=cols)\n",
    "    if plane_col:\n",
    "        detail = detail.sort_values([label_col, plane_col, 'func_label'])\n",
    "    else:\n",
    "        detail = detail.sort_values([label_col, 'func_label'])\n",
    "    print(f'Split {label_name} labels: functional ROIs and trace usage (in_trace_meta)')\n",
    "    display(detail[cols])\n",
    "\n",
    "if 'sub' in locals() and isinstance(sub, pd.DataFrame) and not sub.empty:\n",
    "    trace_keys = None\n",
    "    if Path(TRACE_META_CSV).exists():\n",
    "        tdf = pd.read_csv(TRACE_META_CSV)\n",
    "        if not tdf.empty:\n",
    "            if 'gene' not in tdf.columns:\n",
    "                if 'conf_mask' in tdf.columns:\n",
    "                    if 'gene_from_mask' in globals():\n",
    "                        tdf['gene'] = tdf['conf_mask'].apply(gene_from_mask)\n",
    "                    else:\n",
    "                        def _gene_from_mask_local(path_str):\n",
    "                            name = Path(str(path_str)).name\n",
    "                            m = re.search(r'channel\\d+_(.+?)_cp_masks', name)\n",
    "                            gene = m.group(1) if m else name\n",
    "                            return gene.replace('sst1_', 'sst1.')\n",
    "                        tdf['gene'] = tdf['conf_mask'].apply(_gene_from_mask_local)\n",
    "                else:\n",
    "                    tdf['gene'] = 'unknown'\n",
    "            tdf = tdf[tdf['gene'] == GENE_OF_INTEREST].copy()\n",
    "            if not tdf.empty:\n",
    "                t_plane_col = _pick_plane_col(tdf)\n",
    "                tdf['func_key'] = _make_func_key_local(tdf, t_plane_col)\n",
    "                trace_keys = set(tdf['func_key'].dropna())\n",
    "                print(f'Trace meta unique functional ROIs: {len(trace_keys)}')\n",
    "                if 'n_anat' in locals() and n_anat is not None:\n",
    "                    print(f'  trace - anatomy = {len(trace_keys) - n_anat}')\n",
    "                if 'n_func' in locals():\n",
    "                    print(f'  trace - conf_to_func_pairs = {len(trace_keys) - n_func}')\n",
    "            else:\n",
    "                print('Trace meta found, but no rows for this gene.')\n",
    "    else:\n",
    "        print('Trace meta not found; run [51] to write suite2p_dff_traces_meta.csv for explicit trace reconciliation.')\n",
    "\n",
    "    if trace_keys is not None:\n",
    "        plane_col_sub = _pick_plane_col(sub)\n",
    "        _split_trace_table_local(sub, 'anat_label', 'anatomy', plane_col_sub, trace_keys)\n",
    "        if 'conf_key' in sub.columns:\n",
    "            _split_trace_table_local(sub, 'conf_key', 'confocal', plane_col_sub, trace_keys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc41b3cc",
   "metadata": {},
   "source": [
    "### [51] Suite2p dF/F traces for gene-matched functional ROIs\n",
    "Extract dF/F traces (from raw F) for ROIs that have gene matches and save per-plane arrays + metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6903f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [51]\n",
    "# Suite2p dF/F traces for gene-matched functional ROIs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "if not globals().get('suite2p_by_ref_idx'):\n",
    "    print('Suite2p data not loaded; run the Suite2p cell first.')\n",
    "elif not (OUT_REG / 'conf_to_func_pairs.csv').exists():\n",
    "    print('Missing conf_to_func_pairs.csv; run [50] first.')\n",
    "else:\n",
    "    pairs = pd.read_csv(OUT_REG / 'conf_to_func_pairs.csv')\n",
    "    if pairs.empty:\n",
    "        print('conf_to_func_pairs.csv is empty.')\n",
    "    elif 'func_label' not in pairs.columns:\n",
    "        print('conf_to_func_pairs.csv missing func_label; cannot map to Suite2p.')\n",
    "    else:\n",
    "        pairs = pairs[pairs['func_label'].notna()].copy()\n",
    "        if 'plane' in pairs.columns:\n",
    "            pairs = pairs[pairs['plane'].notna()].copy()\n",
    "            pairs['plane'] = pairs['plane'].astype(int)\n",
    "        pairs['func_label'] = pairs['func_label'].astype(int)\n",
    "        if pairs.empty:\n",
    "            print('No functional matches with func_label.')\n",
    "        else:\n",
    "            if 'conf_mask' in pairs.columns:\n",
    "                if 'gene_from_mask' in globals():\n",
    "                    pairs['gene'] = pairs['conf_mask'].apply(gene_from_mask)\n",
    "                else:\n",
    "                    def _gene_from_mask(path_str):\n",
    "                        name = Path(str(path_str)).name\n",
    "                        m = re.search(r'channel\\d+_(.+?)_cp_masks', name)\n",
    "                        gene = m.group(1) if m else name\n",
    "                        return gene.replace('sst1_', 'sst1.')\n",
    "                    pairs['gene'] = pairs['conf_mask'].apply(_gene_from_mask)\n",
    "            else:\n",
    "                pairs['gene'] = 'unknown'\n",
    "\n",
    "            out_dir = OUT_DERIVED / 'suite2p_traces'\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "            meta_rows = []\n",
    "            for p_idx, plane in suite2p_by_ref_idx.items():\n",
    "                p_idx = int(p_idx)\n",
    "                sub = pairs[pairs['plane'] == p_idx].copy() if 'plane' in pairs.columns else pairs.copy()\n",
    "                if sub.empty:\n",
    "                    continue\n",
    "                dff = plane.get('dff', None)\n",
    "                if dff is None:\n",
    "                    print(f'[Suite2p] plane {p_idx}: missing dff; skip')\n",
    "                    continue\n",
    "                roi_idx = sub['func_label'].to_numpy() - 1\n",
    "                valid = (roi_idx >= 0) & (roi_idx < dff.shape[0])\n",
    "                if not valid.any():\n",
    "                    continue\n",
    "                sub = sub.loc[valid].reset_index(drop=True)\n",
    "                roi_idx = roi_idx[valid]\n",
    "                traces = dff[roi_idx]\n",
    "                out_path = out_dir / f'plane{p_idx}_dff.npy'\n",
    "                np.save(out_path, traces)\n",
    "                sub['roi_idx'] = roi_idx\n",
    "                sub['trace_row'] = np.arange(traces.shape[0])\n",
    "                sub['trace_path'] = str(out_path)\n",
    "                meta_rows.append(sub)\n",
    "                print(f'[Suite2p] plane {p_idx}: saved dF/F traces n={traces.shape[0]} -> {out_path}')\n",
    "            if meta_rows:\n",
    "                meta_df = pd.concat(meta_rows, ignore_index=True)\n",
    "                meta_path = out_dir / 'suite2p_dff_traces_meta.csv'\n",
    "                meta_df.to_csv(meta_path, index=False)\n",
    "                print(f'[Suite2p] saved metadata: {meta_path} (rows={len(meta_df)})')\n",
    "                try:\n",
    "                    display(meta_df.head())\n",
    "                except Exception:\n",
    "                    print(meta_df.head().to_string(index=False))\n",
    "            else:\n",
    "                print('No matched traces to save.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb1ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### [52] Interactive overlay (select plane)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "    _HAS_WIDGETS = True\n",
    "except Exception:\n",
    "    widgets = None\n",
    "    display = None\n",
    "    _HAS_WIDGETS = False\n",
    "    print('ipywidgets not available; skipping interactive overlay. Install ipywidgets to enable.')\n",
    "\n",
    "__overlay_ready = True\n",
    "f_vis = None; a_vis = None; _plane_label = None; _best_z = None; _func_src_label = None\n",
    "_last_overlay = None\n",
    "\n",
    "\n",
    "def _load_overlay_for_plane(pr):\n",
    "    global __overlay_ready, f_vis, a_vis, _plane_label, _best_z, _func_src_label\n",
    "    __overlay_ready = True\n",
    "    f_vis = None; a_vis = None; _func_src_label = None\n",
    "    label_fallback = f\"plane{plane_refs.index(pr) if 'plane_refs' in globals() and pr in plane_refs else '?'}\"\n",
    "    _plane_label = pr.get('label', label_fallback)\n",
    "    _best_z = int(pr.get('best_z', 0))\n",
    "\n",
    "    _anat_stack = None\n",
    "    if 'anat_f' in globals() and globals().get('anat_f') is not None:\n",
    "        _anat_stack = np.asarray(globals()['anat_f'], dtype=float)\n",
    "    if _anat_stack is None and 'ANAT_STACK_PATH' in globals() and ANAT_STACK_PATH is not None:\n",
    "        try:\n",
    "            _anat_stack = np.asarray(imread_any(ANAT_STACK_PATH), dtype=float)\n",
    "        except Exception as _e:\n",
    "            print(f'[52] Could not load anatomy from {ANAT_STACK_PATH}: {_e}')\n",
    "    if _anat_stack is None and 'anat' in globals() and globals().get('anat') is not None:\n",
    "        _anat_stack = np.asarray(globals()['anat'], dtype=float)\n",
    "        print('[52] Using in-memory anat stack (verify this is intensity, not labels).')\n",
    "\n",
    "    if _anat_stack is None:\n",
    "        print('[52] Anatomy intensity stack missing; run earlier cells.')\n",
    "        __overlay_ready = False\n",
    "        return False\n",
    "    if _best_z < 0 or _best_z >= _anat_stack.shape[0]:\n",
    "        print(f'[52] best_z {_best_z} out of bounds for {_plane_label} (z_size={_anat_stack.shape[0]})')\n",
    "        __overlay_ready = False\n",
    "        return False\n",
    "\n",
    "    anat_slice = np.asarray(_anat_stack[_best_z], dtype=float)\n",
    "    func_candidates = [\n",
    "        ('ref_warped_raw', pr.get('ref_warped_raw')),\n",
    "        ('ref_warped', pr.get('ref_warped')),\n",
    "        ('ref_match', pr.get('ref_match')),\n",
    "        ('ref2d_raw', pr.get('ref2d_raw')),\n",
    "        ('ref2d', pr.get('ref2d')),\n",
    "        ('ref', pr.get('ref')),\n",
    "    ]\n",
    "    func_img = None\n",
    "    for lbl, cand in func_candidates:\n",
    "        if cand is not None:\n",
    "            func_img = cand\n",
    "            _func_src_label = lbl\n",
    "            break\n",
    "    if func_img is None:\n",
    "        print(f'[52] No functional image for {_plane_label}.')\n",
    "        __overlay_ready = False\n",
    "        return False\n",
    "\n",
    "    func_img = np.asarray(func_img, dtype=float)\n",
    "    _already_warped = _func_src_label in ('ref_warped_raw', 'ref_warped')\n",
    "    tform_use = pr.get('tform', globals().get('tform', None))\n",
    "    if _already_warped:\n",
    "        tform_use = None  # ref_warped* are already in anatomy space\n",
    "\n",
    "    try:\n",
    "        if tform_use is not None:\n",
    "            f_src = apply_transform_2d(func_img, tform_use, output_shape=anat_slice.shape, order=1)\n",
    "        else:\n",
    "            f_src = func_img\n",
    "        if f_src.shape != anat_slice.shape:\n",
    "            f_src = transform.resize(\n",
    "                f_src, anat_slice.shape, order=1, mode='reflect',\n",
    "                preserve_range=True, anti_aliasing=True).astype(np.float32)\n",
    "        f_vis = norm01(f_src)\n",
    "        a_vis = norm01(anat_slice)\n",
    "        return True\n",
    "    except Exception as _e:\n",
    "        print('[52] Interactive overlay prerequisites missing:', _e)\n",
    "        __overlay_ready = False\n",
    "        return False\n",
    "\n",
    "\n",
    "if 'plane_refs' not in globals() or not plane_refs:\n",
    "    print('[52] No plane_refs available.')\n",
    "    __overlay_ready = False\n",
    "else:\n",
    "    target = None\n",
    "    if 'TARGET_PLANE_LABEL' in globals() and TARGET_PLANE_LABEL is not None:\n",
    "        for pr in plane_refs:\n",
    "            if pr.get('label') == TARGET_PLANE_LABEL:\n",
    "                target = pr\n",
    "                break\n",
    "    if target is None:\n",
    "        target = plane_refs[-1]\n",
    "    _load_overlay_for_plane(target)\n",
    "    _plane_map = {pr.get('label', f'plane{i}'): pr for i, pr in enumerate(plane_refs)}\n",
    "\n",
    "_COLORS = {\n",
    "    'green':   (0.0, 1.0, 0.0),\n",
    "    'magenta': (1.0, 0.0, 1.0),\n",
    "    'red':     (1.0, 0.0, 0.0),\n",
    "    'blue':    (0.0, 0.0, 1.0),\n",
    "    'cyan':    (0.0, 1.0, 1.0),\n",
    "    'yellow':  (1.0, 1.0, 0.0),\n",
    "    'white':   (1.0, 1.0, 1.0)\n",
    "}\n",
    "\n",
    "def _apply_color(gray01, rgb):\n",
    "    r, g, b = rgb\n",
    "    return np.stack([gray01 * r, gray01 * g, gray01 * b], axis=-1)\n",
    "\n",
    "def _render(show_func=True, show_anat=True, func_color='green', anat_color='magenta', func_alpha=1.0, anat_alpha=1.0):\n",
    "    global _last_overlay\n",
    "    if not (__overlay_ready and f_vis is not None and a_vis is not None):\n",
    "        print('[52] Overlay not ready. Run prerequisites first.')\n",
    "        _last_overlay = None\n",
    "        return\n",
    "    out = np.zeros((f_vis.shape[0], f_vis.shape[1], 3), dtype=np.float32)\n",
    "    if show_anat:\n",
    "        out += _apply_color(a_vis, _COLORS[anat_color]) * float(anat_alpha)\n",
    "    if show_func:\n",
    "        out += _apply_color(f_vis, _COLORS[func_color]) * float(func_alpha)\n",
    "    out = np.clip(out, 0, 1)\n",
    "    _last_overlay = out\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(out)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if _HAS_WIDGETS and __overlay_ready:\n",
    "    plane_sel = widgets.Dropdown(options=list(_plane_map.keys()), value=_plane_label, description='Plane')\n",
    "    show_func_cb = widgets.Checkbox(value=True, description='Show functional')\n",
    "    show_anat_cb = widgets.Checkbox(value=True, description='Show anatomy')\n",
    "    func_color_dd = widgets.Dropdown(options=list(_COLORS.keys()), value='green', description='Func LUT')\n",
    "    anat_color_dd = widgets.Dropdown(options=list(_COLORS.keys()), value='magenta', description='Anat LUT')\n",
    "    func_alpha_sl = widgets.FloatSlider(value=1.0, min=0.0, max=1.0, step=0.05, readout_format='.2f', description='Func alpha')\n",
    "    anat_alpha_sl = widgets.FloatSlider(value=1.0, min=0.0, max=1.0, step=0.05, readout_format='.2f', description='Anat alpha')\n",
    "\n",
    "    default_out = Path('.')\n",
    "    if 'OUT_QA' in globals() and globals().get('OUT_QA') is not None:\n",
    "        try:\n",
    "            default_out = Path(OUT_QA)\n",
    "        except Exception:\n",
    "            default_out = Path('.')\n",
    "    elif 'OUTDIR' in globals() and globals().get('OUTDIR') is not None:\n",
    "        try:\n",
    "            default_out = Path(OUTDIR)\n",
    "        except Exception:\n",
    "            default_out = Path('.')\n",
    "    default_path = default_out / f\"{_plane_label}_overlay.png\"\n",
    "    save_path_txt = widgets.Text(value=str(default_path), description='Save path', layout=widgets.Layout(width='70%'))\n",
    "    save_btn = widgets.Button(description='Save PNG', button_style='success')\n",
    "    save_status = widgets.Label(value='')\n",
    "\n",
    "    def _save_overlay(_):\n",
    "        if _last_overlay is None:\n",
    "            save_status.value = 'No overlay rendered yet.'\n",
    "            return\n",
    "        path = Path(save_path_txt.value).expanduser()\n",
    "        try:\n",
    "            path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            plt.imsave(path, _last_overlay)\n",
    "            save_status.value = f'Saved to {path}'\n",
    "        except Exception as _e:\n",
    "            save_status.value = f'Save failed: {_e}'\n",
    "\n",
    "    def _on_plane(change):\n",
    "        if change.get('name') != 'value':\n",
    "            return\n",
    "        pr = _plane_map.get(change['new'])\n",
    "        if pr is None:\n",
    "            return\n",
    "        if _load_overlay_for_plane(pr):\n",
    "            save_path_txt.value = str(default_out / f\"{_plane_label}_overlay.png\")\n",
    "            _render(show_func_cb.value, show_anat_cb.value, func_color_dd.value, anat_color_dd.value, func_alpha_sl.value, anat_alpha_sl.value)\n",
    "\n",
    "    save_btn.on_click(_save_overlay)\n",
    "    plane_sel.observe(_on_plane, names='value')\n",
    "\n",
    "    ui = widgets.VBox([\n",
    "        plane_sel,\n",
    "        widgets.HBox([show_func_cb, func_color_dd, func_alpha_sl]),\n",
    "        widgets.HBox([show_anat_cb, anat_color_dd, anat_alpha_sl]),\n",
    "        widgets.HBox([save_path_txt, save_btn]),\n",
    "        save_status,\n",
    "    ])\n",
    "    out = widgets.interactive_output(_render, {\n",
    "        'show_func': show_func_cb,\n",
    "        'show_anat': show_anat_cb,\n",
    "        'func_color': func_color_dd,\n",
    "        'anat_color': anat_color_dd,\n",
    "        'func_alpha': func_alpha_sl,\n",
    "        'anat_alpha': anat_alpha_sl,\n",
    "    })\n",
    "    display(ui, out)\n",
    "elif _HAS_WIDGETS and not __overlay_ready:\n",
    "    print('[52] Interactive overlay not shown: missing inputs (run previous cells).')\n",
    "elif not _HAS_WIDGETS and __overlay_ready:\n",
    "    _render()\n",
    "elif not _HAS_WIDGETS:\n",
    "    print('ipywidgets not available; skipping interactive overlay.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f92edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# [53]\n",
    "# --- Report figures (NCC, diameters, offsets, HCR QC) ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "from pathlib import Path\n",
    "import os, re\n",
    "from skimage.measure import regionprops_table\n",
    "from scipy.optimize import linear_sum_assignment as _lsa\n",
    "\n",
    "# Palette and sizing (edit as needed)\n",
    "palette = {\n",
    "    'Anatomy': '#6b6b6b',\n",
    "    'Functional': '#2e9f4d',\n",
    "    'HCR': '#d000a7',\n",
    "}\n",
    "ncc_figsize = (10, 5)\n",
    "diam_figsize = (20, 4.5)\n",
    "offset_figsize = (10, 4.5)\n",
    "dist_figsize = (12, 4)\n",
    "conf_offset_figsize = (14, 4.2)\n",
    "violin_alpha = 0.75\n",
    "median_marker = dict(color='black', s=24, zorder=3)\n",
    "line_color = '#c1121f'\n",
    "\n",
    "# --- [18] NCC score curves (legend removed) ---\n",
    "if 'plane_refs' in globals() and plane_refs:\n",
    "    fig, ax = plt.subplots(figsize=ncc_figsize)\n",
    "    for pr in plane_refs:\n",
    "        scores = pr.get('ncc_scores')\n",
    "        if scores is None:\n",
    "            continue\n",
    "        m = re.search(r\"plane(\\d+)\", pr.get('label', ''))\n",
    "        plane_no = m.group(1) if m else '?'\n",
    "        lbl = f\"{FISH_ID} plane {plane_no}\" if 'FISH_ID' in globals() else f\"plane {plane_no}\"\n",
    "        line, = ax.plot(scores, label=lbl)\n",
    "        if 'best_z' in pr:\n",
    "            ax.axvline(pr['best_z'], color=line.get_color(), linestyle='--', alpha=0.7)\n",
    "    ax.set_xlabel('Z')\n",
    "    ax.set_ylabel('NCC score')\n",
    "    ax.set_title('Best-Z scores per plane')\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No plane_refs available for NCC plot')\n",
    "\n",
    "# --- [30] Noise-filtered diameters (simplified aesthetics) ---\n",
    "MIN_XY_HCR_UM = 3.0\n",
    "df_all_src = globals().get('df_all', None)\n",
    "if df_all_src is None or getattr(df_all_src, 'empty', True):\n",
    "    _diam_paths = []\n",
    "    if 'OUT_QA' in globals():\n",
    "        _diam_paths.append(OUT_QA / 'diameters_df_all.pkl')\n",
    "    if 'OUTDIR' in globals():\n",
    "        _diam_paths.append(OUTDIR / 'diameters_df_all.pkl')\n",
    "    for _p in _diam_paths:\n",
    "        if os.path.exists(_p):\n",
    "            try:\n",
    "                df_all_src = pd.read_pickle(_p)\n",
    "                print(f\"[Info] Loaded df_all from cache pickle: {_p}\")\n",
    "                break\n",
    "            except Exception:\n",
    "                df_all_src = None\n",
    "\n",
    "if df_all_src is None or getattr(df_all_src, 'empty', True):\n",
    "    print('No diameter dataframe available; run the diameters cell first.')\n",
    "else:\n",
    "    df_all_filt = df_all_src.copy()\n",
    "    if 'dataset' in df_all_filt.columns and 'HCR' in df_all_filt['dataset'].unique():\n",
    "        drop_mask = df_all_filt['dataset'].eq('HCR') & ((df_all_filt['x_um'] < MIN_XY_HCR_UM) | (df_all_filt['y_um'] < MIN_XY_HCR_UM))\n",
    "        if drop_mask.any():\n",
    "            print(f\"[Filter] Dropped {int(drop_mask.sum())} HCR labels with X or Y < {MIN_XY_HCR_UM} µm\")\n",
    "            df_all_filt = df_all_filt.loc[~drop_mask].reset_index(drop=True)\n",
    "    cats = ['Anatomy', 'Functional', 'HCR']\n",
    "    present = [c for c in cats if c in df_all_filt['dataset'].unique()]\n",
    "\n",
    "    def _series(axis):\n",
    "        return [df_all_filt.loc[df_all_filt['dataset'] == ds, axis].to_numpy(float) for ds in present]\n",
    "\n",
    "    series_x = _series('x_um'); series_y = _series('y_um'); series_z = _series('z_um')\n",
    "    all_vals = np.concatenate([a for a in (series_x + series_y + series_z) if a.size]) if any((a.size for a in (series_x + series_y + series_z))) else np.array([])\n",
    "    y_max = max(10.0, 10.0 * ceil(float(all_vals.max()) / 10.0)) if all_vals.size else 10.0\n",
    "    yticks = np.arange(0.0, y_max + 0.1, 10.0)\n",
    "    colors = [palette.get(ds, '#999999') for ds in present]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=diam_figsize, sharey=True)\n",
    "    for ax, ser, title in zip(axes, [series_x, series_y, series_z], ['X diameter (µm)', 'Y diameter (µm)', 'Z diameter (µm)']):\n",
    "        if not any(a.size for a in ser):\n",
    "            ax.set_visible(False)\n",
    "            continue\n",
    "        parts = ax.violinplot(ser, showmeans=False, showmedians=False, showextrema=False)\n",
    "        for i, pc in enumerate(parts['bodies']):\n",
    "            pc.set_facecolor(colors[i])\n",
    "            pc.set_edgecolor('black')\n",
    "            pc.set_alpha(violin_alpha)\n",
    "        for i, vals in enumerate(ser, start=1):\n",
    "            if vals.size:\n",
    "                ax.scatter([i], [np.median(vals)], **median_marker)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xticks(range(1, len(present) + 1))\n",
    "        ax.set_xticklabels(present)\n",
    "        ax.set_ylim(0, y_max)\n",
    "        ax.set_yticks(yticks)\n",
    "        ax.grid(axis='y', alpha=0.25)\n",
    "    fig.suptitle(f\"Mask diameters by axis (HCR filtered: X/Y ≥ {MIN_XY_HCR_UM} µm)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Summary table for diameters by axis\n",
    "    summary_rows = []\n",
    "    for ds in present:\n",
    "        for axis, col in [('X', 'x_um'), ('Y', 'y_um'), ('Z', 'z_um')]:\n",
    "            vals = df_all_filt.loc[df_all_filt['dataset'] == ds, col].to_numpy(dtype=float)\n",
    "            summary_rows.append({\n",
    "                'dataset': ds,\n",
    "                'axis': axis,\n",
    "                'n': int(vals.size),\n",
    "                'median_um': float(np.median(vals)) if vals.size else np.nan,\n",
    "                'sd_um': float(np.std(vals)) if vals.size else np.nan,\n",
    "            })\n",
    "    if summary_rows:\n",
    "        diam_summary = pd.DataFrame(summary_rows)\n",
    "        try:\n",
    "            display(diam_summary)\n",
    "        except Exception:\n",
    "            print(diam_summary.to_string(index=False))\n",
    "\n",
    "# --- New: XY and Z centroid offsets (antsQC-style) using [36] data ---\n",
    "def _gather_xy_z_offsets():\n",
    "    if 'plane_refs' not in globals() or not plane_refs:\n",
    "        return None, None, None, None, 'No plane_refs available.'\n",
    "    if '_prepare_plane_data' not in globals():\n",
    "        return None, None, None, None, 'Centroid QA helpers not initialized (run [34]).'\n",
    "    DZ = float(VOX_ANAT.get('Z', 1.0)) if 'VOX_ANAT' in globals() else 1.0\n",
    "\n",
    "    def _get_anat_volume():\n",
    "        if 'anat_labels_all' in globals() and globals()['anat_labels_all'] is not None:\n",
    "            return globals()['anat_labels_all']\n",
    "        try:\n",
    "            return _ensure_uint_labels(imread_any(ANAT_LABELS_PATH))\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    anat_vol = _get_anat_volume()\n",
    "    if anat_vol is None:\n",
    "        return None, None, None, None, 'Anatomy labels unavailable for Z offsets.'\n",
    "\n",
    "    try:\n",
    "        props3d = regionprops_table(anat_vol, properties=('label', 'centroid', 'bbox'))\n",
    "        adf3d = pd.DataFrame(props3d)\n",
    "        adf3d = adf3d[adf3d['label'] != 0]\n",
    "        adf3d = adf3d.rename(columns={'centroid-0': 'cz', 'centroid-1': 'cy', 'centroid-2': 'cx',\n",
    "                                      'bbox-0': 'zmin', 'bbox-1': 'ymin', 'bbox-2': 'xmin', 'bbox-3': 'zmax', 'bbox-4': 'ymax', 'bbox-5': 'xmax'})\n",
    "        z_cent = adf3d.set_index('label')['cz'].to_dict()\n",
    "        vx = float(VOX_ANAT.get('X', 1.0) if 'VOX_ANAT' in globals() else 1.0)\n",
    "        vy = float(VOX_ANAT.get('Y', 1.0) if 'VOX_ANAT' in globals() else 1.0)\n",
    "        vz = float(VOX_ANAT.get('Z', 1.0) if 'VOX_ANAT' in globals() else 1.0)\n",
    "        x_um = (adf3d['xmax'] - adf3d['xmin']) * vx\n",
    "        y_um = (adf3d['ymax'] - adf3d['ymin']) * vy\n",
    "        z_um = (adf3d['zmax'] - adf3d['zmin']) * vz\n",
    "        diam_xy_um = (x_um + y_um) / 2.0\n",
    "        mean_rad_xy_um = float((diam_xy_um / 2.0).mean()) if not adf3d.empty else 0.0\n",
    "    except Exception:\n",
    "        z_cent = {}\n",
    "        mean_rad_xy_um = 0.0\n",
    "    if not z_cent:\n",
    "        return None, None, None, None, 'Could not compute anatomy centroids for Z offsets.'\n",
    "\n",
    "    xy_all, z_all = [], []\n",
    "    for p_idx in range(len(plane_refs)):\n",
    "        pdata = _prepare_plane_data(p_idx)\n",
    "        if pdata is None or pdata.get('d') is None or not pdata['d'].size:\n",
    "            continue\n",
    "        xy_all.append(np.asarray(pdata['d'], float))\n",
    "        df_links = pdata.get('df', None)\n",
    "        if df_links is None or 'anat_label' not in df_links.columns:\n",
    "            continue\n",
    "        bz = pdata.get('best_z', plane_refs[p_idx].get('best_z', p_idx))\n",
    "        bz_um = float(bz) * DZ\n",
    "        z_offs = []\n",
    "        for al in df_links['anat_label'].to_numpy(int):\n",
    "            zc = z_cent.get(int(al))\n",
    "            if zc is None:\n",
    "                continue\n",
    "            z_offs.append(abs(float(zc) * DZ - bz_um))\n",
    "        if z_offs:\n",
    "            z_all.append(np.asarray(z_offs, float))\n",
    "    if not xy_all:\n",
    "        return None, None, None, None, 'No centroid matches available.'\n",
    "    xy_vals = np.concatenate(xy_all)\n",
    "    z_vals = np.concatenate(z_all) if z_all else None\n",
    "    return xy_vals, z_vals, mean_rad_xy_um, None, None\n",
    "\n",
    "xy_vals, z_vals, mean_rad_xy_um, _, err = _gather_xy_z_offsets()\n",
    "if err:\n",
    "    print(err)\n",
    "else:\n",
    "    plots = [\n",
    "        ('XY centroid offsets (µm)', xy_vals, '#4c78a8', mean_rad_xy_um, 'XY'),\n",
    "        ('Z centroid offsets (µm)', z_vals, '#f58518', None, 'Z'),\n",
    "    ]\n",
    "    rows = []\n",
    "    max_val = 0.0\n",
    "    for _, vals, _, _, _ in plots:\n",
    "        if vals is not None and np.size(vals):\n",
    "            max_val = max(max_val, float(np.max(vals)))\n",
    "    if max_val == 0.0 and not any(np.size(v) for _, v, _, _, _ in plots if v is not None):\n",
    "        print('No centroid offsets to plot.')\n",
    "    else:\n",
    "        y_max = max(10.0, 10.0 * ceil(max_val / 10.0)) if max_val > 0 else 10.0\n",
    "        yticks = np.arange(0.0, y_max + 0.1, 10.0)\n",
    "        fig, axes = plt.subplots(1, 2, figsize=offset_figsize, sharey=True)\n",
    "        p95_color = '#ff7f0e'\n",
    "        for ax, (title, vals, color, med_line, xtick) in zip(axes, plots):\n",
    "            if vals is None or not np.size(vals):\n",
    "                ax.set_visible(False)\n",
    "                continue\n",
    "            parts = ax.violinplot([vals], positions=[1], showmeans=False, showmedians=False, showextrema=False)\n",
    "            for pc in parts['bodies']:\n",
    "                pc.set_facecolor(color)\n",
    "                pc.set_edgecolor('black')\n",
    "                pc.set_alpha(violin_alpha)\n",
    "            ax.scatter([1], [float(np.median(vals))], **median_marker)\n",
    "            p95 = float(np.percentile(vals, 95))\n",
    "            ax.axhline(p95, color=p95_color, linestyle='-.', linewidth=1.5, label='95th percentile')\n",
    "            if med_line is not None and np.isfinite(med_line) and med_line > 0:\n",
    "                ax.axhline(med_line, color=line_color, linestyle='--', linewidth=1.5, label='Anat mean radius')\n",
    "            ax.legend(loc='upper right', frameon=False)\n",
    "            ax.set_title(title)\n",
    "            ax.set_xticks([1])\n",
    "            ax.set_xticklabels([xtick])\n",
    "            ax.set_ylim(0, y_max)\n",
    "            ax.set_yticks(yticks)\n",
    "            ax.set_ylabel('offset (µm)')\n",
    "            ax.grid(axis='y', alpha=0.25)\n",
    "            rows.append({\n",
    "                'axis': xtick,\n",
    "                'n': int(np.size(vals)),\n",
    "                'median_offset_um': float(np.median(vals)),\n",
    "                'sd_offset_um': float(np.std(vals)),\n",
    "                'p95_offset_um': float(p95),\n",
    "                'anat_mean_radius_um': float(med_line) if med_line is not None else np.nan,\n",
    "            })\n",
    "        fig.suptitle('Centroid offsets (functional → anatomy, all planes)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    if rows:\n",
    "        summary_df = pd.DataFrame(rows)\n",
    "        try:\n",
    "            display(summary_df)\n",
    "        except Exception:\n",
    "            print(summary_df.to_string(index=False))\n",
    "\n",
    "# --- [48] HCR → 2P centroid distances (best-Z functional planes only) ---\n",
    "if not (globals().get('plane_refs') and globals().get('hcr_match_results')):\n",
    "    print('Need plane_refs and hcr_match_results; skipping HCR distance plots.')\n",
    "else:\n",
    "    DZ = float(VOX_ANAT.get('Z', 1.0)) if 'VOX_ANAT' in globals() else 1.0\n",
    "    DY = float(VOX_ANAT.get('Y', 1.0)) if 'VOX_ANAT' in globals() else 1.0\n",
    "    DX = float(VOX_ANAT.get('X', 1.0)) if 'VOX_ANAT' in globals() else 1.0\n",
    "    MIN_OVERLAP_FUNC_ANAT = int(globals().get('MIN_OVERLAP_FUNC_ANAT', 1))\n",
    "    REQUIRE_OVERLAP_FUNC_ANAT = bool(globals().get('REQUIRE_OVERLAP_FUNC_ANAT', True))\n",
    "\n",
    "    def gene_from_mask(path_str):\n",
    "        name = Path(path_str).name\n",
    "        m = re.search(r'channel\\d+_(.+?)_cp_masks', name)\n",
    "        gene = m.group(1) if m else name\n",
    "        return gene.replace('sst1_', 'sst1.')\n",
    "\n",
    "    def load_func_for_plane(pr, p_idx):\n",
    "        if '_get_labels_for_plane' in globals():\n",
    "            try:\n",
    "                arr, desc = _get_labels_for_plane(pr, p_idx); return arr, desc\n",
    "            except Exception:\n",
    "                pass\n",
    "        if '_load_func_labels_for_plane' in globals():\n",
    "            try:\n",
    "                arr, _, desc = globals()['_load_func_labels_for_plane'](p_idx); return arr, desc\n",
    "            except Exception:\n",
    "                pass\n",
    "        return None, None\n",
    "\n",
    "    def norm_func_labels(arr):\n",
    "        arr = _ensure_uint_labels(arr)\n",
    "        if arr.ndim == 3 and arr.shape[-1] in (3, 4): arr = arr[..., 0]\n",
    "        if arr.ndim == 3 and arr.shape[0] == 1: arr = arr[0]\n",
    "        return arr\n",
    "\n",
    "    try:\n",
    "        anat_all = _ensure_uint_labels(imread_any(ANAT_LABELS_PATH))\n",
    "    except Exception as e:\n",
    "        anat_all = None\n",
    "        print('Could not load ANAT_LABELS_PATH:', e)\n",
    "\n",
    "    matched_anat_ids = set()\n",
    "    if anat_all is not None:\n",
    "        from skimage.transform import AffineTransform\n",
    "        z_size = anat_all.shape[0] if anat_all.ndim == 3 else 1\n",
    "        for p_idx, pr in enumerate(plane_refs):\n",
    "            bz = int(pr.get('best_z', 0))\n",
    "            if bz < 0 or (anat_all.ndim == 3 and bz >= z_size): continue\n",
    "            anat_slice = anat_all[bz] if anat_all.ndim == 3 else anat_all\n",
    "            func_raw, _ = load_func_for_plane(pr, p_idx)\n",
    "            if func_raw is None: continue\n",
    "            func_raw = norm_func_labels(func_raw)\n",
    "            if func_raw.ndim != 2: continue\n",
    "            tform_use = pr.get('tform', globals().get('tform', None))\n",
    "            try:\n",
    "                if tform_use is None:\n",
    "                    func_warped = func_raw if func_raw.shape == anat_slice.shape else resample_labels_nn(func_raw, AffineTransform(), output_shape=anat_slice.shape)\n",
    "                else:\n",
    "                    func_warped = resample_labels_nn(func_raw, tform_use, output_shape=anat_slice.shape)\n",
    "            except Exception:\n",
    "                func_warped = func_raw if func_raw.shape == anat_slice.shape else None\n",
    "            if func_warped is None: continue\n",
    "            ov = compute_label_overlap(func_warped, anat_slice, min_overlap_voxels=1)\n",
    "            if ov.empty: continue\n",
    "            ov = ov.rename(columns={'conf_label': 'func_label', 'twoP_label': 'anat_label'})\n",
    "            if REQUIRE_OVERLAP_FUNC_ANAT:\n",
    "                ov = ov[ov['overlap_voxels'] >= MIN_OVERLAP_FUNC_ANAT]\n",
    "            if ov.empty: continue\n",
    "            matched_anat_ids.update(ov['anat_label'].astype(int).tolist())\n",
    "\n",
    "    if not matched_anat_ids:\n",
    "        print('No anatomy labels matched to functional planes; skipping HCR distance plots.')\n",
    "    else:\n",
    "        order_genes = ['sst1.1', 'sst1.2', 'pth2', 'tac3b']\n",
    "        gene_colors = {'sst1.1': '#d62728', 'sst1.2': '#d61ad2', 'pth2': '#00bcd4', 'tac3b': '#ffd400'}\n",
    "        dist_rows, offsets_xy, offsets_z = [], {g: [] for g in order_genes}, {g: [] for g in order_genes}\n",
    "        med_dxy_um = 0.0\n",
    "        med_dz_um = 0.0\n",
    "        if anat_all is not None:\n",
    "            try:\n",
    "                props3d = regionprops_table(anat_all, properties=(\"label\",\"bbox\"))\n",
    "                adf3d = pd.DataFrame(props3d)\n",
    "                adf3d = adf3d[adf3d[\"label\"] != 0]\n",
    "                dx_um = (adf3d[\"bbox-5\"] - adf3d[\"bbox-2\"]) * DX\n",
    "                dy_um = (adf3d[\"bbox-4\"] - adf3d[\"bbox-1\"]) * DY\n",
    "                dz_um = (adf3d[\"bbox-3\"] - adf3d[\"bbox-0\"]) * DZ\n",
    "                med_dxy_um = float(((dx_um + dy_um) / 2.0).median()) if not adf3d.empty else 0.0\n",
    "                med_dz_um = float(dz_um.median()) if not adf3d.empty else 0.0\n",
    "            except Exception:\n",
    "                med_dxy_um = med_dz_um = 0.0\n",
    "\n",
    "        for res in hcr_match_results:\n",
    "            fp = res.get('final_pairs')\n",
    "            if fp is None or fp.empty: continue\n",
    "            gene = gene_from_mask(res.get('mask_path', 'unknown'))\n",
    "            if gene not in order_genes: continue\n",
    "            sub = fp[fp['twoP_label'].isin(matched_anat_ids)].copy()\n",
    "            if sub.empty: continue\n",
    "            dist_rows.append(pd.DataFrame({'distance_um': sub['distance_um'].to_numpy(float), 'gene': gene}))\n",
    "            df_conf, df_2p = res.get('df_conf'), res.get('df_2p')\n",
    "            if df_conf is None or df_2p is None: continue\n",
    "            cmap = df_conf.set_index('label')[['x','y','z']].to_dict('index')\n",
    "            amap = df_2p.set_index('label')[['x','y','z']].to_dict('index')\n",
    "            for _, r in sub.iterrows():\n",
    "                c, a = cmap.get(int(r['conf_label'])), amap.get(int(r['twoP_label']))\n",
    "                if c is None or a is None: continue\n",
    "                dx_um = (float(c['x']) - float(a['x'])) * DX\n",
    "                dy_um = (float(c['y']) - float(a['y'])) * DY\n",
    "                dz_um = (float(c['z']) - float(a['z'])) * DZ\n",
    "                offsets_xy[gene].append(np.sqrt(dx_um*dx_um + dy_um*dy_um))\n",
    "                offsets_z[gene].append(abs(dz_um))\n",
    "\n",
    "        if not dist_rows:\n",
    "            print('No filtered HCR pairs found for best-Z planes.')\n",
    "        else:\n",
    "            df_dist = pd.concat(dist_rows, ignore_index=True)\n",
    "            present = [g for g in order_genes if g in df_dist['gene'].unique()]\n",
    "            if present:\n",
    "                fig, ax = plt.subplots(figsize=(10, 4))\n",
    "                data = [df_dist.loc[df_dist['gene'] == g, 'distance_um'].to_numpy(float) for g in present]\n",
    "                bp = ax.boxplot(data, patch_artist=True, labels=present)\n",
    "                for patch, g in zip(bp['boxes'], present):\n",
    "                    patch.set_facecolor(gene_colors.get(g, '#cccccc'))\n",
    "                    patch.set_edgecolor('black')\n",
    "                ax.set_ylabel('distance (µm)')\n",
    "                ax.set_title('HCR → 2P centroid distances (best-Z anatomy only)')\n",
    "                plt.tight_layout(); plt.show()\n",
    "            dist_summary = df_dist.groupby('gene').agg(n=('distance_um','size'), median_um=('distance_um','median'), sd_um=('distance_um','std')).reindex(order_genes)\n",
    "            try: display(dist_summary)\n",
    "            except Exception: print(dist_summary.dropna(how='all').to_string())\n",
    "\n",
    "            all_off_vals = []\n",
    "            for series_dict in (offsets_xy, offsets_z):\n",
    "                for g in order_genes:\n",
    "                    all_off_vals.extend(series_dict.get(g, []))\n",
    "            max_off = max(all_off_vals) if all_off_vals else 0.0\n",
    "            y_max = 20.0\n",
    "            yticks = np.arange(0.0, y_max + 0.1, 10.0)\n",
    "\n",
    "            fig, axes = plt.subplots(1, 2, figsize=offset_figsize, sharey=True)\n",
    "            for ax, axis_label, series_dict in [(axes[0], 'XY', offsets_xy), (axes[1], 'Z', offsets_z)]:\n",
    "                present_off = [g for g in order_genes if series_dict.get(g)]\n",
    "                if not present_off: ax.set_visible(False); continue\n",
    "                pos = np.arange(1, len(present_off)+1)\n",
    "                data_off = [np.asarray(series_dict[g], float) for g in present_off]\n",
    "                vp = ax.violinplot(data_off, positions=pos, showmeans=False, showmedians=False, showextrema=False)\n",
    "                for pc, g in zip(vp['bodies'], present_off):\n",
    "                    pc.set_facecolor(gene_colors.get(g, '#cccccc'))\n",
    "                    pc.set_edgecolor('black'); pc.set_alpha(violin_alpha)\n",
    "                medians = [np.median(d) for d in data_off]\n",
    "                ax.scatter(pos, medians, **median_marker)\n",
    "                ref_line = med_dxy_um if axis_label == \"XY\" else med_dz_um\n",
    "                if ref_line > 0:\n",
    "                    ax.axhline(ref_line, color=line_color, linestyle=\"--\", linewidth=1.3)\n",
    "                ax.set_xticks(pos); ax.set_xticklabels(present_off, rotation=45, ha='right')\n",
    "                ax.set_title(f'{axis_label} offsets (µm)'); ax.set_ylabel('offset (µm)')\n",
    "                ax.grid(axis='y', alpha=0.25)\n",
    "                ax.set_ylim(0, y_max)\n",
    "                ax.set_yticks(yticks)\n",
    "            fig.suptitle('HCR offsets by gene (best-Z anatomy only)')\n",
    "            plt.tight_layout(); plt.show()\n",
    "\n",
    "            rows = []\n",
    "            for axis_label, series_dict in [('XY', offsets_xy), ('Z', offsets_z)]:\n",
    "                for g in order_genes:\n",
    "                    vals = np.asarray(series_dict.get(g, []), float)\n",
    "                    if vals.size:\n",
    "                        rows.append({'gene': g, 'axis': axis_label, 'n': int(vals.size), 'median_um': float(np.median(vals)), 'sd_um': float(np.std(vals))})\n",
    "            if rows:\n",
    "                off_df = pd.DataFrame(rows)\n",
    "                try: display(off_df)\n",
    "                except Exception: print(off_df.to_string(index=False))\n",
    "# --- [49] Confocal ↔ Anatomy offsets (anat labels intersect functional best_z) ---\n",
    "if not (globals().get('plane_refs') and globals().get('hcr_match_results')):\n",
    "    print('Need plane_refs and hcr_match_results; skipping confocal↔anatomy offsets.')\n",
    "else:\n",
    "    DZ = float(VOX_ANAT.get('Z', 1.0)) if 'VOX_ANAT' in globals() else 1.0\n",
    "    DY = float(VOX_ANAT.get('Y', 1.0)) if 'VOX_ANAT' in globals() else 1.0\n",
    "    DX = float(VOX_ANAT.get('X', 1.0)) if 'VOX_ANAT' in globals() else 1.0\n",
    "    MIN_OVERLAP_FUNC_ANAT = int(globals().get('MIN_OVERLAP_FUNC_ANAT', 1))\n",
    "    REQUIRE_OVERLAP_FUNC_ANAT = bool(globals().get('REQUIRE_OVERLAP_FUNC_ANAT', True))\n",
    "    MAX_DIST_FUNC_ANAT = float(globals().get('MAX_DIST_FUNC_ANAT', np.inf))\n",
    "\n",
    "    twoP_labels_all = _ensure_uint_labels(imread_any(ANAT_LABELS_PATH))\n",
    "    z_size = twoP_labels_all.shape[0]\n",
    "    try:\n",
    "        props3d = regionprops_table(twoP_labels_all, properties=('label','bbox'))\n",
    "        adf3d = pd.DataFrame(props3d)\n",
    "        adf3d = adf3d[adf3d['label'] != 0]\n",
    "        adf3d = adf3d.rename(columns={'bbox-0':'zmin','bbox-1':'ymin','bbox-2':'xmin','bbox-3':'zmax','bbox-4':'ymax','bbox-5':'xmax'})\n",
    "        med_dx_um = float(((adf3d['xmax'] - adf3d['xmin']) * DX).median()) if not adf3d.empty else 0.0\n",
    "        med_dy_um = float(((adf3d['ymax'] - adf3d['ymin']) * DY).median()) if not adf3d.empty else 0.0\n",
    "        med_dz_um = float(((adf3d['zmax'] - adf3d['zmin']) * DZ).median()) if not adf3d.empty else 0.0\n",
    "    except Exception:\n",
    "        med_dx_um = med_dy_um = med_dz_um = 0.0\n",
    "\n",
    "    def _gene_from_mask(path_str):\n",
    "        if 'gene_from_mask' in globals():\n",
    "            try:\n",
    "                return gene_from_mask(path_str)\n",
    "            except Exception:\n",
    "                pass\n",
    "        name = Path(path_str).name\n",
    "        m = re.search(r\"channel\\d+_(.+?)_cp_masks\", name)\n",
    "        gene = m.group(1) if m else name\n",
    "        return gene.replace('sst1_', 'sst1.')\n",
    "\n",
    "    def load_func_for_plane(pr, p_idx):\n",
    "        if '_get_labels_for_plane' in globals():\n",
    "            try:\n",
    "                arr, desc = _get_labels_for_plane(pr, p_idx)\n",
    "                return arr, desc\n",
    "            except Exception:\n",
    "                pass\n",
    "        if '_load_func_labels_for_plane' in globals():\n",
    "            try:\n",
    "                arr, _, desc = globals()['_load_func_labels_for_plane'](p_idx)\n",
    "                return arr, desc\n",
    "            except Exception:\n",
    "                pass\n",
    "        return None, None\n",
    "\n",
    "    def norm_func_labels(arr):\n",
    "        arr = _ensure_uint_labels(arr)\n",
    "        if arr.ndim == 3 and arr.shape[-1] in (3, 4):\n",
    "            arr = arr[..., 0]\n",
    "        if arr.ndim == 3 and arr.shape[0] == 1:\n",
    "            arr = arr[0]\n",
    "        return arr\n",
    "\n",
    "    anat_ids_matched = set()\n",
    "    plane_anat_ids = {}\n",
    "\n",
    "    for p_idx, pr in enumerate(plane_refs):\n",
    "        bz = int(pr.get('best_z', 0))\n",
    "        if bz < 0 or bz >= z_size:\n",
    "            continue\n",
    "        plabel = pr.get('label', f'plane{p_idx}')\n",
    "        anat_slice = _ensure_uint_labels(twoP_labels_all[bz])\n",
    "        func_raw, desc = load_func_for_plane(pr, p_idx)\n",
    "        if func_raw is None:\n",
    "            continue\n",
    "        func_raw = norm_func_labels(func_raw)\n",
    "        if func_raw.ndim != 2:\n",
    "            continue\n",
    "        tform_use = pr.get('tform', globals().get('tform', None))\n",
    "        if tform_use is None:\n",
    "            func_warped = func_raw.copy()\n",
    "        else:\n",
    "            func_warped = resample_labels_nn(func_raw, tform_use, output_shape=anat_slice.shape)\n",
    "        func_warped = _ensure_uint_labels(func_warped)\n",
    "\n",
    "        f_props = regionprops_table(func_warped, properties=('label','centroid'))\n",
    "        a_props = regionprops_table(anat_slice, properties=('label','centroid'))\n",
    "        fdf = pd.DataFrame(f_props).rename(columns={'centroid-0':'cy','centroid-1':'cx'})\n",
    "        adf2d = pd.DataFrame(a_props).rename(columns={'centroid-0':'cy','centroid-1':'cx'})\n",
    "        fdf = fdf[fdf['label'] != 0].reset_index(drop=True)\n",
    "        adf2d = adf2d[adf2d['label'] != 0].reset_index(drop=True)\n",
    "        if fdf.empty or adf2d.empty:\n",
    "            continue\n",
    "\n",
    "        overlap_df = compute_label_overlap(func_warped, anat_slice, min_overlap_voxels=1)\n",
    "        if overlap_df.empty:\n",
    "            continue\n",
    "        overlap_df = overlap_df.rename(columns={'conf_label':'func_label','twoP_label':'anat_label','overlap_voxels':'overlap_px'})\n",
    "\n",
    "        F = fdf[['cx','cy']].to_numpy(); A = adf2d[['cx','cy']].to_numpy()\n",
    "        D = np.sqrt(((F[:,None,:]-A[None,:,:])**2).sum(axis=2))\n",
    "        row_ind, col_ind = _lsa(D)\n",
    "        links = []\n",
    "        for r, c in zip(row_ind, col_ind):\n",
    "            fxp, fyp = float(F[r,0]), float(F[r,1]); axp, ayp = float(A[c,0]), float(A[c,1])\n",
    "            dist_um = float(np.sqrt(((fxp-axp)*DX)**2 + ((fyp-ayp)*DY)**2))\n",
    "            links.append({'func_label': int(fdf.iloc[r]['label']), 'anat_label': int(adf2d.iloc[c]['label']), 'dist_um': dist_um})\n",
    "        links_df = pd.DataFrame(links)\n",
    "        if links_df.empty:\n",
    "            continue\n",
    "        links_df = links_df.merge(overlap_df, on=['func_label','anat_label'], how='left')\n",
    "        links_df['overlap_px'] = links_df['overlap_px'].fillna(0).astype(int)\n",
    "        if REQUIRE_OVERLAP_FUNC_ANAT:\n",
    "            links_df = links_df[links_df['overlap_px'] >= MIN_OVERLAP_FUNC_ANAT]\n",
    "        if np.isfinite(MAX_DIST_FUNC_ANAT) and MAX_DIST_FUNC_ANAT > 0:\n",
    "            links_df = links_df[links_df['dist_um'] <= MAX_DIST_FUNC_ANAT]\n",
    "        if links_df.empty:\n",
    "            continue\n",
    "        anat_ids_matched.update(links_df['anat_label'].tolist())\n",
    "        plane_anat_ids[plabel] = set(links_df['anat_label'].tolist())\n",
    "\n",
    "    if not anat_ids_matched:\n",
    "        print('No anatomy labels matched to functional planes; nothing to filter.')\n",
    "    else:\n",
    "        rows = []\n",
    "        for res in hcr_match_results:\n",
    "            fp = res.get('final_pairs')\n",
    "            if fp is None or fp.empty:\n",
    "                continue\n",
    "            df_conf = res.get('df_conf'); df_2p = res.get('df_2p')\n",
    "            if df_conf is None or df_2p is None:\n",
    "                continue\n",
    "            cmap = df_conf.set_index('label')[['x','y','z']].to_dict('index')\n",
    "            amap = df_2p.set_index('label')[['x','y','z']].to_dict('index')\n",
    "            sub = fp[fp['twoP_label'].isin(anat_ids_matched)]\n",
    "            if sub.empty:\n",
    "                continue\n",
    "            name = Path(res.get('mask_path','unknown')).name\n",
    "            for _, r in sub.iterrows():\n",
    "                cl = int(r['conf_label']); al = int(r['twoP_label'])\n",
    "                c = cmap.get(cl); a = amap.get(al)\n",
    "                if c is None or a is None:\n",
    "                    continue\n",
    "                dx_um = (float(c['x']) - float(a['x'])) * DX\n",
    "                dy_um = (float(c['y']) - float(a['y'])) * DY\n",
    "                dz_um = (float(c['z']) - float(a['z'])) * DZ\n",
    "                rows.append({'mask': name, 'dx_um': dx_um, 'dy_um': dy_um, 'dz_um': dz_um, 'anat_label': al})\n",
    "        if not rows:\n",
    "            print('No confocal pairs intersecting functional-matched anatomy labels.')\n",
    "        else:\n",
    "            df = pd.DataFrame(rows)\n",
    "            df['xy_um'] = np.sqrt(df['dx_um']*df['dx_um'] + df['dy_um']*df['dy_um'])\n",
    "            df['|dz_um|'] = df['dz_um'].abs()\n",
    "            med_off = {'xy_um': df['xy_um'].median(), '|dz_um|': df['|dz_um|'].median()}\n",
    "            med_dxy_um = float(np.sqrt(med_dx_um*med_dx_um + med_dy_um*med_dy_um)) if (med_dx_um > 0 or med_dy_um > 0) else 0.0\n",
    "            n_rows = len(df)\n",
    "            print(f\"n={n_rows} | median XY={med_off['xy_um']:.2f} µm | median |dz|={med_off['|dz_um|']:.2f} µm\")\n",
    "\n",
    "            all_vals = np.concatenate([a for a in (df['xy_um'].to_numpy(), df['|dz_um|'].to_numpy()) if a.size]) if n_rows else np.array([])\n",
    "            y_max = max(10.0, 10.0 * ceil(float(all_vals.max()) / 10.0)) if all_vals.size else 10.0\n",
    "            yticks = np.arange(0.0, y_max + 0.1, 10.0)\n",
    "\n",
    "            fig, axes = plt.subplots(1, 2, figsize=conf_offset_figsize, sharey=True)\n",
    "            plots = [\n",
    "                ('XY plane offsets', df['xy_um'].to_numpy(float), med_off['xy_um'], med_dxy_um, 'XY'),\n",
    "                ('Z offsets', df['|dz_um|'].to_numpy(float), med_off['|dz_um|'], med_dz_um, 'Z'),\n",
    "            ]\n",
    "            for ax, (title, vals, med_val, diam_ref, tick_label) in zip(axes, plots):\n",
    "                if vals.size == 0:\n",
    "                    ax.set_visible(False)\n",
    "                    continue\n",
    "                parts = ax.violinplot([vals], showmeans=False, showmedians=False, showextrema=False)\n",
    "                for pc in parts['bodies']:\n",
    "                    pc.set_facecolor('#cccccc'); pc.set_edgecolor('black'); pc.set_alpha(violin_alpha)\n",
    "                ax.scatter([1], [med_val], **median_marker)\n",
    "                if diam_ref > 0:\n",
    "                    ax.axhline(diam_ref, color=line_color, linestyle='--', linewidth=1.3)\n",
    "                ax.set_ylabel('offset (µm)')\n",
    "                ax.set_title(title)\n",
    "                ax.set_xticks([1]); ax.set_xticklabels([tick_label])\n",
    "                ax.set_ylim(0, y_max); ax.set_yticks(yticks)\n",
    "                ax.grid(axis='y', alpha=0.2)\n",
    "            fig.suptitle('Confocal ↔ Anatomy centroid offsets — XY vs Z (anat labels matched to functional planes)')\n",
    "            plt.tight_layout(); plt.show()\n",
    "\n",
    "            planes = []\n",
    "            for p_idx, pr in enumerate(plane_refs):\n",
    "                plabel = pr.get('label', f'plane{p_idx}')\n",
    "                anat_ids_plane = plane_anat_ids.get(plabel, set())\n",
    "                genes = set()\n",
    "                for res in hcr_match_results:\n",
    "                    fp = res.get('final_pairs')\n",
    "                    if fp is None or fp.empty:\n",
    "                        continue\n",
    "                    sub = fp[fp['twoP_label'].isin(anat_ids_plane)]\n",
    "                    if sub.empty:\n",
    "                        continue\n",
    "                    gene = _gene_from_mask(res.get('mask_path', 'unknown'))\n",
    "                    genes.add(gene)\n",
    "                planes.append({'plane': plabel, 'genes': ', '.join(sorted(genes)) if genes else '-', 'anat_labels': len(anat_ids_plane)})\n",
    "            if planes:\n",
    "                plane_df = pd.DataFrame(planes)\n",
    "                print('Genes present per plane (anat labels that match functional planes):')\n",
    "                display(plane_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53611651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [54]\n",
    "# Functional plane masks + intensity warped into 2P space (per plane, using stored transforms)\n",
    "import numpy as np\n",
    "import tifffile as tiff\n",
    "from pathlib import Path\n",
    "\n",
    "if 'plane_refs' not in globals() or not plane_refs:\n",
    "    print('No plane_refs available; run earlier cells to populate them.')\n",
    "else:\n",
    "    try:\n",
    "        anat_full = _ensure_uint_labels(imread_any(ANAT_LABELS_PATH))\n",
    "    except Exception as e:\n",
    "        anat_full = None\n",
    "        print('Could not load ANAT_LABELS_PATH:', e)\n",
    "    if anat_full is None:\n",
    "        print('Missing anatomy labels; cannot warp functional data.')\n",
    "    else:\n",
    "        DZ = float(VOX_ANAT.get('Z', 1.0)) if 'VOX_ANAT' in globals() else 1.0\n",
    "        DY = float(VOX_ANAT.get('Y', 1.0)) if 'VOX_ANAT' in globals() else 1.0\n",
    "        DX = float(VOX_ANAT.get('X', 1.0)) if 'VOX_ANAT' in globals() else 1.0\n",
    "        mask_outpaths = []\n",
    "        ref_outpaths = []\n",
    "\n",
    "        def _load_func_plane(pr, p_idx):\n",
    "            if '_get_labels_for_plane' in globals():\n",
    "                try:\n",
    "                    arr, _ = _get_labels_for_plane(pr, p_idx)\n",
    "                    if arr is not None:\n",
    "                        return arr\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if '_load_func_labels_for_plane' in globals():\n",
    "                try:\n",
    "                    arr, _, _ = globals()['_load_func_labels_for_plane'](p_idx)\n",
    "                    if arr is not None:\n",
    "                        return arr\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if 'FUNC_LABELS_PATH' in globals():\n",
    "                try:\n",
    "                    arr = _ensure_uint_labels(imread_any(FUNC_LABELS_PATH))\n",
    "                    if arr.ndim == 3 and p_idx < arr.shape[0]:\n",
    "                        return arr[p_idx]\n",
    "                    if arr.ndim == 2:\n",
    "                        return arr\n",
    "                except Exception:\n",
    "                    pass\n",
    "            # search output dirs for masks named like plane label\n",
    "            lbl = pr.get('label', f'plane{p_idx}')\n",
    "            bases = []\n",
    "            if 'OUT_SEG' in globals():\n",
    "                bases.append(Path(OUT_SEG))\n",
    "                bases.append(Path(OUT_SEG) / 'cp_masks')\n",
    "            if 'OUTDIR' in globals():\n",
    "                bases.append(Path(OUTDIR))\n",
    "            for base in bases:\n",
    "                if not base.exists():\n",
    "                    continue\n",
    "                for pat in [f\"{lbl}*mask*.tif\", f\"{lbl}*labels*.tif\", f\"{lbl}*func*.tif\"]:\n",
    "                    for p in base.glob(pat):\n",
    "                        try:\n",
    "                            arr = _ensure_uint_labels(imread_any(p))\n",
    "                            if arr is not None:\n",
    "                                return arr\n",
    "                        except Exception:\n",
    "                            pass\n",
    "            return None\n",
    "\n",
    "        def _load_func_ref(pr, p_idx, lbl=None):\n",
    "            def _try_path(path):\n",
    "                if not path:\n",
    "                    return None\n",
    "                try:\n",
    "                    return imread_any(path)\n",
    "                except Exception:\n",
    "                    return None\n",
    "            def _try_outdir_guess(label):\n",
    "                bases = []\n",
    "                if 'OUT_DERIVED' in globals():\n",
    "                    bases.append(Path(OUT_DERIVED))\n",
    "                if 'OUT_RAW' in globals():\n",
    "                    bases.append(Path(OUT_RAW))\n",
    "                if 'OUT_REG' in globals():\n",
    "                    bases.append(Path(OUT_REG))\n",
    "                if 'OUTDIR' in globals():\n",
    "                    bases.append(Path(OUTDIR))\n",
    "                patterns = [\n",
    "                    f\"{label}*func_ref*.tif\",\n",
    "                    f\"{label}*func_ref*.tiff\",\n",
    "                    f\"{label}*func_img*.tif\",\n",
    "                    f\"{label}*func*.tif\",\n",
    "                    f\"{label}*ref_norm*.tif\",\n",
    "                    f\"{label}*ref_raw*.tif\",\n",
    "                ]\n",
    "                for base in bases:\n",
    "                    if not base.exists():\n",
    "                        continue\n",
    "                    for pat in patterns:\n",
    "                        for p in base.glob(pat):\n",
    "                            if p.is_file():\n",
    "                                arr = _try_path(p)\n",
    "                                if arr is not None:\n",
    "                                    return arr\n",
    "                return None\n",
    "            # prefer references already attached to plane_refs (from cell [12])\n",
    "            for key in ('ref2d', 'ref2d_raw'):\n",
    "                if key in pr and pr[key] is not None:\n",
    "                    return pr[key]\n",
    "            # per-plane hints\n",
    "            for key in ('func_ref', 'func_ref_path', 'func_img', 'func_img_path'):\n",
    "                arr = _try_path(pr.get(key))\n",
    "                if arr is not None:\n",
    "                    return arr\n",
    "            # global fallbacks\n",
    "            for key in ('FUNC_REF_PATH', 'FUNC_IMG_PATH', 'FUNC_PATH'):\n",
    "                if key in globals():\n",
    "                    arr = _try_path(globals().get(key))\n",
    "                    if arr is not None:\n",
    "                        return arr\n",
    "            # OUTDIR search using plane label\n",
    "            if lbl is not None:\n",
    "                arr = _try_outdir_guess(lbl)\n",
    "                if arr is not None:\n",
    "                    return arr\n",
    "            return None\n",
    "\n",
    "        def _as_2d(arr, p_idx):\n",
    "            if arr is None:\n",
    "                return None\n",
    "            if arr.ndim == 3 and arr.shape[-1] in (3, 4):\n",
    "                arr = arr[..., 0]\n",
    "            if arr.ndim == 3 and p_idx < arr.shape[0]:\n",
    "                arr = arr[p_idx]\n",
    "            if arr.ndim == 3 and arr.shape[0] == 1:\n",
    "                arr = arr[0]\n",
    "            if arr.ndim != 2:\n",
    "                return None\n",
    "            return arr\n",
    "\n",
    "        for p_idx, pr in enumerate(plane_refs):\n",
    "            lbl = pr.get('label', f'plane{p_idx}')\n",
    "            # --- Masks ---\n",
    "            func_raw = _load_func_plane(pr, p_idx)\n",
    "            func_raw = _as_2d(func_raw, p_idx)\n",
    "            if func_raw is None:\n",
    "                print(f\"[warp] Missing functional labels for {lbl}; skipping masks.\")\n",
    "            else:\n",
    "                bz = int(pr.get('best_z', 0))\n",
    "                anat_slice = anat_full[bz] if anat_full.ndim == 3 and bz < anat_full.shape[0] else anat_full\n",
    "                tform_use = pr.get('tform', globals().get('tform', None))\n",
    "                try:\n",
    "                    if tform_use is None:\n",
    "                        if func_raw.shape == anat_slice.shape:\n",
    "                            func_warped = func_raw\n",
    "                        else:\n",
    "                            from skimage.transform import AffineTransform\n",
    "                            func_warped = resample_labels_nn(func_raw, AffineTransform(), output_shape=anat_slice.shape)\n",
    "                    else:\n",
    "                        func_warped = resample_labels_nn(func_raw, tform_use, output_shape=anat_slice.shape)\n",
    "                except Exception as e:\n",
    "                    print(f\"[warp] Resample failed for mask {lbl}: {e}\")\n",
    "                    func_warped = None\n",
    "                if func_warped is not None:\n",
    "                    func_warped = _ensure_uint_labels(func_warped)\n",
    "                    out_path = OUT_DERIVED / f\"{lbl}_func_mask_in_2p.tif\"\n",
    "                    try:\n",
    "                        tiff_kwargs = {\n",
    "                            'imagej': True,\n",
    "                            'compression': 'deflate',\n",
    "                            'metadata': {'axes': 'YX', 'spacing': DY, 'unit': 'um'}\n",
    "                        }\n",
    "                        dtype = np.uint16 if func_warped.max() <= np.iinfo(np.uint16).max else np.uint32\n",
    "                        tiff.imwrite(out_path, func_warped.astype(dtype), **tiff_kwargs)\n",
    "                        mask_outpaths.append(out_path)\n",
    "                        print(f\"[warp] Saved {out_path} (mask, shape={func_warped.shape}, best_z={bz})\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"[warp] Failed to save {out_path}: {e}\")\n",
    "\n",
    "            # --- Intensity reference ---\n",
    "            func_ref = _load_func_ref(pr, p_idx, lbl)\n",
    "            func_ref = _as_2d(func_ref, p_idx)\n",
    "            if func_ref is None:\n",
    "                print(f\"[warp] Missing functional reference for {lbl}; skipping intensity.\")\n",
    "                continue\n",
    "            bz = int(pr.get('best_z', 0))\n",
    "            anat_slice = anat_full[bz] if anat_full.ndim == 3 and bz < anat_full.shape[0] else anat_full\n",
    "            tform_use = pr.get('tform', globals().get('tform', None))\n",
    "            try:\n",
    "                if tform_use is None:\n",
    "                    if func_ref.shape == anat_slice.shape:\n",
    "                        func_ref_warped = func_ref\n",
    "                    else:\n",
    "                        from skimage.transform import AffineTransform\n",
    "                        resampler = resample_image if 'resample_image' in globals() else resample_labels_nn\n",
    "                        func_ref_warped = resampler(func_ref, AffineTransform(), output_shape=anat_slice.shape)\n",
    "                else:\n",
    "                    resampler = resample_image if 'resample_image' in globals() else resample_labels_nn\n",
    "                    func_ref_warped = resampler(func_ref, tform_use, output_shape=anat_slice.shape)\n",
    "            except Exception as e:\n",
    "                print(f\"[warp] Resample failed for reference {lbl}: {e}\")\n",
    "                continue\n",
    "            func_ref_warped = np.asarray(func_ref_warped, dtype=np.float32)\n",
    "            out_path_ref = OUT_DERIVED / f\"{lbl}_func_ref_in_2p.tif\"\n",
    "            try:\n",
    "                tiff_kwargs = {\n",
    "                    'imagej': True,\n",
    "                    'compression': 'deflate',\n",
    "                    'metadata': {'axes': 'YX', 'spacing': DY, 'unit': 'um'}\n",
    "                }\n",
    "                tiff.imwrite(out_path_ref, func_ref_warped, **tiff_kwargs)\n",
    "                ref_outpaths.append(out_path_ref)\n",
    "                print(f\"[warp] Saved {out_path_ref} (intensity, shape={func_ref_warped.shape}, best_z={bz})\")\n",
    "            except Exception as e:\n",
    "                print(f\"[warp] Failed to save {out_path_ref}: {e}\")\n",
    "\n",
    "        if mask_outpaths:\n",
    "            print('Warped functional masks saved:' + ''.join(map(str, mask_outpaths)))\n",
    "        if ref_outpaths:\n",
    "            print('Warped functional references saved:' + ''.join(map(str, ref_outpaths)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f5200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [55]\n",
    "# Stimulus timeline (blocks + per-stim bars)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Edit colors here by stimulus type code (LLC, LLB, RLC, RLB, combos with '+')\n",
    "stim_palette = {\n",
    "    'LLC': \"#0a5910\",  # Left, continuous\n",
    "    'LLB': \"#34B18C\",  # Left, bout-like\n",
    "    'RLC': \"#4D0C2C\",  # Right, continuous\n",
    "    'RLB': \"#cf368f\",  # Right, bout-like\n",
    "    'LLC+RLC': \"#281578\",\n",
    "    'LLC+RLB': \"#26200b\",\n",
    "    'LLB+RLC': \"#989999\",\n",
    "    'LLB+RLB': \"#94cae3\",\n",
    "}\n",
    "\n",
    "EXPERIMENT_LOG_CSV = globals().get('EXPERIMENT_LOG_CSV', None)\n",
    "STIM_TIME_SCALE = float(globals().get('STIM_TIME_SCALE', 1.0))  # set to 0.001 if times are ms\n",
    "\n",
    "MEASURE_START_BLOCK = globals().get('MEASURE_START_BLOCK', 1)\n",
    "MEASURE_START_EVENT = globals().get('MEASURE_START_EVENT', 'start')\n",
    "REMOVE_INTERBLOCK_GAPS = bool(globals().get('REMOVE_INTERBLOCK_GAPS', True))\n",
    "\n",
    "EXPERIMENT_META_CSV = globals().get('EXPERIMENT_META_CSV', None)\n",
    "FRAME_RATE = globals().get('FRAME_RATE', None)\n",
    "\n",
    "\n",
    "def _find_experiment_log():\n",
    "    if 'FISH_DIR' not in globals() or globals().get('FISH_DIR') is None:\n",
    "        return None\n",
    "    base = Path(FISH_DIR) / '01_raw' / '2p' / 'metadata'\n",
    "    if not base.exists():\n",
    "        return None\n",
    "    fish_id = globals().get('FISH_ID', None)\n",
    "    patterns = []\n",
    "    if fish_id:\n",
    "        patterns.append(f\"*{fish_id}*experiment_log*.csv\")\n",
    "    patterns.append(\"*experiment_log*.csv\")\n",
    "    hits = []\n",
    "    for pat in patterns:\n",
    "        hits.extend(sorted(base.glob(pat)))\n",
    "    if not hits:\n",
    "        return None\n",
    "    hits = sorted(set(hits), key=lambda p: p.stat().st_mtime)\n",
    "    return hits[-1]\n",
    "\n",
    "\n",
    "def _load_events_df(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if df.empty:\n",
    "        raise ValueError('experiment log is empty')\n",
    "    cols = {c: c.strip().lower() for c in df.columns}\n",
    "    df = df.rename(columns=cols)\n",
    "    event_cols = [c for c in df.columns if 'event' in c]\n",
    "    time_cols = [c for c in df.columns if 'timestamp' in c or re.search(r'\btime\b', c)]\n",
    "    if 'event' in df.columns:\n",
    "        event_col = 'event'\n",
    "    elif event_cols:\n",
    "        event_col = event_cols[0]\n",
    "    else:\n",
    "        event_col = None\n",
    "    if 'timestamp' in df.columns:\n",
    "        time_col = 'timestamp'\n",
    "    elif 'time' in df.columns:\n",
    "        time_col = 'time'\n",
    "    elif time_cols:\n",
    "        time_col = time_cols[0]\n",
    "    else:\n",
    "        time_col = None\n",
    "    if event_col is None or time_col is None:\n",
    "        raise ValueError(f\"Could not infer event/time columns from {list(df.columns)}\")\n",
    "    df = df[[event_col, time_col]].rename(columns={event_col: 'event', time_col: 'time'})\n",
    "    df['event'] = df['event'].astype(str).str.strip()\n",
    "    df['time'] = pd.to_numeric(df['time'], errors='coerce')\n",
    "    df = df.dropna(subset=['event', 'time']).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def _find_metadata_csv():\n",
    "    if 'FISH_DIR' not in globals() or globals().get('FISH_DIR') is None:\n",
    "        return None\n",
    "    base = Path(FISH_DIR) / '01_raw' / '2p' / 'metadata'\n",
    "    if not base.exists():\n",
    "        return None\n",
    "    fish_id = globals().get('FISH_ID', None)\n",
    "    patterns = []\n",
    "    if fish_id:\n",
    "        patterns.append(f\"*{fish_id}*metadata*.csv\")\n",
    "    patterns.append(\"*metadata*.csv\")\n",
    "    hits = []\n",
    "    for pat in patterns:\n",
    "        hits.extend(sorted(base.glob(pat)))\n",
    "    hits = [p for p in hits if 'experiment_log' not in p.name.lower()]\n",
    "    if not hits:\n",
    "        return None\n",
    "    hits = sorted(set(hits), key=lambda p: p.stat().st_mtime)\n",
    "    return hits[-1]\n",
    "\n",
    "\n",
    "def _load_metadata_params(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if df.empty:\n",
    "        raise ValueError('metadata is empty')\n",
    "    cols = {c: c.strip().lower() for c in df.columns}\n",
    "    df = df.rename(columns=cols)\n",
    "    if 'parameter' not in df.columns or 'value' not in df.columns:\n",
    "        raise ValueError(f\"metadata csv missing parameter/value columns: {list(df.columns)}\")\n",
    "    params = {}\n",
    "    for _, row in df.iterrows():\n",
    "        key = str(row['parameter']).strip().lower()\n",
    "        params[key] = row['value']\n",
    "    return params\n",
    "\n",
    "\n",
    "def _parse_float(val):\n",
    "    if val is None:\n",
    "        return None\n",
    "    try:\n",
    "        return float(val)\n",
    "    except Exception:\n",
    "        s = re.sub(r'[^0-9eE+\\-.]', '', str(val))\n",
    "        return float(s) if s else None\n",
    "def _block_key(b):\n",
    "    m = re.search(r'\\d+', b)\n",
    "    return int(m.group(0)) if m else b\n",
    "\n",
    "\n",
    "\n",
    "if FRAME_RATE is None:\n",
    "    meta_path = Path(EXPERIMENT_META_CSV) if EXPERIMENT_META_CSV else _find_metadata_csv()\n",
    "    if meta_path is not None and Path(meta_path).exists():\n",
    "        try:\n",
    "            params = _load_metadata_params(meta_path)\n",
    "            fr_val = params.get('framerate', None)\n",
    "            if fr_val is None:\n",
    "                fr_val = params.get('frame_rate', params.get('fps', None))\n",
    "            FRAME_RATE = _parse_float(fr_val)\n",
    "            if FRAME_RATE is not None:\n",
    "                print(f\"[stim] loaded framerate: {FRAME_RATE} Hz from {meta_path}\")\n",
    "            else:\n",
    "                print(f\"[stim] framerate not found in {meta_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[stim] failed to load metadata {meta_path}: {e}\")\n",
    "    else:\n",
    "        print('[stim] metadata CSV not found; FRAME_RATE remains None')\n",
    "globals()['FRAME_RATE'] = FRAME_RATE\n",
    "df_evt = None\n",
    "log_path = Path(EXPERIMENT_LOG_CSV) if EXPERIMENT_LOG_CSV else _find_experiment_log()\n",
    "if log_path is not None and Path(log_path).exists():\n",
    "    try:\n",
    "        df_evt = _load_events_df(log_path)\n",
    "        df_evt['time'] = df_evt['time'].astype(float) * STIM_TIME_SCALE\n",
    "        print(f\"[stim] loaded experiment log: {log_path} (rows={len(df_evt)})\")\n",
    "    except Exception as e:\n",
    "        print(f\"[stim] failed to load experiment log {log_path}: {e}\")\n",
    "        df_evt = None\n",
    "else:\n",
    "    print('[stim] experiment log not found; using fallback text if provided')\n",
    "\n",
    "if df_evt is None:\n",
    "    events_lines = [\n",
    "        'event\\ttimestamp',\n",
    "        'B0_start\\t0.0010569',\n",
    "        'B0_end\\t777.1481037',\n",
    "        'B0_interblock_pause\\t777.148128',\n",
    "        'B1_start\\t797.2125599',\n",
    "        'B1_prestim0_pause\\t797.2125853',\n",
    "        'B1_stim0_LLB+RLC\\t807.2444901',\n",
    "        'B1_poststim0_pause\\t835.7528767',\n",
    "        'B1_prestim1_pause\\t845.7853059',\n",
    "        'B1_stim1_RLC\\t855.8174374',\n",
    "        'B1_poststim1_pause\\t884.3257082',\n",
    "        'B1_prestim2_pause\\t894.3581208',\n",
    "        'B1_stim2_LLC\\t904.3897776',\n",
    "        'B1_poststim2_pause\\t932.8982779',\n",
    "        'B1_prestim3_pause\\t942.9307749',\n",
    "        'B1_stim3_RLB\\t952.9628211',\n",
    "        'B1_poststim3_pause\\t981.4715352',\n",
    "        'B1_prestim4_pause\\t991.503811',\n",
    "        'B1_stim4_LLB\\t1001.535818',\n",
    "        'B1_poststim4_pause\\t1030.044107',\n",
    "        'B1_prestim5_pause\\t1040.076335',\n",
    "        'B1_stim5_LLC+RLB\\t1050.109128',\n",
    "        'B1_poststim5_pause\\t1078.616961',\n",
    "        'B1_prestim6_pause\\t1088.649402',\n",
    "        'B1_stim6_LLB+RLB\\t1098.68157',\n",
    "        'B1_poststim6_pause\\t1127.190509',\n",
    "        'B1_prestim7_pause\\t1137.22209',\n",
    "        'B1_stim7_LLC+RLC\\t1147.254373',\n",
    "        'B1_poststim7_pause\\t1175.762498',\n",
    "        'B1_prestim8_pause\\t1185.79485',\n",
    "        'B1_stim8_LLB+RLC\\t1195.827073',\n",
    "        'B1_poststim8_pause\\t1224.335683',\n",
    "        'B1_prestim9_pause\\t1234.36798',\n",
    "        'B1_stim9_RLC\\t1244.400215',\n",
    "        'B1_poststim9_pause\\t1272.908224',\n",
    "        'B1_prestim10_pause\\t1282.940385',\n",
    "        'B1_stim10_LLB\\t1292.972826',\n",
    "        'B1_poststim10_pause\\t1321.48119',\n",
    "        'B1_prestim11_pause\\t1331.513386',\n",
    "        'B1_stim11_LLB+RLB\\t1341.54579',\n",
    "        'B1_poststim11_pause\\t1370.053718',\n",
    "        'B1_prestim12_pause\\t1380.086832',\n",
    "        'B1_stim12_RLB\\t1390.118587',\n",
    "        'B1_poststim12_pause\\t1418.626652',\n",
    "        'B1_prestim13_pause\\t1428.659261',\n",
    "        'B1_stim13_LLC\\t1438.69125',\n",
    "        'B1_poststim13_pause\\t1467.199784',\n",
    "        'B1_prestim14_pause\\t1477.23233',\n",
    "        'B1_stim14_LLC+RLC\\t1487.264333',\n",
    "        'B1_poststim14_pause\\t1515.772707',\n",
    "        'B1_prestim15_pause\\t1525.804788',\n",
    "        'B1_stim15_LLC+RLB\\t1535.836865',\n",
    "        'B1_poststim15_pause\\t1564.345568',\n",
    "        'B1_end\\t1574.378153',\n",
    "        'B1_interblock_pause\\t1574.378182',\n",
    "        'B2_start\\t1594.442437',\n",
    "        'B2_prestim16_pause\\t1594.442469',\n",
    "        'B2_stim16_LLC+RLC\\t1604.474414',\n",
    "        'B2_poststim16_pause\\t1632.982893',\n",
    "        'B2_prestim17_pause\\t1643.015816',\n",
    "        'B2_stim17_LLB\\t1653.047204',\n",
    "        'B2_poststim17_pause\\t1681.555664',\n",
    "        'B2_prestim18_pause\\t1691.587675',\n",
    "        'B2_stim18_LLC+RLB\\t1701.620197',\n",
    "        'B2_poststim18_pause\\t1730.128377',\n",
    "        'B2_prestim19_pause\\t1740.160975',\n",
    "        'B2_stim19_LLB+RLC\\t1750.193086',\n",
    "        'B2_poststim19_pause\\t1778.701034',\n",
    "        'B2_prestim20_pause\\t1788.733421',\n",
    "        'B2_stim20_RLC\\t1798.765686',\n",
    "        'B2_poststim20_pause\\t1827.274189',\n",
    "        'B2_prestim21_pause\\t1837.306837',\n",
    "        'B2_stim21_LLC\\t1847.338388',\n",
    "        'B2_poststim21_pause\\t1875.848659',\n",
    "        'B2_prestim22_pause\\t1885.879262',\n",
    "        'B2_stim22_RLB\\t1895.911546',\n",
    "        'B2_poststim22_pause\\t1924.419848',\n",
    "        'B2_prestim23_pause\\t1934.452289',\n",
    "        'B2_stim23_LLB+RLB\\t1944.484321',\n",
    "        'B2_poststim23_pause\\t1972.992424',\n",
    "        'B2_prestim24_pause\\t1983.025132',\n",
    "        'B2_stim24_LLB+RLB\\t1993.057261',\n",
    "        'B2_poststim24_pause\\t2021.565196',\n",
    "        'B2_prestim25_pause\\t2031.598637',\n",
    "        'B2_stim25_LLC\\t2041.629963',\n",
    "        'B2_poststim25_pause\\t2070.138462',\n",
    "        'B2_prestim26_pause\\t2080.17042',\n",
    "        'B2_stim26_LLC+RLC\\t2090.202876',\n",
    "        'B2_poststim26_pause\\t2118.711303',\n",
    "        'B2_prestim27_pause\\t2128.743432',\n",
    "        'B2_stim27_LLB+RLC\\t2138.775671',\n",
    "        'B2_poststim27_pause\\t2167.283619',\n",
    "        'B2_prestim28_pause\\t2177.316613',\n",
    "        'B2_stim28_RLC\\t2187.348411',\n",
    "        'B2_poststim28_pause\\t2215.856799',\n",
    "        'B2_prestim29_pause\\t2225.889512',\n",
    "        'B2_stim29_LLC+RLB\\t2235.921534',\n",
    "        'B2_poststim29_pause\\t2264.429023',\n",
    "        'B2_prestim30_pause\\t2274.462027',\n",
    "        'B2_stim30_LLB\\t2284.493859',\n",
    "        'B2_poststim30_pause\\t2313.002631',\n",
    "        'B2_prestim31_pause\\t2323.035036',\n",
    "        'B2_stim31_RLB\\t2333.067047',\n",
    "        'B2_poststim31_pause\\t2361.575461',\n",
    "        'B2_end\\t2371.607178'\n",
    "    ]\n",
    "    events_txt = \"\".join(events_lines).strip()\n",
    "    lines = [ln.strip() for ln in events_txt.splitlines() if ln.strip() and not ln.startswith('event')]\n",
    "    rows = []\n",
    "    for ln in lines:\n",
    "        evt, ts = ln.split('\t')\n",
    "        rows.append({'event': evt, 'time': float(ts)})\n",
    "    df_evt = pd.DataFrame(rows)\n",
    "\n",
    "if MEASURE_START_BLOCK is not None:\n",
    "    if isinstance(MEASURE_START_BLOCK, str):\n",
    "        block_label = MEASURE_START_BLOCK\n",
    "    else:\n",
    "        block_label = f\"B{int(MEASURE_START_BLOCK)}\"\n",
    "    start_event = f\"{block_label}_{MEASURE_START_EVENT}\"\n",
    "    start_match = df_evt.loc[df_evt['event'] == start_event, 'time']\n",
    "    if len(start_match):\n",
    "        t0 = float(start_match.iloc[0])\n",
    "    else:\n",
    "        block_rows = df_evt[df_evt[\"event\"].str.startswith(f\"{block_label}_\")]\n",
    "        t0 = float(block_rows[\"time\"].min()) if not block_rows.empty else None\n",
    "    if t0 is not None:\n",
    "        df_evt[\"time\"] = df_evt[\"time\"] - t0\n",
    "        df_evt = df_evt[df_evt[\"time\"] >= 0].reset_index(drop=True)\n",
    "        globals()[\"MEASURE_START_TIME\"] = t0\n",
    "        print(f\"[stim] measurement start: {start_event} @ {t0:.3f}s; shifting times so start=0\")\n",
    "    else:\n",
    "        print(f\"[stim] measurement start not found for block {block_label}; no shift applied\")\n",
    "\n",
    "# Identify blocks dynamically\n",
    "block_codes = []\n",
    "for ev in df_evt['event']:\n",
    "    m = re.match(r'^(B\\d+)_', ev)\n",
    "    if m:\n",
    "        block_codes.append(m.group(1))\n",
    "block_codes = sorted(set(block_codes), key=_block_key)\n",
    "if not block_codes:\n",
    "    block_codes = ['B0', 'B1', 'B2']\n",
    "\n",
    "# Parse blocks\n",
    "blocks = []\n",
    "for b in block_codes:\n",
    "    block_events = df_evt[df_evt['event'].str.startswith(f\"{b}_\")]\n",
    "    if block_events.empty:\n",
    "        continue\n",
    "    bstart = None\n",
    "    bend = None\n",
    "    s = block_events.loc[block_events['event'] == f\"{b}_start\", 'time']\n",
    "    if len(s):\n",
    "        bstart = float(s.iloc[0])\n",
    "    e = block_events.loc[block_events['event'] == f\"{b}_end\", 'time']\n",
    "    if len(e):\n",
    "        bend = float(e.iloc[0])\n",
    "    if bstart is None:\n",
    "        bstart = float(block_events['time'].min())\n",
    "    ib = block_events.loc[block_events['event'] == f\"{b}_interblock_pause\", 'time']\n",
    "    ib_time = float(ib.iloc[0]) if len(ib) else None\n",
    "    if bend is None:\n",
    "        bend = ib_time\n",
    "    elif ib_time is not None:\n",
    "        bend = ib_time\n",
    "    if bend is None:\n",
    "        bend = float(block_events['time'].max())\n",
    "    blocks.append({'block': b, 'start': bstart, 'end': bend})\n",
    "\n",
    "# Optionally remove interblock gaps (when acquisition only during blocks)\n",
    "if REMOVE_INTERBLOCK_GAPS and blocks:\n",
    "    blocks = sorted(blocks, key=lambda b: b['start'])\n",
    "    shift_map = {}\n",
    "    shift = 0.0\n",
    "    prev_end = None\n",
    "    for blk in blocks:\n",
    "        orig_start = blk['start']\n",
    "        orig_end = blk['end']\n",
    "        if prev_end is not None:\n",
    "            gap = orig_start - prev_end\n",
    "            if gap < 0:\n",
    "                gap = 0.0\n",
    "            shift += gap\n",
    "        shift_map[blk['block']] = shift\n",
    "        blk['start'] = orig_start - shift\n",
    "        blk['end'] = orig_end - shift\n",
    "        prev_end = orig_end\n",
    "\n",
    "    def _shift_evt_time(row):\n",
    "        m = re.match(r'^(B\\d+)_', row['event'])\n",
    "        if not m:\n",
    "            return row['time']\n",
    "        return row['time'] - shift_map.get(m.group(1), 0.0)\n",
    "\n",
    "    df_evt['time'] = df_evt.apply(_shift_evt_time, axis=1)\n",
    "    df_evt = df_evt.sort_values('time').reset_index(drop=True)\n",
    "\n",
    "# Parse stimuli and pair with post-stim pause as end\n",
    "stims = []\n",
    "for _, row in df_evt.iterrows():\n",
    "    evt = row['event']\n",
    "    m = re.match(r'^(B\\d+)_stim(\\d+)_(.+)$', evt)\n",
    "    if not m:\n",
    "        continue\n",
    "    block = m.group(1)\n",
    "    stim_idx = int(m.group(2))\n",
    "    stim_type = m.group(3)\n",
    "    t0 = float(row['time'])\n",
    "\n",
    "    # find matching poststim for end time\n",
    "    end_time = None\n",
    "    post_name = f\"{block}_poststim{stim_idx}_pause\"\n",
    "    match = df_evt.loc[df_evt['event'] == post_name, 'time']\n",
    "    if len(match):\n",
    "        end_time = float(match.iloc[0])\n",
    "    if end_time is None:\n",
    "        after = df_evt[(df_evt['time'] > t0) & df_evt['event'].str.startswith(f\"{block}_\")].sort_values('time')\n",
    "        end_time = float(after['time'].iloc[0]) if not after.empty else t0 + 10.0\n",
    "    stims.append({\n",
    "        'block': block,\n",
    "        'stim_idx': stim_idx,\n",
    "        'type': stim_type,\n",
    "        'start': t0,\n",
    "        'end': end_time,\n",
    "        'duration': end_time - t0,\n",
    "    })\n",
    "\n",
    "df_stim = pd.DataFrame(stims)\n",
    "if df_stim.empty:\n",
    "    print('No stimuli found.')\n",
    "else:\n",
    "    # Timeline plot\n",
    "    fig, ax = plt.subplots(figsize=(14, 4))\n",
    "    # Blocks as background bands\n",
    "    block_colors = {'B0': '#eeeeee', 'B1': '#e0f0ff', 'B2': '#e6f5e1'}\n",
    "    for blk in blocks:\n",
    "        ax.axvspan(blk['start'], blk['end'], color=block_colors.get(blk['block'], '#f5f5f5'), alpha=0.5, label=f\"{blk['block']}\" if blk['block'] not in getattr(ax, '_block_labels_done', set()) else None)\n",
    "        if not hasattr(ax, '_block_labels_done'):\n",
    "            ax._block_labels_done = set()\n",
    "        ax._block_labels_done.add(blk['block'])\n",
    "        ax.text((blk['start'] + blk['end'])/2, 1.01, blk['block'], ha='center', va='bottom', transform=ax.get_xaxis_transform(), fontsize=9)\n",
    "\n",
    "    # Stim bars (shorter height, no text)\n",
    "    y0, bar_h = 0.35, 0.3\n",
    "    for _, r in df_stim.sort_values(['block','stim_idx']).iterrows():\n",
    "        color = stim_palette.get(r['type'], '#999999')\n",
    "        ax.broken_barh([(r['start'], r['duration'])], (y0, bar_h), facecolors=color, edgecolors='black', linewidth=0.4)\n",
    "\n",
    "    # Midline through bars to indicate time axis\n",
    "    x_min, x_max = float(df_evt['time'].min()), float(df_evt['time'].max())\n",
    "    y_mid = y0 + bar_h / 2.0\n",
    "    ax.hlines(y_mid, x_min, x_max, colors='black', linewidth=1.2, linestyles='-')\n",
    "\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title('Stimulus timeline (baseline + stimulus blocks)')\n",
    "\n",
    "    # Legend for stimulus types (from palette)\n",
    "    from matplotlib.patches import Patch\n",
    "    used_types = df_stim['type'].unique().tolist()\n",
    "    legend_handles = [Patch(facecolor=stim_palette.get(t, '#999999'), edgecolor='black', label=t) for t in used_types]\n",
    "    if legend_handles:\n",
    "        ax.legend(handles=legend_handles, title='Stimulus type', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Summary table\n",
    "    summary = df_stim.groupby(['block', 'type']).agg(n=('type','size'), duration_s=('duration','sum')).reset_index()\n",
    "    try:\n",
    "        display(summary)\n",
    "    except Exception:\n",
    "        print(summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857467d6",
   "metadata": {},
   "source": [
    "### [56] Average dF/F per stimulus and gene\n",
    "Plot stimulus-locked average dF/F traces per gene using Suite2p and the experiment log.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c0a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [56]\n",
    "# Average dF/F per stimulus and gene (Suite2p)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "WINDOW_PRE_SEC = 10\n",
    "WINDOW_POST_SEC = 50\n",
    "TARGET_STIM_TYPES = globals().get('TARGET_STIM_TYPES', None)  # list or None\n",
    "MIN_SEGMENTS = int(globals().get('MIN_SEGMENTS', 3))\n",
    "USE_SEM = bool(globals().get('USE_SEM', True))\n",
    "MAX_GENES = int(globals().get('MAX_GENES', 12))\n",
    "STIM_ORDER = [\"LLB\", \"LLC\", \n",
    "              \"RLB\", \"RLC\", \n",
    "              \"LLB+RLC\", \"LLC+RLB\", \n",
    "              \"LLB+RLB\", \"LLC+RLC\"]\n",
    "PLOT_NCOLS = int(globals().get('PLOT_NCOLS', 2))\n",
    "FIG_WIDTH = float(globals().get('FIG_WIDTH', 12.0))\n",
    "FIG_ROW_HEIGHT = float(globals().get('FIG_ROW_HEIGHT', 3.5))\n",
    "CONF_FUNC_CSV = globals().get('CONF_FUNC_CSV', str(OUT_REG / 'conf_to_func_pairs.csv'))\n",
    "\n",
    "if 'df_stim' not in globals() or globals().get('df_stim') is None or globals()['df_stim'].empty:\n",
    "    print('df_stim is missing; run cell [55] first.')\n",
    "elif not globals().get('suite2p_by_ref_idx'):\n",
    "    print('Suite2p data not loaded; run cell [23a] first.')\n",
    "elif not Path(CONF_FUNC_CSV).exists():\n",
    "    print(f'Missing conf_to_func_pairs.csv: {CONF_FUNC_CSV} (run [50]).')\n",
    "else:\n",
    "    df_stim = globals()['df_stim'].copy()\n",
    "    if TARGET_STIM_TYPES is not None:\n",
    "        df_stim = df_stim[df_stim['type'].isin(list(TARGET_STIM_TYPES))]\n",
    "    if df_stim.empty:\n",
    "        print('No stimuli available after filtering.')\n",
    "    else:\n",
    "        pairs = pd.read_csv(CONF_FUNC_CSV)\n",
    "        if pairs.empty:\n",
    "            print('conf_to_func_pairs.csv is empty.')\n",
    "        else:\n",
    "            if 'gene' not in pairs.columns:\n",
    "                if 'conf_mask' in pairs.columns:\n",
    "                    if 'gene_from_mask' in globals():\n",
    "                        pairs['gene'] = pairs['conf_mask'].apply(gene_from_mask)\n",
    "                    else:\n",
    "                        def _gene_from_mask(path_str):\n",
    "                            name = Path(str(path_str)).name\n",
    "                            m = re.search(r'channel\\d+_(.+?)_cp_masks', name)\n",
    "                            gene = m.group(1) if m else name\n",
    "                            return gene.replace('sst1_', 'sst1.')\n",
    "                        pairs['gene'] = pairs['conf_mask'].apply(_gene_from_mask)\n",
    "                else:\n",
    "                    pairs['gene'] = 'unknown'\n",
    "\n",
    "            pairs = pairs[pairs['func_label'].notna() & pairs['plane'].notna()].copy()\n",
    "            pairs['plane'] = pairs['plane'].astype(int)\n",
    "            pairs['func_label'] = pairs['func_label'].astype(int)\n",
    "            pairs = pairs.drop_duplicates(subset=['plane', 'func_label', 'gene'])\n",
    "\n",
    "            s2p_map = globals()['suite2p_by_ref_idx']\n",
    "            pairs = pairs[pairs['plane'].isin(s2p_map.keys())].copy()\n",
    "            if pairs.empty:\n",
    "                print('No matching Suite2p planes for confocal pairs.')\n",
    "            else:\n",
    "                fps = globals().get('FRAME_RATE', None)\n",
    "                if fps is None:\n",
    "                    fs_vals = []\n",
    "                    for p in s2p_map.values():\n",
    "                        fs = p.get('ops', {}).get('fs', None)\n",
    "                        if fs is not None:\n",
    "                            fs_vals.append(float(fs))\n",
    "                    if fs_vals:\n",
    "                        fps = fs_vals[0]\n",
    "                        if len(set(fs_vals)) > 1:\n",
    "                            print('[stim] WARNING: multiple fs values found; using the first')\n",
    "                    else:\n",
    "                        print('FRAME_RATE not available; cannot align to time.')\n",
    "                        fps = None\n",
    "\n",
    "                if fps is None or fps <= 0:\n",
    "                    print('Invalid FRAME_RATE; set FRAME_RATE or ensure metadata has framerate.')\n",
    "                else:\n",
    "                    win_len = int(round((WINDOW_PRE_SEC + WINDOW_POST_SEC) * fps)) + 1\n",
    "                    tvec = (np.arange(win_len, dtype=np.float32) / float(fps)) - float(WINDOW_PRE_SEC)\n",
    "\n",
    "                    # Precompute event windows per stimulus type\n",
    "                    event_windows = {}\n",
    "                    durations = {}\n",
    "                    for _, row in df_stim.iterrows():\n",
    "                        stype = row['type']\n",
    "                        t0 = float(row['start'])\n",
    "                        dur = float(row.get('duration', 0.0))\n",
    "                        idx0 = int(round((t0 - WINDOW_PRE_SEC) * fps))\n",
    "                        idx1 = idx0 + win_len\n",
    "                        event_windows.setdefault(stype, []).append((idx0, idx1))\n",
    "                        durations.setdefault(stype, []).append(dur)\n",
    "\n",
    "                    stim_types = sorted(event_windows.keys())\n",
    "                    genes = sorted(pairs['gene'].dropna().unique().tolist())\n",
    "                    if not genes:\n",
    "                        print('No genes found in confocal pairs.')\n",
    "                    else:\n",
    "                        cmap = plt.cm.get_cmap('tab20')\n",
    "                        palette = cmap(np.linspace(0, 1, max(len(genes), 1)))\n",
    "                        gene_colors = {g: palette[i % len(palette)] for i, g in enumerate(genes)}\n",
    "\n",
    "                        results = {stype: {} for stype in stim_types}\n",
    "                        for stype in stim_types:\n",
    "                            windows = event_windows.get(stype, [])\n",
    "                            if not windows:\n",
    "                                continue\n",
    "                            for gene in genes:\n",
    "                                segs = []\n",
    "                                sub = pairs[pairs['gene'] == gene]\n",
    "                                roi_seen = set()\n",
    "                                for _, row in sub.iterrows():\n",
    "                                    plane = int(row['plane'])\n",
    "                                    roi_idx = int(row['func_label']) - 1\n",
    "                                    dff = s2p_map[plane].get('dff', None)\n",
    "                                    if dff is None:\n",
    "                                        continue\n",
    "                                    T = dff.shape[1]\n",
    "                                    roi_key = (plane, roi_idx)\n",
    "                                    roi_seen.add(roi_key)\n",
    "                                    if roi_idx < 0 or roi_idx >= dff.shape[0]:\n",
    "                                        continue\n",
    "                                    trace = dff[roi_idx]\n",
    "                                    for idx0, idx1 in windows:\n",
    "                                        if idx0 < 0 or idx1 > T:\n",
    "                                            continue\n",
    "                                        segs.append(trace[idx0:idx1])\n",
    "                                if segs:\n",
    "                                    arr = np.vstack(segs)\n",
    "                                    mean = arr.mean(axis=0)\n",
    "                                    sem = arr.std(axis=0, ddof=1) / np.sqrt(arr.shape[0]) if arr.shape[0] > 1 else None\n",
    "                                    results[stype][gene] = {\n",
    "                                        'mean': mean,\n",
    "                                        'sem': sem,\n",
    "                                        'n_segments': int(arr.shape[0]),\n",
    "                                        'n_rois': int(len(roi_seen)),\n",
    "                                    }\n",
    "\n",
    "                        # Build plot data with thresholds\n",
    "                        plot_data = {}\n",
    "                        for stype in stim_types:\n",
    "                            gene_items = [(g, v) for g, v in results.get(stype, {}).items() if v['n_segments'] >= MIN_SEGMENTS]\n",
    "                            if not gene_items:\n",
    "                                continue\n",
    "                            gene_items = sorted(gene_items, key=lambda kv: kv[1]['n_segments'], reverse=True)\n",
    "                            gene_items = gene_items[:MAX_GENES]\n",
    "                            plot_data[stype] = gene_items\n",
    "\n",
    "                        if not plot_data:\n",
    "                            print(f'[stim] No genes with >= {MIN_SEGMENTS} segments for any stimulus')\n",
    "                        else:\n",
    "                            plot_stims = []\n",
    "                            if STIM_ORDER:\n",
    "                                for s in STIM_ORDER:\n",
    "                                    if s in plot_data and s not in plot_stims:\n",
    "                                        plot_stims.append(s)\n",
    "                            for s in sorted(plot_data.keys()):\n",
    "                                if s not in plot_stims:\n",
    "                                    plot_stims.append(s)\n",
    "                            ncols = max(1, int(PLOT_NCOLS))\n",
    "                            nrows = int(math.ceil(len(plot_stims) / ncols))\n",
    "                            fig, axes = plt.subplots(nrows, ncols, figsize=(FIG_WIDTH, FIG_ROW_HEIGHT * nrows), sharex=True, sharey=True)\n",
    "                            axes = np.atleast_1d(axes).ravel()\n",
    "\n",
    "                            for ax, stype in zip(axes, plot_stims):\n",
    "                                gene_items = plot_data[stype]\n",
    "                                for gene, res in gene_items:\n",
    "                                    color = gene_colors.get(gene, None)\n",
    "                                    ax.plot(tvec, res['mean'], label=f\"{gene} (n={res['n_segments']})\", color=color)\n",
    "                                    if USE_SEM and res['sem'] is not None:\n",
    "                                        ax.fill_between(tvec, res['mean'] - res['sem'], res['mean'] + res['sem'], alpha=0.2, color=color)\n",
    "\n",
    "                                dur_vals = durations.get(stype, [])\n",
    "                                if dur_vals:\n",
    "                                    stim_dur = float(np.median(dur_vals))\n",
    "                                    ax.axvspan(0, stim_dur, color='#cccccc', alpha=0.2)\n",
    "\n",
    "                                ax.axvline(0, color='k', linestyle='--', linewidth=1.0)\n",
    "                                ax.set_title(f\"Stimulus-locked dF/F: {stype}\")\n",
    "                                ax.set_xlabel('Time (s)')\n",
    "                                ax.set_ylabel('dF/F')\n",
    "                                ax.set_ylim(0.0, 0.15)\n",
    "                                ax.set_yticks(np.arange(0.0, 0.151, 0.01))\n",
    "                                ax.legend(fontsize=8, ncol=2)\n",
    "\n",
    "                            for ax in axes[len(plot_stims):]:\n",
    "                                ax.axis('off')\n",
    "\n",
    "                            fig.tight_layout()\n",
    "                            plt.show()\n",
    "\n",
    "                        # Summary table\n",
    "                        summary_rows = []\n",
    "                        for stype in stim_types:\n",
    "                            for gene in genes:\n",
    "                                res = results.get(stype, {}).get(gene, None)\n",
    "                                if res is None:\n",
    "                                    continue\n",
    "                                summary_rows.append({\n",
    "                                    'stim_type': stype,\n",
    "                                    'gene': gene,\n",
    "                                    'n_segments': res['n_segments'],\n",
    "                                    'n_rois': res['n_rois'],\n",
    "                                })\n",
    "                        summary_df = pd.DataFrame(summary_rows)\n",
    "                        summary_df = summary_df[summary_df['n_segments'] > 0].reset_index(drop=True)\n",
    "                        if not summary_df.empty:\n",
    "                            try:\n",
    "                                display(summary_df)\n",
    "                            except Exception:\n",
    "                                print(summary_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ac32f0",
   "metadata": {},
   "source": [
    "### [57] Full-session gene traces with stimulus spans\n",
    "Plot mean dF/F per gene across the full experiment with stimulus-type shading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bb21de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [57]\n",
    "# Full-session mean dF/F per gene with stimulus spans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "FULL_TRACE_GENES = globals().get('FULL_TRACE_GENES', None)  # list or None\n",
    "FULL_TRACE_MAX_GENES = int(globals().get('FULL_TRACE_MAX_GENES', globals().get('MAX_GENES', 12)))\n",
    "FULL_TRACE_DOWNSAMPLE = int(globals().get('FULL_TRACE_DOWNSAMPLE', 1))\n",
    "FULL_TRACE_SMOOTH_SEC = float(globals().get('FULL_TRACE_SMOOTH_SEC', 0.0))\n",
    "FULL_TRACE_ALPHA = float(globals().get('FULL_TRACE_ALPHA', 0.9))\n",
    "FULL_TRACE_LINEWIDTH = float(globals().get('FULL_TRACE_LINEWIDTH', 1.0))\n",
    "FULL_FIG_WIDTH_IN = float(globals().get('FULL_FIG_WIDTH_IN', 13.33))\n",
    "FULL_FIG_HEIGHT_IN = float(globals().get('FULL_FIG_HEIGHT_IN', 7.5))\n",
    "FULL_FIG_DPI = int(globals().get('FULL_FIG_DPI', 300))\n",
    "STIM_SPAN_ALPHA = float(globals().get('STIM_SPAN_ALPHA', 0.15))\n",
    "FULL_Y_MIN = globals().get('FULL_Y_MIN', None)\n",
    "FULL_Y_MAX = globals().get('FULL_Y_MAX', None)\n",
    "FULL_Y_TICK = float(globals().get('FULL_Y_TICK', 0.02))\n",
    "CONF_FUNC_CSV = globals().get('CONF_FUNC_CSV', str(OUT_REG / 'conf_to_func_pairs.csv'))\n",
    "\n",
    "if 'df_stim' not in globals() or globals().get('df_stim') is None or globals()['df_stim'].empty:\n",
    "    print('df_stim is missing; run cell [55] first.')\n",
    "elif not globals().get('suite2p_by_ref_idx'):\n",
    "    print('Suite2p data not loaded; run cell [23a] first.')\n",
    "elif not Path(CONF_FUNC_CSV).exists():\n",
    "    print(f'Missing conf_to_func_pairs.csv: {CONF_FUNC_CSV} (run [50]).')\n",
    "else:\n",
    "    df_stim = globals()['df_stim'].copy()\n",
    "    pairs = pd.read_csv(CONF_FUNC_CSV)\n",
    "    if pairs.empty:\n",
    "        print('conf_to_func_pairs.csv is empty.')\n",
    "    else:\n",
    "        if 'gene' not in pairs.columns:\n",
    "            if 'conf_mask' in pairs.columns:\n",
    "                if 'gene_from_mask' in globals():\n",
    "                    pairs['gene'] = pairs['conf_mask'].apply(gene_from_mask)\n",
    "                else:\n",
    "                    def _gene_from_mask(path_str):\n",
    "                        name = Path(str(path_str)).name\n",
    "                        m = re.search(r'channel\\d+_(.+?)_cp_masks', name)\n",
    "                        gene = m.group(1) if m else name\n",
    "                        return gene.replace('sst1_', 'sst1.')\n",
    "                    pairs['gene'] = pairs['conf_mask'].apply(_gene_from_mask)\n",
    "            else:\n",
    "                pairs['gene'] = 'unknown'\n",
    "\n",
    "        pairs = pairs[pairs['func_label'].notna() & pairs['plane'].notna()].copy()\n",
    "        pairs['plane'] = pairs['plane'].astype(int)\n",
    "        pairs['func_label'] = pairs['func_label'].astype(int)\n",
    "        pairs = pairs.drop_duplicates(subset=['plane', 'func_label', 'gene'])\n",
    "\n",
    "        s2p_map = globals()['suite2p_by_ref_idx']\n",
    "        pairs = pairs[pairs['plane'].isin(s2p_map.keys())].copy()\n",
    "        if pairs.empty:\n",
    "            print('No matching Suite2p planes for confocal pairs.')\n",
    "        else:\n",
    "            fps = globals().get('FRAME_RATE', None)\n",
    "            if fps is None:\n",
    "                fs_vals = []\n",
    "                for p in s2p_map.values():\n",
    "                    fs = p.get('ops', {}).get('fs', None)\n",
    "                    if fs is not None:\n",
    "                        fs_vals.append(float(fs))\n",
    "                if fs_vals:\n",
    "                    fps = fs_vals[0]\n",
    "                    if len(set(fs_vals)) > 1:\n",
    "                        print('[stim] WARNING: multiple fs values found; using the first')\n",
    "                else:\n",
    "                    print('FRAME_RATE not available; cannot build time axis.')\n",
    "                    fps = None\n",
    "\n",
    "            if fps is None or fps <= 0:\n",
    "                print('Invalid FRAME_RATE; set FRAME_RATE or ensure metadata has framerate.')\n",
    "            else:\n",
    "                lengths = [p['dff'].shape[1] for p in s2p_map.values() if p.get('dff') is not None]\n",
    "                if not lengths:\n",
    "                    print('Suite2p dF/F traces missing.')\n",
    "                else:\n",
    "                    T_min = int(min(lengths))\n",
    "                    if len(set(lengths)) > 1:\n",
    "                        print('[stim] WARNING: planes have different lengths; truncating to shortest.')\n",
    "\n",
    "                    if FULL_TRACE_DOWNSAMPLE < 1:\n",
    "                        FULL_TRACE_DOWNSAMPLE = 1\n",
    "\n",
    "                    time_full = np.arange(T_min, dtype=np.float32) / float(fps)\n",
    "                    if FULL_TRACE_DOWNSAMPLE > 1:\n",
    "                        time_full = time_full[::FULL_TRACE_DOWNSAMPLE]\n",
    "\n",
    "                    # Build gene -> plane -> roi indices\n",
    "                    gene_groups = {}\n",
    "                    for _, row in pairs.iterrows():\n",
    "                        gene = row['gene']\n",
    "                        plane = int(row['plane'])\n",
    "                        roi_idx = int(row['func_label']) - 1\n",
    "                        if roi_idx < 0:\n",
    "                            continue\n",
    "                        gene_groups.setdefault(gene, {}).setdefault(plane, set()).add(roi_idx)\n",
    "\n",
    "                    # Choose genes\n",
    "                    if FULL_TRACE_GENES is not None:\n",
    "                        genes = [g for g in FULL_TRACE_GENES if g in gene_groups]\n",
    "                    else:\n",
    "                        gene_counts = []\n",
    "                        for g, planes in gene_groups.items():\n",
    "                            n = sum(len(v) for v in planes.values())\n",
    "                            gene_counts.append((g, n))\n",
    "                        gene_counts = sorted(gene_counts, key=lambda x: x[1], reverse=True)\n",
    "                        genes = [g for g, _ in gene_counts[:FULL_TRACE_MAX_GENES]]\n",
    "\n",
    "                    if not genes:\n",
    "                        print('No genes available for full-session plot.')\n",
    "                    else:\n",
    "                        # Compute mean trace per gene\n",
    "                        gene_traces = {}\n",
    "                        for gene in genes:\n",
    "                            planes = gene_groups.get(gene, {})\n",
    "                            if not planes:\n",
    "                                continue\n",
    "                            acc = np.zeros(T_min, dtype=np.float32)\n",
    "                            count = 0\n",
    "                            for plane, roi_set in planes.items():\n",
    "                                dff = s2p_map[plane].get('dff', None)\n",
    "                                if dff is None:\n",
    "                                    continue\n",
    "                                roi_idx = np.array(sorted(roi_set), dtype=int)\n",
    "                                roi_idx = roi_idx[(roi_idx >= 0) & (roi_idx < dff.shape[0])]\n",
    "                                if roi_idx.size == 0:\n",
    "                                    continue\n",
    "                                acc += dff[roi_idx, :T_min].sum(axis=0)\n",
    "                                count += int(roi_idx.size)\n",
    "                            if count > 0:\n",
    "                                mean = acc / float(count)\n",
    "                                if FULL_TRACE_SMOOTH_SEC and FULL_TRACE_SMOOTH_SEC > 0:\n",
    "                                    win = int(round(FULL_TRACE_SMOOTH_SEC * fps))\n",
    "                                    if win > 1:\n",
    "                                        kernel = np.ones(win, dtype=np.float32) / float(win)\n",
    "                                        mean = np.convolve(mean, kernel, mode='same')\n",
    "                                if FULL_TRACE_DOWNSAMPLE > 1:\n",
    "                                    mean = mean[::FULL_TRACE_DOWNSAMPLE]\n",
    "                                gene_traces[gene] = mean\n",
    "\n",
    "                        if not gene_traces:\n",
    "                            print('No traces computed for selected genes.')\n",
    "                        else:\n",
    "                            # Stimulus spans\n",
    "                            stim_palette = globals().get('stim_palette', None)\n",
    "                            if stim_palette is None:\n",
    "                                stim_palette = {\n",
    "                                    'LLC': '#0a5910',\n",
    "                                    'LLB': '#34B18C',\n",
    "                                    'RLC': '#4D0C2C',\n",
    "                                    'RLB': '#cf368f',\n",
    "                                    'LLC+RLC': '#281578',\n",
    "                                    'LLC+RLB': '#26200b',\n",
    "                                    'LLB+RLC': '#989999',\n",
    "                                    'LLB+RLB': '#94cae3',\n",
    "                                }\n",
    "\n",
    "                            cmap = plt.cm.get_cmap('tab20')\n",
    "                            palette = cmap(np.linspace(0, 1, max(len(genes), 1)))\n",
    "                            gene_colors = {g: palette[i % len(palette)] for i, g in enumerate(genes)}\n",
    "\n",
    "                            fig, ax = plt.subplots(figsize=(FULL_FIG_WIDTH_IN, FULL_FIG_HEIGHT_IN), dpi=FULL_FIG_DPI)\n",
    "                            span_labels = set()\n",
    "                            for _, row in df_stim.iterrows():\n",
    "                                stype = row['type']\n",
    "                                t0 = float(row['start'])\n",
    "                                t1 = float(row['end'])\n",
    "                                color = stim_palette.get(stype, '#cccccc')\n",
    "                                label = stype if stype not in span_labels else None\n",
    "                                ax.axvspan(t0, t1, color=color, alpha=STIM_SPAN_ALPHA, label=label)\n",
    "                                span_labels.add(stype)\n",
    "\n",
    "                            for gene, trace in gene_traces.items():\n",
    "                                ax.plot(time_full, trace, label=gene, color=gene_colors.get(gene), alpha=FULL_TRACE_ALPHA, linewidth=FULL_TRACE_LINEWIDTH)\n",
    "\n",
    "                            ax.set_title('Full-session mean dF/F by gene')\n",
    "                            ax.set_xlabel('Time (s)')\n",
    "                            ax.set_ylabel('dF/F')\n",
    "\n",
    "                            if FULL_Y_MIN is not None or FULL_Y_MAX is not None:\n",
    "                                ymin = float(FULL_Y_MIN) if FULL_Y_MIN is not None else None\n",
    "                                ymax = float(FULL_Y_MAX) if FULL_Y_MAX is not None else None\n",
    "                                ax.set_ylim(ymin, ymax)\n",
    "                            if FULL_Y_TICK and (FULL_Y_MIN is not None) and (FULL_Y_MAX is not None):\n",
    "                                ax.set_yticks(np.arange(float(FULL_Y_MIN), float(FULL_Y_MAX) + 1e-9, float(FULL_Y_TICK)))\n",
    "\n",
    "                            # Legends: gene traces and stimulus spans\n",
    "                            gene_handles, gene_labels = ax.get_legend_handles_labels()\n",
    "                            stim_handles = []\n",
    "                            stim_labels = []\n",
    "                            for h, lab in zip(gene_handles, gene_labels):\n",
    "                                if lab in stim_palette:\n",
    "                                    stim_handles.append(h)\n",
    "                                    stim_labels.append(lab)\n",
    "                            trace_handles = [h for h, lab in zip(gene_handles, gene_labels) if lab not in stim_palette]\n",
    "                            trace_labels = [lab for lab in gene_labels if lab not in stim_palette]\n",
    "\n",
    "                            gene_legend = None\n",
    "                            if trace_handles:\n",
    "                                gene_legend = ax.legend(trace_handles, trace_labels, loc='upper right', fontsize=8, ncol=2, title='Genes')\n",
    "                                ax.add_artist(gene_legend)\n",
    "                            if stim_handles:\n",
    "                                ax.legend(stim_handles, stim_labels, loc='upper left', fontsize=8, ncol=2, title='Stimuli')\n",
    "\n",
    "                            plt.tight_layout()\n",
    "                            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8210b9dc",
   "metadata": {
    "tags": [
     "functional",
     "hcr"
    ]
   },
   "outputs": [],
   "source": [
    "# --- HCR to functional space (per-gene intensity; save only) ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tifffile as tiff\n",
    "from skimage.transform import AffineTransform, warp\n",
    "import re\n",
    "\n",
    "try:\n",
    "    import ants  # type: ignore\n",
    "    HAVE_ANTSPY = True\n",
    "except Exception as _e:\n",
    "    HAVE_ANTSPY = False\n",
    "    _ANTSPY_ERR = _e\n",
    "\n",
    "# Config\n",
    "TFORM_PLANE_INDEX = int(globals().get('TFORM_PLANE_INDEX', 0))\n",
    "TFORM_LABEL = globals().get('TFORM_LABEL', None)\n",
    "TFORM_MATRIX = globals().get('TFORM_MATRIX', None)  # optional functional -> 2P matrix\n",
    "TFORM_CSV_PATH = globals().get(\n",
    "    'TFORM_CSV_PATH',\n",
    "    str(OUT_REG / 'tforms_by_plane.csv') if 'OUT_REG' in globals() else None,\n",
    ")\n",
    "\n",
    "HCR_ALIGNED_DIRS = globals().get('HCR_ALIGNED_DIRS', None)\n",
    "HCR_FILE_GLOBS = globals().get('HCR_FILE_GLOBS', ['*.nrrd', '*.tif', '*.tiff'])\n",
    "HCR_EXCLUDE_RE = globals().get('HCR_EXCLUDE_RE', r'(gcamp|channel1)')\n",
    "HCR_INTERP = globals().get('HCR_INTERP', 'linear')  # linear or nearest\n",
    "USE_ANTSPY_RESAMPLE = bool(globals().get('USE_ANTSPY_RESAMPLE', True))\n",
    "FIXED_ANTSPY_PATH = globals().get('FIXED_ANTSPY_PATH', None)\n",
    "\n",
    "OUTPUT_DTYPE = globals().get('OUTPUT_DTYPE', np.uint32)\n",
    "SAVE_OUTPUT = bool(globals().get('SAVE_OUTPUT', True))\n",
    "OUT_SUBDIR = globals().get('OUT_SUBDIR', 'hcr_in_func')\n",
    "SHOW_QC = bool(globals().get('SHOW_QC', False))\n",
    "\n",
    "def _load_any(path):\n",
    "    if 'imread_any' in globals():\n",
    "        return imread_any(path)\n",
    "    p = Path(path)\n",
    "    ext = p.suffix.lower()\n",
    "    if ext in ('.tif', '.tiff'):\n",
    "        return tiff.imread(str(p))\n",
    "    if ext in ('.npy', '.npz'):\n",
    "        obj = np.load(str(p), allow_pickle=True)\n",
    "        if isinstance(obj, np.lib.npyio.NpzFile):\n",
    "            for k in ('arr_0', 'data', 'stack'):\n",
    "                if k in obj:\n",
    "                    return obj[k]\n",
    "            return obj[list(obj.keys())[0]]\n",
    "        return obj\n",
    "    raise ValueError(f'Unsupported file type: {path}')\n",
    "\n",
    "if 'norm01' in globals():\n",
    "    _norm01 = globals()['norm01']\n",
    "else:\n",
    "    def _norm01(img):\n",
    "        img = img.astype(np.float32)\n",
    "        lo, hi = np.percentile(img, (1, 99))\n",
    "        if hi <= lo:\n",
    "            hi = float(img.max()) if img.size else 1.0\n",
    "            lo = float(img.min()) if img.size else 0.0\n",
    "        return np.clip((img - lo) / (hi - lo + 1e-6), 0, 1)\n",
    "\n",
    "def _first_not_none(*vals):\n",
    "    for v in vals:\n",
    "        if v is not None:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "def _get_anat_shape():\n",
    "    if 'anat' in globals() and globals().get('anat') is not None:\n",
    "        return np.asarray(globals()['anat']).shape, 'anat'\n",
    "    if 'ANAT_STACK_PATH' in globals() and globals().get('ANAT_STACK_PATH'):\n",
    "        return np.asarray(_load_any(globals()['ANAT_STACK_PATH'])).shape, 'ANAT_STACK_PATH'\n",
    "    if 'ANAT_INT_PATH' in globals() and globals().get('ANAT_INT_PATH'):\n",
    "        return np.asarray(_load_any(globals()['ANAT_INT_PATH'])).shape, 'ANAT_INT_PATH'\n",
    "    return None, 'none'\n",
    "\n",
    "def _maybe_reorder_to_shape(vol, target_shape):\n",
    "    if target_shape is None or vol.ndim != 3:\n",
    "        return vol, 'no-check'\n",
    "    if vol.shape == target_shape:\n",
    "        return vol, 'match'\n",
    "    axes = [(0, 1, 2), (0, 2, 1), (1, 0, 2), (1, 2, 0), (2, 0, 1), (2, 1, 0)]\n",
    "    for ax in axes:\n",
    "        if tuple(vol.shape[i] for i in ax) == target_shape:\n",
    "            if ax != (0, 1, 2):\n",
    "                return np.transpose(vol, ax), f'transpose{ax}'\n",
    "            return vol, 'match'\n",
    "    return vol, 'mismatch'\n",
    "\n",
    "def _ants_to_zyx(img):\n",
    "    arr = img.numpy()\n",
    "    if arr.ndim == 3:\n",
    "        return np.transpose(arr, (2, 1, 0))\n",
    "    return arr\n",
    "\n",
    "# Pick plane_ref (functional single-plane)\n",
    "if 'plane_refs' not in globals() or not globals()['plane_refs']:\n",
    "    raise RuntimeError('plane_refs missing; run the functional reference + best_z + tform cells first.')\n",
    "\n",
    "# Resolve HCR intensity volumes\n",
    "if HCR_ALIGNED_DIRS is None:\n",
    "    if 'FISH_DIR' in globals():\n",
    "        base = Path(FISH_DIR) / '02_reg'\n",
    "        HCR_ALIGNED_DIRS = [base / '01_rbest-2p' / 'aligned', base / '03_rn-2p' / 'aligned']\n",
    "    else:\n",
    "        HCR_ALIGNED_DIRS = []\n",
    "\n",
    "aligned_dirs = [Path(d) for d in HCR_ALIGNED_DIRS]\n",
    "ex_re = re.compile(HCR_EXCLUDE_RE, re.IGNORECASE)\n",
    "\n",
    "hcr_paths = []\n",
    "for d in aligned_dirs:\n",
    "    if not d.exists():\n",
    "        print(f'[HCR] missing dir: {d}')\n",
    "        continue\n",
    "    for g in HCR_FILE_GLOBS:\n",
    "        hcr_paths.extend(sorted(d.glob(g)))\n",
    "\n",
    "# Filter excludes and de-dup\n",
    "seen = set()\n",
    "filtered = []\n",
    "for p in hcr_paths:\n",
    "    if ex_re.search(p.name):\n",
    "        continue\n",
    "    s = str(p)\n",
    "    if s in seen:\n",
    "        continue\n",
    "    seen.add(s)\n",
    "    filtered.append(p)\n",
    "hcr_paths = filtered\n",
    "\n",
    "print(f'[HCR] found {len(hcr_paths)} intensity volumes (after exclude)')\n",
    "for p in hcr_paths:\n",
    "    print('  -', p)\n",
    "\n",
    "# Anatomy shape for sanity\n",
    "anat_shape, anat_source = _get_anat_shape()\n",
    "print('[HCR] anatomy shape source:', anat_source, 'shape:', anat_shape)\n",
    "\n",
    "# Optional tform CSV for fallback\n",
    "_tform_df = None\n",
    "if TFORM_CSV_PATH is not None and Path(TFORM_CSV_PATH).exists():\n",
    "    _tform_df = pd.read_csv(TFORM_CSV_PATH)\n",
    "    print('[HCR] loaded tforms CSV:', TFORM_CSV_PATH)\n",
    "\n",
    "# Output directory\n",
    "if 'OUT_HCR' in globals() and OUT_SUBDIR == 'hcr_in_func':\n",
    "    out_dir = Path(OUT_HCR)\n",
    "else:\n",
    "    out_dir = Path(OUTDIR) / OUT_SUBDIR if 'OUTDIR' in globals() else Path('.') / OUT_SUBDIR\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Resolve fixed image for ANTs resampling\n",
    "fixed_ants_img = None\n",
    "if USE_ANTSPY_RESAMPLE and HAVE_ANTSPY:\n",
    "    if FIXED_ANTSPY_PATH is None:\n",
    "        if 'ANAT_INT_PATH' in globals() and globals().get('ANAT_INT_PATH'):\n",
    "            FIXED_ANTSPY_PATH = str(globals()['ANAT_INT_PATH'])\n",
    "        elif 'ANAT_STACK_PATH' in globals() and globals().get('ANAT_STACK_PATH'):\n",
    "            FIXED_ANTSPY_PATH = str(globals()['ANAT_STACK_PATH'])\n",
    "    if FIXED_ANTSPY_PATH:\n",
    "        try:\n",
    "            fixed_ants_img = ants.image_read(str(FIXED_ANTSPY_PATH))\n",
    "            print('[HCR] ANTs fixed image:', FIXED_ANTSPY_PATH, 'shape', fixed_ants_img.shape)\n",
    "        except Exception as _e:\n",
    "            print('[HCR] ANTs could not read fixed image; fallback to numpy.', _e)\n",
    "    else:\n",
    "        print('[HCR] FIXED_ANTSPY_PATH not set; ANTs resample disabled.')\n",
    "elif USE_ANTSPY_RESAMPLE and not HAVE_ANTSPY:\n",
    "    print('[HCR] ANTsPy not available; fallback to numpy.', _ANTSPY_ERR)\n",
    "\n",
    "# Helper: select tform/best_z for a plane\n",
    "\n",
    "def _tform_for_plane(pref, p_idx):\n",
    "    best_z = int(pref.get('best_z', 0))\n",
    "    tform = pref.get('tform', None)\n",
    "    source = 'plane_refs'\n",
    "\n",
    "    if TFORM_MATRIX is not None:\n",
    "        M = np.asarray(TFORM_MATRIX, dtype=float)\n",
    "        if M.shape == (2, 3):\n",
    "            M = np.vstack([M, [0.0, 0.0, 1.0]])\n",
    "        tform = AffineTransform(matrix=M)\n",
    "        source = 'TFORM_MATRIX'\n",
    "        return tform, best_z, source\n",
    "\n",
    "    if tform is None and _tform_df is not None:\n",
    "        row = None\n",
    "        if TFORM_LABEL is not None and 'label' in _tform_df.columns:\n",
    "            hits = _tform_df[_tform_df['label'] == TFORM_LABEL]\n",
    "            if len(hits) == 1:\n",
    "                row = hits.iloc[0]\n",
    "        if row is None and 'label' in _tform_df.columns and pref.get('label') is not None:\n",
    "            hits = _tform_df[_tform_df['label'] == pref.get('label')]\n",
    "            if len(hits) == 1:\n",
    "                row = hits.iloc[0]\n",
    "        if row is None and 'plane_index' in _tform_df.columns:\n",
    "            hits = _tform_df[_tform_df['plane_index'] == p_idx]\n",
    "            if len(hits) >= 1:\n",
    "                row = hits.iloc[0]\n",
    "        if row is None and len(_tform_df) == 1:\n",
    "            row = _tform_df.iloc[0]\n",
    "        if row is not None:\n",
    "            best_z = int(row['best_z']) if 'best_z' in row else best_z\n",
    "            M = np.array(\n",
    "                [\n",
    "                    [row['m00'], row['m01'], row['m02']],\n",
    "                    [row['m10'], row['m11'], row['m12']],\n",
    "                    [0.0, 0.0, 1.0],\n",
    "                ],\n",
    "                dtype=float,\n",
    "            )\n",
    "            tform = AffineTransform(matrix=M)\n",
    "            source = 'TFORM_CSV'\n",
    "\n",
    "    return tform, best_z, source\n",
    "\n",
    "# Process each HCR intensity volume\n",
    "for hcr_path in hcr_paths:\n",
    "    name = hcr_path.stem\n",
    "    print(f'[HCR] loading {hcr_path.name}')\n",
    "\n",
    "    hcr_vol = None\n",
    "    if USE_ANTSPY_RESAMPLE and fixed_ants_img is not None:\n",
    "        try:\n",
    "            mov = ants.image_read(str(hcr_path))\n",
    "            interp_type = 1 if HCR_INTERP == 'linear' else 0\n",
    "            mov_r = ants.resample_image_to_target(mov, fixed_ants_img, interp_type=interp_type)\n",
    "            hcr_vol = _ants_to_zyx(mov_r)\n",
    "            print('[HCR] ANTs resample:', mov.shape, '->', mov_r.shape, 'ZYX', hcr_vol.shape)\n",
    "        except Exception as _e:\n",
    "            print('[HCR] ANTs resample failed; fallback to numpy.', _e)\n",
    "            hcr_vol = None\n",
    "\n",
    "    if hcr_vol is None:\n",
    "        hcr_vol = np.asarray(_load_any(hcr_path))\n",
    "        print('[HCR] raw shape:', hcr_vol.shape, 'dtype:', hcr_vol.dtype)\n",
    "        hcr_vol, status = _maybe_reorder_to_shape(hcr_vol, anat_shape)\n",
    "        print('[HCR] shape check:', status, '->', hcr_vol.shape)\n",
    "        if status == 'mismatch':\n",
    "            print('[HCR] WARNING: shape mismatch vs anatomy; skipping', hcr_path.name)\n",
    "            continue\n",
    "\n",
    "    if hcr_vol.ndim != 3:\n",
    "        print('[HCR] WARNING: expected 3D HCR volume; skipping', hcr_path.name)\n",
    "        continue\n",
    "\n",
    "    if anat_shape is not None and hcr_vol.shape != anat_shape:\n",
    "        print('[HCR] WARNING: HCR shape != anatomy shape:', hcr_vol.shape, 'vs', anat_shape)\n",
    "\n",
    "    out_dtype = np.dtype(OUTPUT_DTYPE)\n",
    "    if not np.issubdtype(hcr_vol.dtype, np.integer):\n",
    "        print('[HCR] note: HCR dtype is not integer; output will be rounded to OUTPUT_DTYPE.')\n",
    "    elif hcr_vol.dtype != out_dtype:\n",
    "        print(f'[HCR] note: HCR dtype {hcr_vol.dtype} -> output {out_dtype}.')\n",
    "\n",
    "    # Loop planes\n",
    "    for p_idx, pref in enumerate(globals()['plane_refs']):\n",
    "        func_ref = _first_not_none(pref.get('ref2d_raw'), pref.get('ref2d'), pref.get('ref_match'))\n",
    "        if func_ref is None:\n",
    "            print(f'[HCR] plane {p_idx}: missing func_ref; skipping')\n",
    "            continue\n",
    "        func_ref = np.asarray(func_ref)\n",
    "\n",
    "        tform, best_z, source = _tform_for_plane(pref, p_idx)\n",
    "        if tform is None:\n",
    "            print(f'[HCR] plane {p_idx}: missing tform; skipping')\n",
    "            continue\n",
    "        if not (0 <= best_z < hcr_vol.shape[0]):\n",
    "            print(f'[HCR] plane {p_idx}: best_z {best_z} out of bounds (z_size={hcr_vol.shape[0]}); skipping')\n",
    "            continue\n",
    "\n",
    "        hcr_2d = hcr_vol[int(best_z)]\n",
    "        order = 0 if HCR_INTERP == 'nearest' else 1\n",
    "        hcr_in_func = warp(\n",
    "            hcr_2d.astype(np.float32),\n",
    "            inverse_map=tform,\n",
    "            output_shape=func_ref.shape,\n",
    "            order=order,\n",
    "            preserve_range=True,\n",
    "        )\n",
    "\n",
    "        if SAVE_OUTPUT:\n",
    "            out_path = out_dir / f'{name}_in_func_plane{p_idx}_z{best_z}.tif'\n",
    "            if np.issubdtype(out_dtype, np.integer):\n",
    "                info = np.iinfo(out_dtype)\n",
    "                out_arr = np.clip(np.rint(hcr_in_func), info.min, info.max).astype(out_dtype)\n",
    "            else:\n",
    "                out_arr = hcr_in_func.astype(out_dtype)\n",
    "            tiff.imwrite(str(out_path), out_arr)\n",
    "            print(f'[HCR] saved {out_path} (plane {p_idx}, best_z {best_z}, tform={source})')\n",
    "\n",
    "        if SHOW_QC:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            plt.imshow(_norm01(func_ref), cmap='gray')\n",
    "            plt.imshow(_norm01(hcr_in_func), cmap='magma', alpha=0.35)\n",
    "            plt.title(f'{name} | plane {p_idx} | best_z={best_z}')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369303a3",
   "metadata": {
    "tags": [
     "organize"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Organize analysis outputs into stable layout ---\n",
    "AUTO_ORGANIZE_OUTPUTS = bool(globals().get(\"AUTO_ORGANIZE_OUTPUTS\", True))\n",
    "\n",
    "if AUTO_ORGANIZE_OUTPUTS:\n",
    "    try:\n",
    "        from tools.organize_analysis_outputs import organize\n",
    "        organize(ANALYSIS_DIR, apply=True, move_unknown=False, verbose=True)\n",
    "    except Exception as _e:\n",
    "        print(\"[organize] failed:\", _e)\n",
    "        print(\"[organize] fallback: python tools/organize_analysis_outputs.py --analysis-dir <...> --apply\")\n",
    "else:\n",
    "    print(\"[organize] AUTO_ORGANIZE_OUTPUTS is False; skipping.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "registrations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
