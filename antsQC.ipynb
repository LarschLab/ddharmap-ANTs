{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "521906e7",
   "metadata": {},
   "source": [
    "# Filtered Matching + Publish (confocal ↔ 2P)\n",
    "\n",
    "Self-contained QC notebook to: (1) gather helpers in one cell, (2) run matching with distance + overlap gating, optional 1–1 on the 2P side, (3) compute QC metrics (IoU, splits/merges), and (4) publish filtered outputs under legacy variable names so your plotting cells pick them up.\n",
    "\n",
    "Usage:\n",
    "- Adjust paths and thresholds in the Config cell.\n",
    "- Run cells top→bottom; plotting cells can live below and use the published variables (e.g., `pairs`, `match_df`, `summary_stats`).\n",
    "- Inputs are read from disk; no dependency on the original notebook's kernel state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0834819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- shared functions (auto) ---\n",
    "from tools.paths import *\n",
    "from tools.io_images import *\n",
    "from tools.hcr_channels import *\n",
    "from tools.image_ops import *\n",
    "from tools.labels import *\n",
    "from tools.transforms import *\n",
    "from tools.qc_plots import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e5528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default QC output directory\n",
    "from pathlib import Path\n",
    "QC_OUTPUT_DIR = Path('/Users/ddharmap/dataProcessing/2p_HCR/analysis/qc/output')\n",
    "QC_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print('QC_OUTPUT_DIR =', QC_OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9cb100",
   "metadata": {
    "tags": [
     "config"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Config: file paths, spacings, transforms, and thresholds ---\n",
    "from pathlib import Path\n",
    "\n",
    "# Try to auto-detect confocal labels in 2P grid from common filenames in CWD\n",
    "CANDIDATE_CONF_LABELS = [\n",
    "    'confocal_labels_in_2P_space_labels_uint16.tif',\n",
    "    'stack_confocal_labels_in_2P_space_labels_uint16.tif'\n",
    "]\n",
    "CONF_LABELS_2P_PATH = next((str(Path(p)) for p in CANDIDATE_CONF_LABELS if Path(p).exists()), None)\n",
    "\n",
    "# 2P segmentation path (Cellpose *_seg.npy). Set to your dataset.\n",
    "TWOP_SEG_NPY = globals().get('TWOP_SEG_NPY', '/Users/ddharmap/dataProcessing/2p_HCR/data/L427_f02/L427_f02_anatomy_2P_cort_seg_anis1.npy')\n",
    "\n",
    "# Optional: original-space confocal label TIFF and intensities (for warping)\n",
    "# If set and ANTsPy is available, the notebook can warp the confocal mask to 2P and cache results.\n",
    "CONF_LABEL_TIFF = \"/Volumes/jlarsch/default/D2c/07_Data/Danin/Cellpose/trainingCort/hcr_test/L427_f02_round1_channel2_cort_gauss_cp_masks.tif\"\n",
    "CONF_NRRD_PATH = \"/Volumes/jlarsch/default/D2c/07_Data/Matilde/Microscopy/L427_f02/02_reg/00_preprocessing/r1/L427_f02_round1_channel2_cort.nrrd\"\n",
    "TWOP_NRRD_PATH = \"/Volumes/jlarsch/default/D2c/07_Data/Matilde/Microscopy/L427_f02/02_reg/00_preprocessing/2p_anatomy/L427_f02_anatomy_2P_cort.nrrd\"\n",
    "WARP_PATH = \"/Volumes/jlarsch/default/D2c/07_Data/Matilde/Microscopy/L427_f02/02_reg/01_r1-2p/transMatrices/L427_f02_round1_GCaMP_to_ref1Warp.nii.gz\"\n",
    "AFFINE_PATH = \"/Volumes/jlarsch/default/D2c/07_Data/Matilde/Microscopy/L427_f02/02_reg/01_r1-2p/transMatrices/L427_f02_round1_GCaMP_to_ref0GenericAffine.mat\"\n",
    "INVERT_AFFINE   = bool(globals().get('INVERT_AFFINE', False))\n",
    "\n",
    "# Warp caching\n",
    "FORCE_RECOMPUTE_WARP = bool(globals().get('FORCE_RECOMPUTE_WARP', False))\n",
    "WARP_CACHE_BASENAME = QC_OUTPUT_DIR / 'confocal_labels_in_2P_space'\n",
    "\n",
    "# 2P voxel spacings (µm)\n",
    "VOX_2P = {\"dz\": 2.0, \"dy\": 0.6506220, \"dx\": 0.6506220}\n",
    "VOX_CONF = {\"dz\": 1.0, \"dy\": 0.20756645602494875, \"dx\": 0.20756645602494875}\n",
    "# Flip this to True to read spacings from the NRRDs via ANTs (slow on network paths).\n",
    "USE_IMAGE_SPACING = False  # keeps notebook fast when False\n",
    "\n",
    "# Matching + gating\n",
    "MATCH_METHOD = globals().get('MATCH_METHOD', 'nn')     # 'nn' or 'hungarian'\n",
    "MAX_DISTANCE_UM = float(globals().get('MAX_DISTANCE_UM', 10))\n",
    "REQUIRE_OVERLAP = bool(globals().get('REQUIRE_OVERLAP', True))\n",
    "MIN_OVERLAP_VOXELS = int(globals().get('MIN_OVERLAP_VOXELS', 1))\n",
    "DEDUP_BY_TWOP = globals().get('DEDUP_BY_TWOP', 'max_overlap')  # {'none','closest','max_overlap'}\n",
    "\n",
    "# What downstream plots should consume\n",
    "PLOT_USE_FINAL_1TO1 = bool(globals().get('PLOT_USE_FINAL_1TO1', True))\n",
    "\n",
    "print('CONF_LABELS_2P_PATH =', CONF_LABELS_2P_PATH)\n",
    "print('TWOP_SEG_NPY         =', TWOP_SEG_NPY)\n",
    "print('FORCE_RECOMPUTE_WARP =', FORCE_RECOMPUTE_WARP)\n",
    "print('WARP_CACHE_BASENAME  =', WARP_CACHE_BASENAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574f4f82",
   "metadata": {
    "tags": [
     "eval",
     "config"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Evaluation datasets (pre-warped confocal in 2P space) ---\n",
    "# Only external datasets here; the current ANTs run is added automatically from memory.\n",
    "EVAL_DATASETS = {\n",
    "    # Example placeholders — update to your files\n",
    "    'bigwarp':  {'name': 'BigWarp',  'conf_labels_2p_path': '/Users/ddharmap/dataProcessing/cellpose/trainingCort/manualBW/L427_f02_round1_channel2_cort_gauss_cp_masks_BW_in_2P.tif'},\n",
    "    'baseline': {'name': 'Baseline', 'conf_labels_2p_path': '/Users/ddharmap/dataProcessing/cellpose/trainingCort/manualBW/L427_f02_round1_channel2_cort_gauss_cp_masks_untransformed_in_2P.tif'},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b0e94b",
   "metadata": {
    "tags": [
     "spacing",
     "auto"
    ]
   },
   "outputs": [],
   "source": [
    "def _detect_spacing_from_nrrd(path):\n",
    "    try:\n",
    "        import ants  # type: ignore\n",
    "    except Exception:\n",
    "        return None\n",
    "    from pathlib import Path\n",
    "    if path is None or not Path(path).exists():\n",
    "        return None\n",
    "    img = ants.image_read(path)\n",
    "    sp = tuple(float(s) for s in img.spacing)  # (dx, dy, dz) or (dx, dy)\n",
    "    if len(sp) == 3:\n",
    "        return {\"dx\": sp[0], \"dy\": sp[1], \"dz\": sp[2]}\n",
    "    if len(sp) == 2:\n",
    "        return {\"dx\": sp[0], \"dy\": sp[1]}\n",
    "    return None\n",
    "\n",
    "def _merge_spacing(current: dict, detected: dict | None) -> tuple[dict, bool]:\n",
    "    if detected is None:\n",
    "        return current, False\n",
    "    new = dict(current)\n",
    "    for k in (\"dz\", \"dy\", \"dx\"):\n",
    "        if k in detected:\n",
    "            new[k] = detected[k]\n",
    "    return new, True\n",
    "\n",
    "if USE_IMAGE_SPACING:\n",
    "    vox2p_det = _detect_spacing_from_nrrd(TWOP_NRRD_PATH)\n",
    "    VOX_2P, ch2p = _merge_spacing(VOX_2P, vox2p_det)\n",
    "\n",
    "    voxconf_det = _detect_spacing_from_nrrd(CONF_NRRD_PATH)\n",
    "    VOX_CONF, chconf = _merge_spacing(VOX_CONF, voxconf_det)\n",
    "\n",
    "    print(\"VOX_2P  =\", VOX_2P,  \"(detected from NRRD)\" if ch2p else \"(kept config)\")\n",
    "    print(\"VOX_CONF=\", VOX_CONF, \"(detected from NRRD)\" if chconf else \"(kept config)\")\n",
    "else:\n",
    "    print(\"Using configured spacings (set USE_IMAGE_SPACING=True to detect):\")\n",
    "    print(\"VOX_2P  =\", VOX_2P)\n",
    "    print(\"VOX_CONF=\", VOX_CONF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c323898e",
   "metadata": {
    "tags": [
     "helpers",
     "auto",
     "generated",
     "codeANTs"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Helpers: imports, loaders, matching, overlap, QC ---\n",
    "import json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from skimage.measure import regionprops_table\n",
    "from IPython.display import display, HTML\n",
    "from pathlib import Path\n",
    "import tifffile as tiff\n",
    "\n",
    "def load_labels_any(path: str) -> np.ndarray:\n",
    "    assert path is not None and Path(path).exists(), f'Label file not found: {path}'\n",
    "    if path.endswith('.npy') or path.endswith('.npz'):\n",
    "        obj = np.load(path, allow_pickle=True)\n",
    "        if isinstance(obj, np.lib.npyio.NpzFile):\n",
    "            # Try common keys\n",
    "            for k in ('masks','labels','arr_0'):\n",
    "                if k in obj: return np.asarray(obj[k])\n",
    "            raise RuntimeError(f'Unsupported npz structure in {path}')\n",
    "        else:\n",
    "            arr = obj\n",
    "            if isinstance(arr, np.ndarray):\n",
    "                # Some Cellpose *_seg.npy are dicts; handle both\n",
    "                if arr.dtype == object and arr.shape == () and isinstance(arr.item(), dict):\n",
    "                    d = arr.item()\n",
    "                    for k in ('masks','labels'):\n",
    "                        if k in d: return np.asarray(d[k])\n",
    "                    raise RuntimeError('Dict npy has no masks/labels key')\n",
    "                return arr\n",
    "            # Fallback if np.load returns a Python object (rare)\n",
    "            try:\n",
    "                d = arr.item()\n",
    "                for k in ('masks','labels'):\n",
    "                    if k in d: return np.asarray(d[k])\n",
    "            except Exception:\n",
    "                pass\n",
    "            raise RuntimeError(f'Unsupported npy content in {path}')\n",
    "    elif path.endswith('.tif') or path.endswith('.tiff'):\n",
    "        return tiff.imread(path)\n",
    "    else:\n",
    "        raise RuntimeError(f'Unsupported label format: {path}')\n",
    "\n",
    "def compute_centroids(mask: np.ndarray) -> pd.DataFrame:\n",
    "    props = regionprops_table(mask, properties=('label','centroid'))\n",
    "    df = pd.DataFrame(props)\n",
    "    # regionprops_table returns centroid-0 (z), centroid-1 (y), centroid-2 (x)\n",
    "    df = df.rename(columns={'centroid-0':'z','centroid-1':'y','centroid-2':'x'})\n",
    "    df = df[df['label'] != 0].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def idx_to_um(df: pd.DataFrame, vox: dict) -> np.ndarray:\n",
    "    return np.column_stack([df['z'].to_numpy()*vox['dz'],\n",
    "                           df['y'].to_numpy()*vox['dy'],\n",
    "                           df['x'].to_numpy()*vox['dx']])\n",
    "\n",
    "def nearest_neighbor_match(P_src_um: np.ndarray, P_dst_um: np.ndarray):\n",
    "    tree = cKDTree(P_dst_um)\n",
    "    dists, nn = tree.query(P_src_um, k=1)\n",
    "    return dists, nn\n",
    "\n",
    "def hungarian_match(P_src_um: np.ndarray, P_dst_um: np.ndarray, max_cost=np.inf):\n",
    "    # Compute cost matrix lazily in blocks if needed; for moderate sizes do dense\n",
    "    from scipy.spatial.distance import cdist\n",
    "    C = cdist(P_src_um, P_dst_um)\n",
    "    if np.isfinite(max_cost):\n",
    "        C[C > max_cost] = max_cost\n",
    "    row_ind, col_ind = linear_sum_assignment(C)\n",
    "    dists = C[row_ind, col_ind]\n",
    "    return dists, col_ind, row_ind\n",
    "\n",
    "def compute_label_overlap(conf_labels_2p: np.ndarray, twop_labels: np.ndarray, min_overlap_voxels=1) -> pd.DataFrame:\n",
    "    assert conf_labels_2p.shape == twop_labels.shape, 'Label volumes must share shape'\n",
    "    a = conf_labels_2p.ravel()\n",
    "    b = twop_labels.ravel()\n",
    "    # Exclude background early\n",
    "    m = (a != 0) & (b != 0)\n",
    "    if not m.any():\n",
    "        return pd.DataFrame(columns=['conf_label','twoP_label','overlap_voxels'], dtype=int)\n",
    "    a = a[m].astype(np.int64, copy=False)\n",
    "    b = b[m].astype(np.int64, copy=False)\n",
    "    # Combine pairs into a single 64-bit key (safe for uint32 labels)\n",
    "    key = (a << 32) | b\n",
    "    uniq, counts = np.unique(key, return_counts=True)\n",
    "    conf = (uniq >> 32).astype(np.int64)\n",
    "    twop = (uniq & ((1<<32)-1)).astype(np.int64)\n",
    "    df = pd.DataFrame({'conf_label': conf, 'twoP_label': twop, 'overlap_voxels': counts.astype(int)})\n",
    "    if min_overlap_voxels > 1:\n",
    "        df = df[df['overlap_voxels'] >= int(min_overlap_voxels)].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def summarize_distances(dists: np.ndarray, valid_mask: np.ndarray) -> dict:\n",
    "    dists = np.asarray(dists)\n",
    "    valid_mask = np.asarray(valid_mask, dtype=bool)\n",
    "    if dists.size == 0:\n",
    "        return {\n",
    "            'n': 0, 'mean': 0.0, 'median': 0.0, 'p90': 0.0, 'max': 0.0,\n",
    "            'within_gate': 0, 'within_gate_frac': 0.0\n",
    "        }\n",
    "    return {\n",
    "        'n': int(dists.size),\n",
    "        'mean': float(np.mean(dists)),\n",
    "        'median': float(np.median(dists)),\n",
    "        'p90': float(np.percentile(dists, 90)),\n",
    "        'max': float(np.max(dists)),\n",
    "        'within_gate': int(valid_mask.sum()),\n",
    "        'within_gate_frac': float(valid_mask.mean())\n",
    "    }\n",
    "\n",
    "def display_scrollable(df: pd.DataFrame, max_h=600):\n",
    "    html = df.to_html(index=False).replace('<table', f'<table style=\"display:block; max-height:{max_h}px; overflow-y:auto; width:100%;\"')\n",
    "    display(HTML(html))\n",
    "\n",
    "# --- ANTs (optional) and warp helpers ---\n",
    "try:\n",
    "    import ants  # type: ignore\n",
    "    HAVE_ANTSPY = True\n",
    "except Exception as e:  # pragma: no cover\n",
    "    HAVE_ANTSPY = False\n",
    "    print('ANTsPy not available; warp/points transform cells will be disabled. ', e)\n",
    "\n",
    "def fs_info(path: str) -> dict:\n",
    "    import os, time\n",
    "    exists = os.path.exists(path) if path else False\n",
    "    size_b = os.path.getsize(path) if exists else None\n",
    "    mtime = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(os.path.getmtime(path))) if exists else None\n",
    "    return {'exists': exists, 'size_bytes': size_b, 'size_MB': (size_b/1e6 if size_b else None), 'modified': mtime}\n",
    "\n",
    "def _ants_clone_geometry(dst_img, like_img):\n",
    "    dst_img.set_spacing(like_img.spacing)\n",
    "    dst_img.set_origin(like_img.origin)\n",
    "    dst_img.set_direction(like_img.direction)\n",
    "    return dst_img\n",
    "\n",
    "def warp_label_tiff_with_ants(tiff_path, mov_img_int, fix_img_int, warp_path, affine_path, *, vox_moving=None, save_basename=None):\n",
    "    import numpy as np\n",
    "    import tifffile as tiff\n",
    "    from pathlib import Path\n",
    "    tiff_path = Path(tiff_path)\n",
    "    lab_zyx = tiff.imread(str(tiff_path))\n",
    "    if lab_zyx.ndim != 3:\n",
    "        raise ValueError(f'Expected 3D TIFF (Z,Y,X); got shape {lab_zyx.shape}')\n",
    "    lab_xyz = np.transpose(lab_zyx, (2, 1, 0)).astype(np.int32, copy=False)\n",
    "    mov_label_img = ants.from_numpy(lab_xyz)\n",
    "    _ants_clone_geometry(mov_label_img, mov_img_int)\n",
    "    if mov_label_img.shape != mov_img_int.shape:\n",
    "        raise RuntimeError(f'Label XYZ shape {mov_label_img.shape} != moving intensity XYZ shape {mov_img_int.shape}.')\n",
    "    transformlist = [str(warp_path), str(affine_path)]\n",
    "    whichtoinvert = [False, bool(INVERT_AFFINE)]\n",
    "    warped_xyz = ants.apply_transforms(\n",
    "        fixed=fix_img_int,\n",
    "        moving=mov_label_img,\n",
    "        transformlist=transformlist,\n",
    "        whichtoinvert=whichtoinvert,\n",
    "        interpolator='nearestNeighbor',\n",
    "    ).numpy().astype(np.int32, copy=False)\n",
    "    warped_zyx = np.transpose(warped_xyz, (2, 1, 0))\n",
    "    if save_basename is None:\n",
    "        save_basename = str(tiff_path.with_suffix('')) + '_in_fixed'\n",
    "    out = {}\n",
    "    max_id = int(warped_zyx.max())\n",
    "    nz = int((warped_zyx > 0).sum())\n",
    "    print(f'[warp_label_tiff_with_ants] warped shape(ZYX)={warped_zyx.shape} maxID={max_id} nonzero={nz}')\n",
    "    import numpy as _np\n",
    "    if max_id <= 65535:\n",
    "        out_tif = save_basename + '_labels_uint16.tif'\n",
    "        tiff.imwrite(out_tif, warped_zyx.astype(_np.uint16))\n",
    "        out['tif'] = out_tif\n",
    "    else:\n",
    "        out_npy = save_basename + '_labels_int32.npy'\n",
    "        _np.save(out_npy, warped_zyx)\n",
    "        out['npy'] = out_npy\n",
    "        warped_img = ants.from_numpy(np.transpose(warped_zyx, (2, 1, 0)))\n",
    "        _ants_clone_geometry(warped_img, fix_img_int)\n",
    "        out_nii = save_basename + '_labels_int32.nii.gz'\n",
    "        ants.image_write(warped_img, out_nii)\n",
    "        out['nii'] = out_nii\n",
    "    return warped_zyx, out\n",
    "\n",
    "def warp_cache_candidates(save_basename: str):\n",
    "    from pathlib import Path\n",
    "    # Prefer *.npy (faster load) over TIFF if both exist\n",
    "    return [Path(f'{save_basename}_labels_int32.npy'), Path(f'{save_basename}_labels_uint16.tif')]\n",
    "\n",
    "def load_cached_warp(save_basename: str):\n",
    "    import numpy as np\n",
    "    import tifffile as tiff\n",
    "    for c in warp_cache_candidates(save_basename):\n",
    "        if c.exists():\n",
    "            if c.suffix == '.npy':\n",
    "                return np.load(c, mmap_mode='r'), c\n",
    "            if c.suffix == '.tif':\n",
    "                return tiff.imread(str(c)), c\n",
    "    return None, None\n",
    "\n",
    "def warp_metadata_path(save_basename: str) -> Path:\n",
    "    return Path(f'{save_basename}_warp_meta.json')\n",
    "\n",
    "def read_warp_metadata(save_basename: str):\n",
    "    p = warp_metadata_path(save_basename)\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    return json.loads(p.read_text())\n",
    "\n",
    "def write_warp_metadata(save_basename: str, metadata: dict) -> None:\n",
    "    p = warp_metadata_path(save_basename)\n",
    "    p.write_text(json.dumps(metadata, indent=2))\n",
    "\n",
    "def warp_inputs_fingerprint() -> dict:\n",
    "    def stamp(path: str) -> dict:\n",
    "        info = fs_info(path)\n",
    "        return {'path': path, 'exists': info['exists'], 'size_bytes': info['size_bytes'], 'modified': info['modified']}\n",
    "    fp = {\n",
    "        'CONF_LABEL_TIFF': stamp(CONF_LABEL_TIFF) if CONF_LABEL_TIFF else None,\n",
    "        'CONF_NRRD_PATH':  stamp(CONF_NRRD_PATH)  if CONF_NRRD_PATH  else None,\n",
    "        'TWOP_NRRD_PATH':  stamp(TWOP_NRRD_PATH)  if TWOP_NRRD_PATH  else None,\n",
    "        'WARP_PATH':       stamp(WARP_PATH)       if WARP_PATH       else None,\n",
    "        'AFFINE_PATH':     stamp(AFFINE_PATH)     if AFFINE_PATH     else None,\n",
    "        'INVERT_AFFINE':   bool(INVERT_AFFINE),\n",
    "    }\n",
    "    if HAVE_ANTSPY:\n",
    "        fp['ANTsPy_version'] = getattr(ants, '__version__', None)\n",
    "    return fp\n",
    "\n",
    "def apply_transforms_to_points_um(points_um: np.ndarray, mov_spacing_um: dict, fix_spacing_um: dict, *, warp_path: str, affine_path: str, invert_affine: bool = False):\n",
    "    import numpy as np\n",
    "    if not HAVE_ANTSPY:\n",
    "        raise RuntimeError('ANTsPy required for apply_transforms_to_points.')\n",
    "    # µm -> index in moving\n",
    "    x = points_um[:, 2] / mov_spacing_um['dx']\n",
    "    y = points_um[:, 1] / mov_spacing_um['dy']\n",
    "    z = points_um[:, 0] / mov_spacing_um['dz']\n",
    "    df_idx = pd.DataFrame({'x': x, 'y': y, 'z': z})\n",
    "    transformlist = [str(warp_path), str(affine_path)]\n",
    "    whichtoinvert = [False, bool(invert_affine)]\n",
    "    fixed_idx = ants.apply_transforms_to_points(3, df_idx, transformlist, whichtoinvert=whichtoinvert)\n",
    "    # index -> µm in fixed\n",
    "    zu = fixed_idx['z'].to_numpy() * fix_spacing_um['dz']\n",
    "    yu = fixed_idx['y'].to_numpy() * fix_spacing_um['dy']\n",
    "    xu = fixed_idx['x'].to_numpy() * fix_spacing_um['dx']\n",
    "    return np.column_stack([zu, yu, xu])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d2cd1c",
   "metadata": {
    "tags": [
     "warp"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Warp confocal labels → 2P (cache-aware) ---\n",
    "# Fast path: use cached warp without reading large NRRDs\n",
    "cache_fp = warp_inputs_fingerprint()\n",
    "cached_labels, cached_source = load_cached_warp(WARP_CACHE_BASENAME)\n",
    "use_cache = (cached_labels is not None) and (not FORCE_RECOMPUTE_WARP)\n",
    "if use_cache:\n",
    "    conf_labels_2p = cached_labels\n",
    "    print(f'Loaded cached warp from {cached_source}')\n",
    "elif HAVE_ANTSPY and all(v is not None for v in [CONF_LABEL_TIFF, CONF_NRRD_PATH, TWOP_NRRD_PATH, WARP_PATH, AFFINE_PATH]):\n",
    "    # Slow path: need to run ANTs warp\n",
    "    mov_img_int = ants.image_read(CONF_NRRD_PATH)\n",
    "    fix_img_int = ants.image_read(TWOP_NRRD_PATH)\n",
    "    conf_labels_2p, out_paths = warp_label_tiff_with_ants(\n",
    "        CONF_LABEL_TIFF, mov_img_int, fix_img_int,\n",
    "        warp_path=WARP_PATH, affine_path=AFFINE_PATH,\n",
    "        save_basename=str(WARP_CACHE_BASENAME),\n",
    "    )\n",
    "    write_warp_metadata(WARP_CACHE_BASENAME, cache_fp)\n",
    "    print('Saved warp artifacts:', out_paths)\n",
    "else:\n",
    "    if not HAVE_ANTSPY:\n",
    "        print('ANTsPy not available; set CONF_LABELS_2P_PATH to pre-warped labels to proceed.')\n",
    "    else:\n",
    "        print('Warp inputs incomplete; using CONF_LABELS_2P_PATH if provided.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201c2a49",
   "metadata": {
    "tags": [
     "load"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Load label volumes and compute centroids ---\n",
    "# Prefer conf_labels_2p from the warp cell; otherwise load from disk if path is provided.\n",
    "if 'conf_labels_2p' not in globals():\n",
    "    # Prefer conf_labels_2p from the warp cell; otherwise load from disk if path is provided.\n",
    "    assert CONF_LABELS_2P_PATH is not None, 'Could not auto-detect confocal labels in 2P grid; set CONF_LABELS_2P_PATH.'\n",
    "    conf_labels_2p = load_labels_any(CONF_LABELS_2P_PATH)\n",
    "    conf_labels_2p = load_labels_any(CONF_LABELS_2P_PATH)\n",
    "    conf_labels_2p = load_labels_any(CONF_LABELS_2P_PATH)\n",
    "masks_2p = load_labels_any(TWOP_SEG_NPY)\n",
    "assert conf_labels_2p.shape == masks_2p.shape, f'Shape mismatch: {conf_labels_2p.shape} vs {masks_2p.shape}'\n",
    "\n",
    "df_conf = compute_centroids(conf_labels_2p)\n",
    "df_2p   = compute_centroids(masks_2p)\n",
    "\n",
    "P_conf_in_2p_um = idx_to_um(df_conf, VOX_2P)\n",
    "P_2p_um         = idx_to_um(df_2p, VOX_2P)\n",
    "\n",
    "print(f'Loaded confocal labels (warped→2P): {conf_labels_2p.shape}, dtype={conf_labels_2p.dtype}')\n",
    "print(f'Loaded 2P labels: {masks_2p.shape}, dtype={masks_2p.dtype}')\n",
    "print(f'Centroids: conf={P_conf_in_2p_um.shape[0]} | 2P={P_2p_um.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843f226a",
   "metadata": {
    "tags": [
     "match"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Matching + distance gate + overlap + optional 1–1 by 2P ---\n",
    "labels_conf = df_conf['label'].to_numpy()\n",
    "labels_2p   = df_2p['label'].to_numpy()\n",
    "\n",
    "if MATCH_METHOD == 'nn':\n",
    "    dists, nn = nearest_neighbor_match(P_conf_in_2p_um, P_2p_um)\n",
    "    matched_twoP_labels = labels_2p[nn]\n",
    "    matched_conf_labels = labels_conf\n",
    "elif MATCH_METHOD == 'hungarian':\n",
    "    dists, col_ind, row_ind = hungarian_match(P_conf_in_2p_um, P_2p_um, max_cost=np.inf)\n",
    "    matched_conf_labels = labels_conf[row_ind]\n",
    "    matched_twoP_labels = labels_2p[col_ind]\n",
    "else:\n",
    "    raise ValueError('MATCH_METHOD must be \"nn\" or \"hungarian\"')\n",
    "\n",
    "valid = dists <= float(MAX_DISTANCE_UM)\n",
    "matches = pd.DataFrame({\n",
    "    'conf_label': matched_conf_labels,\n",
    "    'twoP_label': matched_twoP_labels,\n",
    "    'distance_um': dists,\n",
    "    'within_gate': valid\n",
    "}).sort_values('distance_um', ascending=True).reset_index(drop=True)\n",
    "\n",
    "# Resolve label arrays for overlap\n",
    "conf_warped_labels = conf_labels_2p\n",
    "twoP_labels = masks_2p\n",
    "\n",
    "if REQUIRE_OVERLAP or DEDUP_BY_TWOP in {'closest','max_overlap'}:\n",
    "    overlap_df = compute_label_overlap(conf_warped_labels, twoP_labels, min_overlap_voxels=int(MIN_OVERLAP_VOXELS))\n",
    "    matches = matches.merge(overlap_df[['conf_label','twoP_label','overlap_voxels']],\n",
    "                           on=['conf_label','twoP_label'], how='left')\n",
    "    matches['overlap_voxels'] = matches['overlap_voxels'].fillna(0).astype(int)\n",
    "\n",
    "    if REQUIRE_OVERLAP:\n",
    "        matches['within_gate'] = matches['within_gate'] & (matches['overlap_voxels'] >= int(MIN_OVERLAP_VOXELS))\n",
    "\n",
    "    if DEDUP_BY_TWOP in {'closest','max_overlap'}:\n",
    "        m = matches['within_gate'].to_numpy()\n",
    "        if m.any():\n",
    "            sub = matches.loc[m].copy()\n",
    "            if DEDUP_BY_TWOP == 'closest':\n",
    "                sub = sub.sort_values(['twoP_label','distance_um'], ascending=[True, True])\n",
    "            else:\n",
    "                sub = sub.sort_values(['twoP_label','overlap_voxels','distance_um'], ascending=[True, False, True])\n",
    "            keep_idx = sub.drop_duplicates(subset=['twoP_label'], keep='first').index\n",
    "            drop_idx = sub.index.difference(keep_idx)\n",
    "            if len(drop_idx) > 0:\n",
    "                matches.loc[drop_idx, 'within_gate'] = False\n",
    "\n",
    "matches = matches.sort_values('distance_um', ascending=True).reset_index(drop=True)\n",
    "\n",
    "summary = summarize_distances(matches['distance_um'].to_numpy(), matches['within_gate'].to_numpy())\n",
    "print('Summary:', json.dumps(summary, indent=2))\n",
    "display_scrollable(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0822e26e",
   "metadata": {
    "tags": [
     "qc"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Post-matching QC (IoU-based) ---\n",
    "# Defines: final_pairs (high-confidence 1–1), review (needs attention), and prints a QC summary.\n",
    "# Notes:\n",
    "# - IOU_MIN = 0.05 means intersection is at least 5% of the union (symmetric).\n",
    "# - To require side-specific coverage, enable USE_FRAC_FILTERS and set MIN_OVERLAP_FRAC_*.\n",
    "\n",
    "# Thresholds\n",
    "IOU_MIN = 0.05            # intersection-over-union >= 5% of the union\n",
    "USE_FRAC_FILTERS = False  # also require side-specific fractions?\n",
    "MIN_OVERLAP_FRAC_CONF = 0.0\n",
    "MIN_OVERLAP_FRAC_TWOP = 0.0\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except Exception:\n",
    "    display = None  # if not in a notebook\n",
    "\n",
    "# Resolve label volumes in 2P space\n",
    "conf_warped_labels = globals().get('conf_labels_2p', globals().get('conf_warped_labels'))\n",
    "twoP_labels        = globals().get('masks_2p',      globals().get('twoP_labels'))\n",
    "assert conf_warped_labels is not None and twoP_labels is not None, \\\n",
    "    \"Need `conf_labels_2p` (or conf_warped_labels) and `masks_2p` (or twoP_labels).\"\n",
    "assert conf_warped_labels.shape == twoP_labels.shape, \\\n",
    "    f\"Shape mismatch: {conf_warped_labels.shape} vs {twoP_labels.shape}\"\n",
    "\n",
    "# Fallback for compute_label_overlap if not defined earlier\n",
    "if 'compute_label_overlap' not in globals():\n",
    "    def compute_label_overlap(conf, twop, min_overlap_voxels=1):\n",
    "        conf = np.asarray(conf); twop = np.asarray(twop)\n",
    "        a = conf.ravel(); b = twop.ravel()\n",
    "        m = (a != 0) & (b != 0)\n",
    "        if not m.any():\n",
    "            return pd.DataFrame(columns=['conf_label','twoP_label','overlap_voxels'], dtype=int)\n",
    "        a = a[m].astype(np.int64, copy=False); b = b[m].astype(np.int64, copy=False)\n",
    "        key = (a << 32) | b\n",
    "        uniq, counts = np.unique(key, return_counts=True)\n",
    "        return pd.DataFrame({\n",
    "            'conf_label': (uniq >> 32).astype(np.int64),\n",
    "            'twoP_label': (uniq & ((1<<32)-1)).astype(np.int64),\n",
    "            'overlap_voxels': counts.astype(int)\n",
    "        })\n",
    "\n",
    "# Ensure matches table exists\n",
    "assert 'matches' in globals(), \"Expected `matches` DataFrame from matching step.\"\n",
    "assert 'within_gate' in matches.columns, \"Expected `within_gate` column in matches.\"\n",
    "\n",
    "# Ensure overlap_voxels present on matches\n",
    "if 'overlap_voxels' not in matches.columns:\n",
    "    ov = compute_label_overlap(conf_warped_labels, twoP_labels, min_overlap_voxels=1)\n",
    "    matches = matches.merge(\n",
    "        ov[['conf_label','twoP_label','overlap_voxels']],\n",
    "        on=['conf_label','twoP_label'], how='left'\n",
    "    )\n",
    "    matches['overlap_voxels'] = matches['overlap_voxels'].fillna(0).astype(int)\n",
    "\n",
    "# Per-label volumes\n",
    "def label_volumes(arr):\n",
    "    labels, counts = np.unique(arr, return_counts=True)\n",
    "    s = pd.Series(counts, index=labels)\n",
    "    return s.drop(index=0, errors='ignore').astype(int)\n",
    "\n",
    "conf_vol_s = label_volumes(conf_warped_labels)\n",
    "twoP_vol_s = label_volumes(twoP_labels)\n",
    "\n",
    "matches['conf_vol'] = matches['conf_label'].map(conf_vol_s).fillna(0).astype(int)\n",
    "matches['twoP_vol'] = matches['twoP_label'].map(twoP_vol_s).fillna(0).astype(int)\n",
    "\n",
    "# IoU and overlap fractions\n",
    "den = matches['conf_vol'] + matches['twoP_vol'] - matches['overlap_voxels']\n",
    "matches['iou'] = np.divide(\n",
    "    matches['overlap_voxels'], den,\n",
    "    out=np.zeros_like(den, dtype=float), where=(den > 0)\n",
    ")\n",
    "matches['overlap_frac_conf'] = np.divide(\n",
    "    matches['overlap_voxels'], matches['conf_vol'],\n",
    "    out=np.zeros_like(matches['conf_vol'], dtype=float), where=(matches['conf_vol'] > 0)\n",
    ")\n",
    "matches['overlap_frac_twoP'] = np.divide(\n",
    "    matches['overlap_voxels'], matches['twoP_vol'],\n",
    "    out=np.zeros_like(matches['twoP_vol'], dtype=float), where=(matches['twoP_vol'] > 0)\n",
    ")\n",
    "\n",
    "# Topology among accepted (within_gate = True)\n",
    "acc = matches.loc[matches['within_gate']].copy()\n",
    "conf_counts = acc['conf_label'].value_counts()\n",
    "twop_counts = acc['twoP_label'].value_counts()\n",
    "\n",
    "def _pair_type(row):\n",
    "    if not row['within_gate']:\n",
    "        return 'rejected'\n",
    "    cm = int(conf_counts.get(row['conf_label'], 0))\n",
    "    tm = int(twop_counts.get(row['twoP_label'], 0))\n",
    "    if cm == 1 and tm == 1: return '1-1'\n",
    "    if cm > 1 and tm == 1:  return 'merge'   # many conf -> one 2P\n",
    "    if cm == 1 and tm > 1:  return 'split'   # one conf -> many 2P\n",
    "    return 'complex'        # many-to-many\n",
    "\n",
    "matches['pair_type'] = matches.apply(_pair_type, axis=1)\n",
    "\n",
    "# Quality label (IoU; optional side-specific fractions)\n",
    "if USE_FRAC_FILTERS:\n",
    "    ok_frac = (\n",
    "        (matches['overlap_frac_conf'] >= float(MIN_OVERLAP_FRAC_CONF)) &\n",
    "        (matches['overlap_frac_twoP'] >= float(MIN_OVERLAP_FRAC_TWOP))\n",
    "    )\n",
    "else:\n",
    "    ok_frac = True\n",
    "\n",
    "matches['quality'] = np.where(\n",
    "    matches['within_gate'] & (matches['iou'] >= float(IOU_MIN)) & ok_frac, 'good',\n",
    "    np.where(matches['within_gate'], 'iffy', 'rejected')\n",
    ")\n",
    "\n",
    "# Final outputs\n",
    "final_pairs = matches[\n",
    "    (matches['pair_type'] == '1-1') & (matches['quality'] == 'good')\n",
    "].copy().sort_values(['distance_um','twoP_label'])\n",
    "\n",
    "review = matches[\n",
    "    ((matches['pair_type'].isin(['split','merge','complex'])) & matches['within_gate']) |\n",
    "    ((matches['pair_type'] == '1-1') & (matches['quality'] != 'good'))\n",
    "].sort_values(['pair_type','iou','distance_um'], ascending=[True, False, True]).copy()\n",
    "\n",
    "# Disambiguated QC summary\n",
    "gate_mask = (matches['distance_um'] <= float(globals().get('MAX_DISTANCE_UM', 5.0)))\n",
    "if bool(globals().get('REQUIRE_OVERLAP', False)):\n",
    "    gate_mask = gate_mask & (matches['overlap_voxels'] >= int(globals().get('MIN_OVERLAP_VOXELS', 1)))\n",
    "\n",
    "qc = pd.Series({\n",
    "    'gate_only_pairs': int(gate_mask.sum()),\n",
    "    'accepted_pairs_after_dedup': int(matches['within_gate'].sum()),\n",
    "    'final_1to1_good': int(final_pairs.shape[0]),\n",
    "    'splits_among_accepted': int((matches['pair_type'] == 'split').sum()),\n",
    "    'merges_among_accepted': int((matches['pair_type'] == 'merge').sum()),\n",
    "    'complex_among_accepted': int((matches['pair_type'] == 'complex').sum()),\n",
    "}, name='QC Summary')\n",
    "\n",
    "print(qc)\n",
    "print(\"\\nfinal_pairs (head):\")\n",
    "display(final_pairs.head(20)) if display else print(final_pairs.head(20).to_string(index=False))\n",
    "print(\"\\nreview (head):\")\n",
    "display(review.head(20)) if display else print(review.head(20).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb50cf6",
   "metadata": {
    "tags": [
     "eval",
     "freeze"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Freeze evaluation pairs (final 1–1 good) ---\n",
    "assert 'final_pairs' in globals(), 'Run the QC cell first to create final_pairs.'\n",
    "# Only the label mapping is needed to evaluate across datasets\n",
    "eval_pairs = final_pairs[['conf_label','twoP_label']].copy()\n",
    "print(f'Frozen evaluation pairs: {len(eval_pairs)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b31fd62",
   "metadata": {
    "tags": [
     "eval",
     "precompute"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Precompute centroids per dataset (confocal in 2P space) ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tifffile as tiff\n",
    "\n",
    "# Helper: generic label loader\n",
    "def _load_labels_any(path: str):\n",
    "    p = str(path)\n",
    "    if p.endswith(('.tif', '.tiff')):\n",
    "        return tiff.imread(p)\n",
    "    if p.endswith('.npy'):\n",
    "        return np.load(p)\n",
    "    if p.endswith('.npz'):\n",
    "        obj = np.load(p, allow_pickle=True)\n",
    "        for k in ('masks','labels','arr_0'):\n",
    "            if k in obj: return obj[k]\n",
    "    raise RuntimeError(f'Unsupported label format: {p}')\n",
    "\n",
    "# Build 2P coordinate LUT once\n",
    "assert 'df_2p' in globals() and 'P_2p_um' in globals(), 'Need 2P centroids and coords.'\n",
    "_twoP_lut = dict(zip(df_2p['label'].to_numpy(), P_2p_um))\n",
    "\n",
    "# Cache per dataset\n",
    "EVAL_CACHE = {}\n",
    "EVAL_EXPORT_DIR = QC_OUTPUT_DIR / 'eval'\n",
    "EVAL_EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# Add current run as 'ANTs' from in-memory variables if available\n",
    "if all(k in globals() for k in ('conf_labels_2p','df_conf','P_conf_in_2p_um')):\n",
    "    EVAL_CACHE['ants'] = {\n",
    "        'name': 'ANTs',\n",
    "        'conf_arr': conf_labels_2p,\n",
    "        'df_conf': df_conf,\n",
    "        'P_conf_um': P_conf_in_2p_um,\n",
    "        'conf_lut': dict(zip(df_conf['label'].to_numpy(), P_conf_in_2p_um)),\n",
    "    }\n",
    "for key, meta in EVAL_DATASETS.items():\n",
    "    path = meta.get('conf_labels_2p_path')\n",
    "    if not path or not Path(path).exists():\n",
    "        print(f\"[WARN] Skipping '{key}' — missing file: {path}\")\n",
    "        continue\n",
    "    arr = _load_labels_any(path)\n",
    "    df_conf_ds = compute_centroids(arr)\n",
    "    P_conf_ds  = idx_to_um(df_conf_ds, VOX_2P)  # already in 2P grid\n",
    "    conf_lut   = dict(zip(df_conf_ds['label'].to_numpy(), P_conf_ds))\n",
    "    EVAL_CACHE[key] = {\n",
    "        'name': meta.get('name', key),\n",
    "        'conf_arr': arr,\n",
    "        'df_conf': df_conf_ds,\n",
    "        'P_conf_um': P_conf_ds,\n",
    "        'conf_lut': conf_lut,\n",
    "    }\n",
    "\n",
    "print('Eval datasets cached:', [f\"{k}({v['name']})\" for k,v in EVAL_CACHE.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be816fe2",
   "metadata": {
    "tags": [
     "eval",
     "summary"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Comparative summary across datasets (frozen eval_pairs) ---\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "def _eval_distances_for_pairs(conf_lut: dict, twoP_lut: dict, pairs_df: pd.DataFrame, drop_missing=True):\n",
    "    ds = []\n",
    "    miss = 0\n",
    "    for _, r in pairs_df.iterrows():\n",
    "        a = conf_lut.get(int(r['conf_label']))\n",
    "        b = twoP_lut.get(int(r['twoP_label']))\n",
    "        if a is None or b is None:\n",
    "            miss += 1\n",
    "            if not drop_missing:\n",
    "                ds.append(np.nan)\n",
    "            continue\n",
    "        dz, dy, dx = a[0]-b[0], a[1]-b[1], a[2]-b[2]\n",
    "        ds.append(float(np.sqrt(dz*dz + dy*dy + dx*dx)))\n",
    "    arr = np.asarray(ds, dtype=float)\n",
    "    if drop_missing:\n",
    "        arr = arr[np.isfinite(arr)]\n",
    "    return arr, miss\n",
    "\n",
    "assert 'EVAL_CACHE' in globals() and 'eval_pairs' in globals(), 'Run precompute + freeze cells first.'\n",
    "_twoP_lut = dict(zip(df_2p['label'].to_numpy(), P_2p_um))\n",
    "\n",
    "rows = []\n",
    "_all_tidy = []\n",
    "for key, payload in EVAL_CACHE.items():\n",
    "    dists, dropped = _eval_distances_for_pairs(payload['conf_lut'], _twoP_lut, eval_pairs, drop_missing=True)\n",
    "    stats = {\n",
    "        'dataset': payload['name'],\n",
    "        'n': int(dists.size),\n",
    "        'median': float(np.median(dists)) if dists.size else 0.0,\n",
    "        'p90': float(np.percentile(dists, 90)) if dists.size else 0.0,\n",
    "        'mean': float(np.mean(dists)) if dists.size else 0.0,\n",
    "        'max': float(np.max(dists)) if dists.size else 0.0,\n",
    "        'dropped_pairs': int(dropped),\n",
    "    }\n",
    "    rows.append(stats)\n",
    "    _all_tidy += [{'dataset': payload['name'], 'distance_um': float(x)} for x in dists]\n",
    "\n",
    "compare_df = pd.DataFrame(rows).sort_values('dataset')\n",
    "print('Comparative summary (final 1–1 good, frozen pairs):')\n",
    "try:\n",
    "    display(compare_df)\n",
    "except Exception:\n",
    "    print(compare_df.to_string(index=False))\n",
    "\n",
    "dist_by_dataset = pd.DataFrame(_all_tidy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9571c402",
   "metadata": {
    "tags": [
     "eval",
     "viewer"
    ]
   },
   "outputs": [],
   "source": [
    "# --- 3D viewer: switch datasets (confocal) + fixed 2P background ---\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from skimage.measure import marching_cubes\n",
    "\n",
    "assert 'EVAL_CACHE' in globals() and len(EVAL_CACHE) > 0, 'Run precompute datasets cell.'\n",
    "\n",
    "# Colors\n",
    "CONF_COLOR = '#f254a6'  # confocal (magenta)\n",
    "TWO_P_COLOR = '#33a6ff' # 2P (azure)\n",
    "PAIR_LINE_COLOR = 'red'\n",
    "PAIR_LINE_WIDTH = 5\n",
    "OPACITY = 0.10\n",
    "STEP_SIZE = 1\n",
    "\n",
    "# Voxel spacing (µm)\n",
    "dz = float(VOX_2P.get('dz', 1.0))\n",
    "dy = float(VOX_2P.get('dy', 1.0))\n",
    "dx = float(VOX_2P.get('dx', 1.0))\n",
    "\n",
    "# Build 2P background mesh once\n",
    "mask_2p = (masks_2p > 0)\n",
    "if np.any(mask_2p):\n",
    "    vT, fT, _, _ = marching_cubes(mask_2p.astype(np.uint8), level=0.5, spacing=(dz, dy, dx), step_size=STEP_SIZE)\n",
    "    iT, jT, kT = fT.T.astype(np.int32, copy=False)\n",
    "    zT, yT, xT = vT[:, 0], vT[:, 1], vT[:, 2]\n",
    "    t_twoP = go.Mesh3d(x=xT, y=yT, z=zT, i=iT, j=jT, k=kT, name='2P mask', color=TWO_P_COLOR, opacity=OPACITY, lighting=dict(ambient=0.5))\n",
    "else:\n",
    "    t_twoP = go.Mesh3d(x=[], y=[], z=[], i=[], j=[], k=[], name='2P mask', color=TWO_P_COLOR, opacity=OPACITY)\n",
    "\n",
    "# 2P centroids\n",
    "Z2, Y2, X2 = P_2p_um[:,0], P_2p_um[:,1], P_2p_um[:,2]\n",
    "pts_twoP = go.Scatter3d(x=X2, y=Y2, z=Z2, mode='markers', name='2P centroids', marker=dict(size=2, color=TWO_P_COLOR), showlegend=True)\n",
    "\n",
    "# Build per-dataset traces (conf mesh, conf centroids, pair lines)\n",
    "all_traces = [t_twoP, pts_twoP]\n",
    "trace_groups = {}\n",
    "\n",
    "# LUTs\n",
    "_twoP_lut = dict(zip(df_2p['label'].to_numpy(), P_2p_um))\n",
    "\n",
    "for key, payload in EVAL_CACHE.items():\n",
    "    name = payload['name']\n",
    "    arr = payload['conf_arr']\n",
    "    conf_mask = (arr > 0)\n",
    "    if np.any(conf_mask):\n",
    "        vC, fC, _, _ = marching_cubes(conf_mask.astype(np.uint8), level=0.5, spacing=(dz, dy, dx), step_size=STEP_SIZE)\n",
    "        iC, jC, kC = fC.T.astype(np.int32, copy=False)\n",
    "        zC, yC, xC = vC[:, 0], vC[:, 1], vC[:, 2]\n",
    "        t_conf = go.Mesh3d(x=xC, y=yC, z=zC, i=iC, j=jC, k=kC, name=f'{name} conf mask', color=CONF_COLOR, opacity=OPACITY, lighting=dict(ambient=0.5))\n",
    "    else:\n",
    "        t_conf = go.Mesh3d(x=[], y=[], z=[], i=[], j=[], k=[], name=f'{name} conf mask', color=CONF_COLOR, opacity=OPACITY)\n",
    "\n",
    "    P_conf = payload['P_conf_um']\n",
    "    Zc, Yc, Xc = P_conf[:,0], P_conf[:,1], P_conf[:,2]\n",
    "    pts_conf = go.Scatter3d(x=Xc, y=Yc, z=Zc, mode='markers', name=f'{name} conf centroids', marker=dict(size=2, color=CONF_COLOR), showlegend=True)\n",
    "\n",
    "    # Pair lines using frozen eval_pairs\n",
    "    xl, yl, zl = [], [], []\n",
    "    conf_lut = payload['conf_lut']\n",
    "    for _, r in eval_pairs.iterrows():\n",
    "        a = conf_lut.get(int(r['conf_label']))\n",
    "        b = _twoP_lut.get(int(r['twoP_label']))\n",
    "        if a is None or b is None:\n",
    "            continue\n",
    "        x0, y0, z0 = a[2], a[1], a[0]\n",
    "        x1, y1, z1 = b[2], b[1], b[0]\n",
    "        xl += [x0, x1, None]; yl += [y0, y1, None]; zl += [z0, z1, None]\n",
    "    pair_lines = go.Scatter3d(x=xl, y=yl, z=zl, mode='lines', name=f'{name} pairs', line=dict(color=PAIR_LINE_COLOR, width=PAIR_LINE_WIDTH), hoverinfo='skip', showlegend=True)\n",
    "\n",
    "    idx0 = len(all_traces)\n",
    "    all_traces += [t_conf, pts_conf, pair_lines]\n",
    "    trace_groups[key] = [idx0, idx0+1, idx0+2]\n",
    "\n",
    "# Initial visibility: 2P background + first dataset\n",
    "visible = [True, True] + [False]*(len(all_traces)-2)\n",
    "first_key = next(iter(trace_groups))\n",
    "for i in trace_groups[first_key]:\n",
    "    visible[i] = True\n",
    "\n",
    "fig = go.Figure(data=all_traces)\n",
    "\n",
    "# Dropdown to switch dataset\n",
    "buttons = []\n",
    "for key, idxs in trace_groups.items():\n",
    "    vis = [True, True] + [False]*(len(all_traces)-2)\n",
    "    for i in idxs:\n",
    "        vis[i] = True\n",
    "    buttons.append(dict(label=EVAL_CACHE[key]['name'], method='update', args=[{'visible': vis}, {'title': f\"3D view — dataset: {EVAL_CACHE[key]['name']}\"}]))\n",
    "\n",
    "fig.update_layout(\n",
    "    width=1400, height=900,\n",
    "    title=f\"3D view — dataset: {EVAL_CACHE[first_key]['name']}\",\n",
    "    scene=dict(xaxis_title='x (µm)', yaxis_title='y (µm)', zaxis_title='z (µm)', aspectmode='data'),\n",
    "    legend=dict(x=0.02, y=0.95, font=dict(size=10)),\n",
    "    updatemenus=[dict(type='dropdown', direction='down', x=1.05, y=0.95, showactive=True, xanchor='left', yanchor='top', buttons=buttons)],\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dbacdc",
   "metadata": {
    "tags": [
     "eval",
     "violin"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Violin: Baseline, BigWarp, ANTs (10 µm ticks, offset labels) ---\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "\n",
    "assert 'dist_by_dataset' in globals() and not dist_by_dataset.empty, 'Run comparative summary cell first.'\n",
    "\n",
    "# Enforce order\n",
    "preferred = ['Baseline', 'BigWarp', 'ANTs']\n",
    "cats_all = list(dist_by_dataset['dataset'].unique())\n",
    "cats = [c for c in preferred if c in cats_all] + [c for c in cats_all if c not in preferred]\n",
    "\n",
    "# Gather series and stats\n",
    "series, stats = [], []\n",
    "for c in cats:\n",
    "    vals = dist_by_dataset.loc[dist_by_dataset['dataset'] == c, 'distance_um'].to_numpy(float)\n",
    "    series.append(vals)\n",
    "    stats.append({'dataset': c, 'n': int(vals.size), 'median': float(np.median(vals)) if vals.size else 0.0})\n",
    "\n",
    "# Y-axis in 10 µm increments\n",
    "all_vals = np.concatenate(series) if any(s.size for s in series) else np.array([])\n",
    "y_max_data = float(np.max(all_vals)) if all_vals.size else 10.0\n",
    "y_max = max(10.0, 10.0 * ceil(y_max_data / 10.0))\n",
    "yticks = np.arange(0.0, y_max + 0.1, 10.0)\n",
    "\n",
    "plt.figure(figsize=(8, 4.5))\n",
    "parts = plt.violinplot(series, showmeans=False, showmedians=False, showextrema=False)\n",
    "\n",
    "# Style\n",
    "for pc in parts['bodies']:\n",
    "    pc.set_facecolor('#87bfff')\n",
    "    pc.set_edgecolor('black')\n",
    "    pc.set_alpha(0.7)\n",
    "\n",
    "# Offset annotations for legibility\n",
    "x_offset = 0.20\n",
    "y_offset = 0.02 * y_max\n",
    "bbox_style = dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1.0)\n",
    "\n",
    "# Median markers + labels\n",
    "for i, vals in enumerate(series, start=1):\n",
    "    if vals.size:\n",
    "        med = float(np.median(vals))\n",
    "        plt.scatter([i], [med], color='crimson', zorder=3, s=28)\n",
    "        plt.text(i + x_offset, med + y_offset,\n",
    "                 f\"median={med:.2f} µm\\nn={vals.size}\",\n",
    "                 va='bottom', ha='left', fontsize=8, bbox=bbox_style, clip_on=False, zorder=4)\n",
    "\n",
    "plt.xticks(range(1, len(cats) + 1), cats)\n",
    "plt.yticks(yticks)\n",
    "plt.ylim(0, y_max)\n",
    "plt.xlim(0.5, len(cats) + 0.8)  # right margin so labels don’t clip\n",
    "plt.ylabel('distance (µm)')\n",
    "plt.title('Final 1–1 good pairs — distance per dataset')\n",
    "plt.grid(axis='y', alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print a compact median table in the same order\n",
    "median_table = pd.DataFrame(stats, columns=['dataset', 'n', 'median'])\n",
    "try:\n",
    "    display(median_table)\n",
    "except Exception:\n",
    "    print(median_table.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c95c7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Axis-wise centroid errors: XY vs Z (Baseline, BigWarp, ANTs) ---\n",
    "# Computes per-pair XY planar error and Z error (in µm) between confocal and 2P centroids,\n",
    "# using the frozen eval_pairs and the cached datasets. Plots side-by-side violins.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "\n",
    "assert 'eval_pairs' in globals(), \"Run the freeze-eval-pairs cell first.\"\n",
    "assert 'df_2p' in globals() and 'P_2p_um' in globals(), \"Need 2P centroids/coords.\"\n",
    "# Ensure ANTs (current run) is present in EVAL_CACHE\n",
    "if 'EVAL_CACHE' not in globals():\n",
    "    EVAL_CACHE = {}\n",
    "EVAL_EXPORT_DIR = QC_OUTPUT_DIR / 'eval'\n",
    "EVAL_EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "if 'ants' not in EVAL_CACHE and all(k in globals() for k in ('conf_labels_2p','df_conf','P_conf_in_2p_um')):\n",
    "    EVAL_CACHE['ants'] = {\n",
    "        'name': 'ANTs',\n",
    "        'conf_arr': conf_labels_2p,\n",
    "        'df_conf': df_conf,\n",
    "        'P_conf_um': P_conf_in_2p_um,\n",
    "        'conf_lut': dict(zip(df_conf['label'].to_numpy(), P_conf_in_2p_um)),\n",
    "    }\n",
    "\n",
    "# Build 2P LUT once\n",
    "twoP_lut = dict(zip(df_2p['label'].to_numpy(), P_2p_um))\n",
    "\n",
    "# Helper: compute XY and Z errors for a dataset (drop pairs missing on either side)\n",
    "def axis_errors_for_dataset(conf_lut: dict, twoP_lut: dict, pairs_df: pd.DataFrame) -> tuple[np.ndarray, np.ndarray, int]:\n",
    "    xy_list, z_list = [], []\n",
    "    dropped = 0\n",
    "    for _, r in pairs_df.iterrows():\n",
    "        a = conf_lut.get(int(r['conf_label']))  # (z,y,x) in µm\n",
    "        b = twoP_lut.get(int(r['twoP_label']))  # (z,y,x) in µm\n",
    "        if a is None or b is None:\n",
    "            dropped += 1\n",
    "            continue\n",
    "        dz = float(a[0] - b[0])\n",
    "        dy = float(a[1] - b[1])\n",
    "        dx = float(a[2] - b[2])\n",
    "        xy = float(np.sqrt(dy*dy + dx*dx))\n",
    "        z  = float(abs(dz))\n",
    "        xy_list.append(xy)\n",
    "        z_list.append(z)\n",
    "    return np.asarray(xy_list, float), np.asarray(z_list, float), dropped\n",
    "\n",
    "# Desired order (only include those present)\n",
    "order_conf = ['Baseline', 'BigWarp', 'ANTs']\n",
    "present_order = [nm for nm in order_conf if any(v.get('name', k) == nm for k, v in EVAL_CACHE.items())]\n",
    "\n",
    "# Build tidy dataframe of errors\n",
    "rows = []\n",
    "for key, payload in EVAL_CACHE.items():\n",
    "    name = payload.get('name', key)\n",
    "    if name not in order_conf:\n",
    "        continue\n",
    "    xy, z, dropped = axis_errors_for_dataset(payload['conf_lut'], twoP_lut, eval_pairs)\n",
    "    rows += [{'dataset': name, 'xy_um': float(v), 'z_um': np.nan} for v in xy]\n",
    "    rows += [{'dataset': name, 'xy_um': np.nan,      'z_um': float(v)} for v in z]\n",
    "\n",
    "dist_axis = pd.DataFrame(rows)\n",
    "if dist_axis.empty:\n",
    "    raise RuntimeError(\"No axis-wise errors computed. Check EVAL_CACHE and eval_pairs.\")\n",
    "\n",
    "# Split per-axis series in desired order\n",
    "def collect_series(df: pd.DataFrame, col: str, cats: list[str]) -> list[np.ndarray]:\n",
    "    return [df.loc[df['dataset'] == c, col].dropna().to_numpy(float) for c in cats]\n",
    "\n",
    "series_xy = collect_series(dist_axis, 'xy_um', present_order)\n",
    "series_z  = collect_series(dist_axis, 'z_um',  present_order)\n",
    "\n",
    "# Axis titles\n",
    "labels = present_order  # e.g., ['Baseline','BigWarp','ANTs']\n",
    "\n",
    "# Y-axis ticks (make them readable; 2 µm steps up to a 10-multiple ceiling)\n",
    "def y_ticks_from_series(series_list):\n",
    "    all_vals = np.concatenate([s for s in series_list if s.size]) if any(s.size for s in series_list) else np.array([])\n",
    "    ymax_data = float(np.max(all_vals)) if all_vals.size else 10.0\n",
    "    ymax = max(10.0, 10.0 * ceil(ymax_data / 10.0))\n",
    "    step = 2.0 if ymax <= 20 else 5.0\n",
    "    yticks = np.arange(0.0, ymax + 0.1, step)\n",
    "    return ymax, yticks\n",
    "\n",
    "xy_ymax, xy_yticks = y_ticks_from_series(series_xy)\n",
    "z_ymax,  z_yticks  = y_ticks_from_series(series_z)\n",
    "\n",
    "# Plot side-by-side violins: XY error (left), Z error (right)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4.2), sharey=False)\n",
    "\n",
    "for ax, ser, title, ymax, yticks in [\n",
    "    (axes[0], series_xy, 'XY centroid error (µm)', xy_ymax, xy_yticks),\n",
    "    (axes[1], series_z,  'Z centroid error (µm)',  z_ymax,  z_yticks),\n",
    "]:\n",
    "    parts = ax.violinplot(ser, showmeans=False, showmedians=False, showextrema=False)\n",
    "    # Style\n",
    "    for pc in parts['bodies']:\n",
    "        pc.set_facecolor('#87bfff')\n",
    "        pc.set_edgecolor('black')\n",
    "        pc.set_alpha(0.7)\n",
    "    # Median markers + offset labels\n",
    "    x_offset = 0.18\n",
    "    y_offset = 0.02 * ymax\n",
    "    bbox_style = dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1.0)\n",
    "    for i, vals in enumerate(ser, start=1):\n",
    "        if vals.size:\n",
    "            med = float(np.median(vals))\n",
    "            ax.scatter([i], [med], color='crimson', zorder=3, s=26)\n",
    "            ax.text(i + x_offset, med + y_offset, f\"median={med:.2f} µm\\nn={vals.size}\",\n",
    "                    va='bottom', ha='left', fontsize=8, bbox=bbox_style, clip_on=False, zorder=4)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(range(1, len(labels) + 1))\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_ylim(0, ymax)\n",
    "    ax.set_yticks(yticks)\n",
    "    ax.grid(axis='y', alpha=0.2)\n",
    "\n",
    "fig.suptitle('Axis-wise centroid errors by dataset')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print compact medians (same dataset order)\n",
    "def med(col, ser_list, names):\n",
    "    out = []\n",
    "    for nm, vals in zip(names, ser_list):\n",
    "        out.append({'dataset': nm, 'n': int(vals.size), 'median_' + col: float(np.median(vals)) if vals.size else np.nan})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "xy_med = med('xy_um', series_xy, labels)\n",
    "z_med  = med('z_um',  series_z,  labels)\n",
    "summary_axis = pd.merge(xy_med, z_med, on=['dataset'], how='outer')\n",
    "try:\n",
    "    display(summary_axis)\n",
    "except Exception:\n",
    "    print(summary_axis.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1caa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mask diameters (µm): 2P vs confocal datasets (Baseline, BigWarp, ANTs) ---\n",
    "# Computes per-label diameters along Z, Y, X (via bbox extents) and compares distributions.\n",
    "# Uses eval_pairs to restrict to the same matched labels across datasets.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "from skimage.measure import regionprops_table\n",
    "\n",
    "assert 'VOX_2P' in globals(), \"Need VOX_2P with {'dz','dy','dx'}.\"\n",
    "assert 'masks_2p' in globals() and 'df_2p' in globals() and 'P_2p_um' in globals(), \"Need 2P labels and centroids.\"\n",
    "assert 'eval_pairs' in globals(), \"Run the freeze-eval-pairs cell first.\"\n",
    "# Confocal datasets should be available in EVAL_CACHE; ensure ANTs(current) is included\n",
    "if 'EVAL_CACHE' not in globals():\n",
    "    EVAL_CACHE = {}\n",
    "EVAL_EXPORT_DIR = QC_OUTPUT_DIR / 'eval'\n",
    "EVAL_EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "if 'ants' not in EVAL_CACHE and all(k in globals() for k in ('conf_labels_2p','df_conf','P_conf_in_2p_um')):\n",
    "    EVAL_CACHE['ants'] = {\n",
    "        'name': 'ANTs',\n",
    "        'conf_arr': conf_labels_2p,\n",
    "        'df_conf': df_conf,\n",
    "        'P_conf_um': P_conf_in_2p_um,\n",
    "        'conf_lut': dict(zip(df_conf['label'].to_numpy(), P_conf_in_2p_um)),\n",
    "    }\n",
    "\n",
    "# Helper: diameters (Z,Y,X) in µm from a label array in 2P grid, optionally restricted to a set of labels\n",
    "def diameters_um_from_array(arr, vox, restrict_labels=None):\n",
    "    props = regionprops_table(arr, properties=('label','bbox'))\n",
    "    df = pd.DataFrame(props)\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=['label','z_um','y_um','x_um'])\n",
    "    # bbox indices for 3D come as: bbox-0:zmin, bbox-1:ymin, bbox-2:xmin, bbox-3:zmax, bbox-4:ymax, bbox-5:xmax\n",
    "    df = df.rename(columns={\n",
    "        'bbox-0':'zmin','bbox-1':'ymin','bbox-2':'xmin',\n",
    "        'bbox-3':'zmax','bbox-4':'ymax','bbox-5':'xmax'\n",
    "    })\n",
    "    df['z_um'] = (df['zmax'] - df['zmin']) * float(vox['dz'])\n",
    "    df['y_um'] = (df['ymax'] - df['ymin']) * float(vox['dy'])\n",
    "    df['x_um'] = (df['xmax'] - df['xmin']) * float(vox['dx'])\n",
    "    df = df[['label','z_um','y_um','x_um']].copy()\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    if restrict_labels is not None:\n",
    "        keep = np.array(list(set(restrict_labels)), dtype=int)\n",
    "        df = df[df['label'].isin(keep)].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Restrict to matched IDs for fair comparison\n",
    "conf_eval_ids = np.unique(eval_pairs['conf_label'].to_numpy(int))\n",
    "twop_eval_ids = np.unique(eval_pairs['twoP_label'].to_numpy(int))\n",
    "\n",
    "# 2P diameters (single dataset)\n",
    "df_2p_diam = diameters_um_from_array(masks_2p, VOX_2P, restrict_labels=twop_eval_ids)\n",
    "df_2p_diam['dataset'] = '2P'\n",
    "\n",
    "# Confocal datasets (Baseline, BigWarp, ANTs)\n",
    "order_conf = ['Baseline','BigWarp','ANTs']  # desired order\n",
    "conf_dfs = []\n",
    "present = []\n",
    "for key, payload in EVAL_CACHE.items():\n",
    "    name = payload.get('name', key)\n",
    "    if name not in order_conf:\n",
    "        continue\n",
    "    arr = payload['conf_arr']\n",
    "    df_d = diameters_um_from_array(arr, VOX_2P, restrict_labels=conf_eval_ids)\n",
    "    df_d['dataset'] = name\n",
    "    conf_dfs.append(df_d)\n",
    "    present.append(name)\n",
    "\n",
    "if not conf_dfs:\n",
    "    raise RuntimeError(\"No confocal datasets found in EVAL_CACHE matching Baseline/BigWarp/ANTs.\")\n",
    "\n",
    "# Tidy table for plotting\n",
    "df_conf_all = pd.concat(conf_dfs, ignore_index=True)\n",
    "# Column order: Baseline, BigWarp, ANTs (only those present)\n",
    "cats_conf = [c for c in order_conf if c in present]\n",
    "\n",
    "# Build series per axis for plotting: [2P, Baseline, BigWarp, ANTs]\n",
    "def series_for_axis(df_2p, df_conf, axis):\n",
    "    s = []\n",
    "    s.append(df_2p[f'{axis}_um'].to_numpy(float))  # 2P\n",
    "    for c in cats_conf:\n",
    "        s.append(df_conf.loc[df_conf['dataset']==c, f'{axis}_um'].to_numpy(float))\n",
    "    return s\n",
    "\n",
    "series_x = series_for_axis(df_2p_diam, df_conf_all, 'x')\n",
    "series_y = series_for_axis(df_2p_diam, df_conf_all, 'y')\n",
    "series_z = series_for_axis(df_2p_diam, df_conf_all, 'z')\n",
    "\n",
    "# Common y-axis (10 µm ticks)\n",
    "all_vals = np.concatenate([a for a in series_x + series_y + series_z if a.size]) if any(\n",
    "    (a.size for a in (series_x + series_y + series_z))) else np.array([])\n",
    "y_max_data = float(np.max(all_vals)) if all_vals.size else 10.0\n",
    "from math import ceil\n",
    "y_max = max(10.0, 10.0 * ceil(y_max_data / 10.0))\n",
    "yticks = np.arange(0.0, y_max + 0.1, 10.0)\n",
    "\n",
    "# Labels and colors\n",
    "cats_plot = ['2P'] + cats_conf  # e.g., ['2P','Baseline','BigWarp','ANTs']\n",
    "colors = ['#bbbbbb'] + ['#87bfff'] * len(cats_conf)  # grey for 2P, blue for conf\n",
    "\n",
    "# Plot 3 subplots: X, Y, Z\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 4.2), sharey=True)\n",
    "for ax, ser, title in zip(axes, [series_x, series_y, series_z], ['X diameter (µm)', 'Y diameter (µm)', 'Z diameter (µm)']):\n",
    "    parts = ax.violinplot(ser, showmeans=False, showmedians=False, showextrema=False)\n",
    "    # Color violins\n",
    "    for i, pc in enumerate(parts['bodies']):\n",
    "        pc.set_facecolor(colors[i])\n",
    "        pc.set_edgecolor('black')\n",
    "        pc.set_alpha(0.7)\n",
    "    # Medians + n (offset for clarity)\n",
    "    x_offset = 0.18\n",
    "    y_offset = 0.02 * y_max\n",
    "    bbox_style = dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1.0)\n",
    "    for i, vals in enumerate(ser, start=1):\n",
    "        if vals.size:\n",
    "            med = float(np.median(vals))\n",
    "            ax.scatter([i], [med], color='crimson', zorder=3, s=26)\n",
    "            ax.text(i + x_offset, med + y_offset, f\"median={med:.2f} µm\\nn={vals.size}\",\n",
    "                    va='bottom', ha='left', fontsize=8, bbox=bbox_style, clip_on=False, zorder=4)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(range(1, len(cats_plot)+1))\n",
    "    ax.set_xticklabels(cats_plot, rotation=0)\n",
    "    ax.set_ylim(0, y_max)\n",
    "    ax.set_yticks(yticks)\n",
    "    ax.grid(axis='y', alpha=0.2)\n",
    "\n",
    "fig.suptitle('Mask diameters by axis — 2P vs confocal datasets')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print compact medians (in Baseline, BigWarp, ANTs order for conf; plus 2P)\n",
    "def med_row(name, df, prefix=''):\n",
    "    if df.empty:\n",
    "        return {'dataset': name, 'x_median': np.nan, 'y_median': np.nan, 'z_median': np.nan, 'n': 0}\n",
    "    return {\n",
    "        'dataset': name,\n",
    "        'x_median': float(np.median(df['x_um'])) if len(df['x_um']) else np.nan,\n",
    "        'y_median': float(np.median(df['y_um'])) if len(df['y_um']) else np.nan,\n",
    "        'z_median': float(np.median(df['z_um'])) if len(df['z_um']) else np.nan,\n",
    "        'n': int(len(df))\n",
    "    }\n",
    "\n",
    "rows = [med_row('2P', df_2p_diam)]\n",
    "for c in cats_conf:\n",
    "    rows.append(med_row(c, df_conf_all.loc[df_conf_all['dataset']==c]))\n",
    "med_table = pd.DataFrame(rows)\n",
    "try:\n",
    "    display(med_table)\n",
    "except Exception:\n",
    "    print(med_table.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c673b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Summary for accepted_pairs_after_dedup (within_gate == True) ---\n",
    "import numpy as np, json\n",
    "\n",
    "assert 'matches' in globals() and 'summarize_distances' in globals(), \"Need `matches` and `summarize_distances`.\"\n",
    "\n",
    "accepted_mask = matches['within_gate'].to_numpy()\n",
    "accepted_dists = matches.loc[accepted_mask, 'distance_um'].to_numpy(float)\n",
    "\n",
    "# Compute summary on accepted-only distances (n = number of accepted pairs)\n",
    "summary_accepted_after_dedup = summarize_distances(\n",
    "    accepted_dists,\n",
    "    np.ones_like(accepted_dists, dtype=bool)\n",
    ")\n",
    "\n",
    "print(f\"accepted_pairs_after_dedup: {accepted_mask.sum()}\")\n",
    "print(\"Summary (accepted-only):\")\n",
    "print(json.dumps(summary_accepted_after_dedup, indent=2))\n",
    "\n",
    "# Optional aliases if you want to reuse elsewhere\n",
    "accepted_summary = summary_accepted_after_dedup\n",
    "summary_within_gate_only = summary_accepted_after_dedup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e62ae82",
   "metadata": {
    "tags": [
     "publish"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Publish to downstream plots under legacy names ---\n",
    "scope = 'final_1to1' if PLOT_USE_FINAL_1TO1 and ('final_pairs' in globals()) else 'within_gate'\n",
    "if scope == 'final_1to1':\n",
    "    plot_df = final_pairs.copy()\n",
    "    key_final = set(map(tuple, plot_df[['conf_label','twoP_label']].to_numpy()))\n",
    "    accepted_mask = matches[['conf_label','twoP_label']].apply(tuple, axis=1).isin(key_final).to_numpy()\n",
    "else:\n",
    "    plot_df = matches.loc[matches['within_gate']].copy()\n",
    "    accepted_mask = matches['within_gate'].to_numpy()\n",
    "\n",
    "matches_for_plots = plot_df\n",
    "dists_all = matches['distance_um'].to_numpy()\n",
    "dists_filtered = matches_for_plots['distance_um'].to_numpy()\n",
    "summary_stats = summarize_distances(dists_all, accepted_mask)\n",
    "distance_summary = summary_stats\n",
    "\n",
    "# aliases\n",
    "pair_table = matches_for_plots\n",
    "pairings = matches_for_plots\n",
    "pairs_df = matches_for_plots\n",
    "pairs = matches_for_plots\n",
    "match_df = matches_for_plots\n",
    "\n",
    "dists = dists_all\n",
    "distances = dists_all\n",
    "accepted_dists = dists_filtered\n",
    "distances_filtered = dists_filtered\n",
    "\n",
    "valid_mask = accepted_mask\n",
    "within_gate_mask = accepted_mask\n",
    "\n",
    "print(f'Published scope for plots: {scope}')\n",
    "print(f'- pairs: {len(matches_for_plots)} rows')\n",
    "print(f'- dists_all: {dists_all.size} | dists_filtered: {dists_filtered.size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b7a50d",
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Export masks/labels/overlay for accepted pairs ---\n",
    "import numpy as np\n",
    "import tifffile as tiff\n",
    "from pathlib import Path\n",
    "\n",
    "# Validate prerequisites\n",
    "needed = ['conf_labels_2p','masks_2p','VOX_2P']\n",
    "missing = [n for n in needed if n not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing required variables in notebook scope: {missing}\")\n",
    "if conf_labels_2p.shape != masks_2p.shape:\n",
    "    raise ValueError(f\"Shape mismatch: conf_labels_2p {conf_labels_2p.shape} vs masks_2p {masks_2p.shape}\")\n",
    "\n",
    "# Choose scope from published outputs (matches_for_plots preferred)\n",
    "if 'matches_for_plots' in globals():\n",
    "    scope_df = matches_for_plots.copy()\n",
    "elif 'matches' in globals():\n",
    "    scope_df = matches[matches['within_gate']].copy()\n",
    "else:\n",
    "    raise RuntimeError('No matches or matches_for_plots available for export scope.')\n",
    "\n",
    "if scope_df.empty:\n",
    "    print('No accepted matches to export. Adjust gates or recompute matches.')\n",
    "else:\n",
    "    conf_ids = np.unique(scope_df['conf_label'].to_numpy(dtype=int))\n",
    "    twoP_ids = np.unique(scope_df['twoP_label'].to_numpy(dtype=int))\n",
    "    conf_ids = conf_ids[conf_ids != 0]\n",
    "    twoP_ids = twoP_ids[twoP_ids != 0]\n",
    "\n",
    "    # Boolean masks (ZYX)\n",
    "    conf_within_mask = np.isin(conf_labels_2p, conf_ids)\n",
    "    twoP_within_mask = np.isin(masks_2p, twoP_ids)\n",
    "\n",
    "    out_dir = QC_OUTPUT_DIR / 'tiff_exports'\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    dz_um = float(VOX_2P.get('dz', 1.0))\n",
    "\n",
    "    # 1) Binary masks (8-bit)\n",
    "    tiff.imwrite(\n",
    "        out_dir / 'conf_within_mask.tif',\n",
    "        (conf_within_mask.astype(np.uint8) * 255),\n",
    "        imagej=True,\n",
    "        metadata={'axes': 'ZYX', 'spacing': dz_um, 'unit': 'um'},\n",
    "        compression='deflate',\n",
    "    )\n",
    "    tiff.imwrite(\n",
    "        out_dir / 'twoP_within_mask.tif',\n",
    "        (twoP_within_mask.astype(np.uint8) * 255),\n",
    "        imagej=True,\n",
    "        metadata={'axes': 'ZYX', 'spacing': dz_um, 'unit': 'um'},\n",
    "        compression='deflate',\n",
    "    )\n",
    "\n",
    "    # 2) Label stacks (preserve IDs; background=0)\n",
    "    conf_within_labels = np.where(conf_within_mask, conf_labels_2p, 0)\n",
    "    twoP_within_labels = np.where(twoP_within_mask, masks_2p, 0)\n",
    "\n",
    "    def min_unsigned_dtype(max_val: int):\n",
    "        import numpy as _np\n",
    "        if max_val <= _np.iinfo(_np.uint16).max:\n",
    "            return _np.uint16\n",
    "        elif max_val <= _np.iinfo(_np.uint32).max:\n",
    "            return _np.uint32\n",
    "        else:\n",
    "            return _np.uint64\n",
    "\n",
    "    conf_dtype = min_unsigned_dtype(int(conf_within_labels.max()))\n",
    "    twoP_dtype = min_unsigned_dtype(int(twoP_within_labels.max()))\n",
    "    conf_within_labels = conf_within_labels.astype(conf_dtype, copy=False)\n",
    "    twoP_within_labels = twoP_within_labels.astype(twoP_dtype, copy=False)\n",
    "\n",
    "    def imwrite_with_meta(path, arr):\n",
    "        import numpy as _np\n",
    "        if arr.dtype in (_np.uint8, _np.uint16):\n",
    "            tiff.imwrite(\n",
    "                path, arr, imagej=True,\n",
    "                metadata={'axes': 'ZYX', 'spacing': dz_um, 'unit': 'um'},\n",
    "                compression='deflate',\n",
    "            )\n",
    "        else:\n",
    "            tiff.imwrite(path, arr, compression='deflate')\n",
    "\n",
    "    imwrite_with_meta(out_dir / 'conf_within_labels.tif', conf_within_labels)\n",
    "    imwrite_with_meta(out_dir / 'twoP_within_labels.tif', twoP_within_labels)\n",
    "\n",
    "    # 3) RGB overlay (conf=magenta, 2P=azure)\n",
    "    WRITE_OVERLAY = True\n",
    "    if WRITE_OVERLAY:\n",
    "        rgb = np.zeros(conf_within_mask.shape + (3,), dtype=np.uint8)\n",
    "        rgb[..., 0] = np.where(conf_within_mask, 242, 0)\n",
    "        rgb[..., 1] = np.where(conf_within_mask,  84, 0)\n",
    "        rgb[..., 2] = np.where(conf_within_mask, 166, 0)\n",
    "        rgb[..., 0] = np.clip(rgb[..., 0] + np.where(twoP_within_mask,  51, 0), 0, 255)\n",
    "        rgb[..., 1] = np.clip(rgb[..., 1] + np.where(twoP_within_mask, 166, 0), 0, 255)\n",
    "        rgb[..., 2] = np.clip(rgb[..., 2] + np.where(twoP_within_mask, 255, 0), 0, 255)\n",
    "        tiff.imwrite(out_dir / 'within_gate_overlay_rgb.tif', rgb, photometric='rgb', compression='deflate')\n",
    "\n",
    "    print(f'Saved exports to: {out_dir.resolve()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a1c8e",
   "metadata": {
    "tags": [
     "plots",
     "plotly"
    ]
   },
   "outputs": [],
   "source": [
    "# PLOTLY_3D_MATCH_VIEW\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from skimage.measure import marching_cubes\n",
    "\n",
    "# Tunables\n",
    "OPACITY = globals().get('OPACITY', 0.10)\n",
    "STEP_SIZE = globals().get('STEP_SIZE', 1)\n",
    "PAIR_LINE_COLOR = globals().get('PAIR_LINE_COLOR', 'red')\n",
    "PAIR_LINE_WIDTH = globals().get('PAIR_LINE_WIDTH', 5)\n",
    "\n",
    "# Validate deps\n",
    "need = ['conf_labels_2p','masks_2p','VOX_2P','df_conf','df_2p','P_conf_in_2p_um','P_2p_um']\n",
    "missing = [n for n in need if n not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f'Missing prerequisites: {missing}')\n",
    "\n",
    "# Choose which pairs to visualize\n",
    "df_pairs = None\n",
    "if 'matches_for_plots' in globals():\n",
    "    df_pairs = matches_for_plots.copy()\n",
    "elif 'matches' in globals():\n",
    "    df_pairs = matches.copy()\n",
    "else:\n",
    "    raise RuntimeError('Need matches or matches_for_plots for plotting.')\n",
    "\n",
    "dz, dy, dx = float(VOX_2P['dz']), float(VOX_2P['dy']), float(VOX_2P['dx'])\n",
    "\n",
    "def map_labels_to_values(label_vol, mapping, default_value, dtype):\n",
    "    max_label = int(label_vol.max())\n",
    "    lut = np.full(max_label + 1, default_value, dtype=dtype)\n",
    "    for k, v in mapping.items():\n",
    "        k = int(k)\n",
    "        if 0 < k <= max_label:\n",
    "            lut[k] = v\n",
    "    return lut[label_vol]\n",
    "\n",
    "# Fallback if within_gate column is absent\n",
    "wg_col = 'within_gate' if 'within_gate' in df_pairs.columns else None\n",
    "if wg_col is None:\n",
    "    df_pairs = df_pairs.copy()\n",
    "    df_pairs['_within_view'] = True\n",
    "    wg_col = '_within_view'\n",
    "\n",
    "# Lookup per-label best pair\n",
    "conf_lookup = (\n",
    "    df_pairs.sort_values('distance_um')\n",
    "            .drop_duplicates('conf_label', keep='first')\n",
    "            .set_index('conf_label')\n",
    ")\n",
    "twoP_lookup = (\n",
    "    df_pairs.sort_values('distance_um')\n",
    "            .drop_duplicates('twoP_label', keep='first')\n",
    "            .set_index('twoP_label')\n",
    ")\n",
    "\n",
    "conf_within_map = conf_lookup[wg_col].to_dict()\n",
    "twoP_within_map = twoP_lookup[wg_col].to_dict()\n",
    "conf_dist_map = conf_lookup['distance_um'].to_dict()\n",
    "twoP_dist_map = twoP_lookup['distance_um'].to_dict()\n",
    "\n",
    "conf_mask_all = (conf_labels_2p > 0)\n",
    "twoP_mask_all = (masks_2p > 0)\n",
    "\n",
    "# Distances; cap for coloring\n",
    "all_d = df_pairs['distance_um'].to_numpy(dtype=float)\n",
    "d_cap = float(np.nanmax([10.0, np.nanpercentile(all_d, 95)])) if np.isfinite(all_d).any() else 10.0\n",
    "\n",
    "conf_within_vol = map_labels_to_values(conf_labels_2p, conf_within_map, False, np.bool_)\n",
    "twoP_within_vol = map_labels_to_values(masks_2p,      twoP_within_map, False, np.bool_)\n",
    "conf_dist_vol   = np.clip(map_labels_to_values(conf_labels_2p, conf_dist_map, 0.0, np.float32), 0.0, d_cap)\n",
    "twoP_dist_vol   = np.clip(map_labels_to_values(masks_2p,      twoP_dist_map, 0.0, np.float32), 0.0, d_cap)\n",
    "\n",
    "# Marching cubes surfaces (in µm coords)\n",
    "def build_surface(mask):\n",
    "    if not np.any(mask):\n",
    "        return np.array([]), np.array([]), np.array([]), np.array([]), np.array([]), np.array([])\n",
    "    verts, faces, _, _ = marching_cubes(mask.astype(np.uint8), level=0.5, spacing=(dz, dy, dx), step_size=STEP_SIZE)\n",
    "    i, j, k = faces.T.astype(np.int32, copy=False)\n",
    "    zc, yc, xc = verts[:, 0], verts[:, 1], verts[:, 2]\n",
    "    return xc, yc, zc, i, j, k\n",
    "\n",
    "xC, yC, zC, iC, jC, kC = build_surface(conf_mask_all)\n",
    "xT, yT, zT, iT, jT, kT = build_surface(twoP_mask_all)\n",
    "\n",
    "def sample_nearest(vol, z_um, y_um, x_um):\n",
    "    if vol.size == 0 or len(z_um) == 0:\n",
    "        return np.array([], dtype=vol.dtype)\n",
    "    zi = np.clip(np.round(z_um / dz).astype(int), 0, vol.shape[0] - 1)\n",
    "    yi = np.clip(np.round(y_um / dy).astype(int), 0, vol.shape[1] - 1)\n",
    "    xi = np.clip(np.round(x_um / dx).astype(int), 0, vol.shape[2] - 1)\n",
    "    return vol[zi, yi, xi]\n",
    "\n",
    "# Intensities per metric\n",
    "conf_int_within = sample_nearest(conf_within_vol, zC, yC, xC).astype(float)\n",
    "twoP_int_within = sample_nearest(twoP_within_vol, zT, yT, xT).astype(float)\n",
    "conf_int_dist   = sample_nearest(conf_dist_vol,   zC, yC, xC).astype(float)\n",
    "twoP_int_dist   = sample_nearest(twoP_dist_vol,   zT, yT, xT).astype(float)\n",
    "conf_int_z      = zC.astype(float)\n",
    "twoP_int_z      = zT.astype(float)\n",
    "\n",
    "WITHIN_CS = [[0.0, '#e74c3c'], [1.0, '#2ecc71']]  # red/green\n",
    "DIST_CS   = 'Viridis'\n",
    "Z_CS      = 'Plasma'\n",
    "CONF_COLOR = '#f254a6'\n",
    "TWO_P_COLOR = '#33a6ff'\n",
    "\n",
    "def mesh_metric(x, y, z, i, j, k, inten, name, colorscale, cmin, cmax, showscale=False):\n",
    "    if len(x) == 0:\n",
    "        return go.Mesh3d(x=[], y=[], z=[], i=[], j=[], k=[], name=name, opacity=OPACITY, showlegend=True)\n",
    "    return go.Mesh3d(\n",
    "        x=x, y=y, z=z, i=i, j=j, k=k,\n",
    "        name=name,\n",
    "        intensity=inten,\n",
    "        colorscale=colorscale,\n",
    "        cmin=cmin, cmax=cmax,\n",
    "        showscale=showscale,\n",
    "        opacity=OPACITY,\n",
    "        lighting=dict(ambient=0.5),\n",
    "    )\n",
    "\n",
    "def mesh_solid(x, y, z, i, j, k, color, name):\n",
    "    if len(x) == 0:\n",
    "        return go.Mesh3d(x=[], y=[], z=[], i=[], j=[], k=[], name=name, opacity=OPACITY, showlegend=True)\n",
    "    return go.Mesh3d(x=x, y=y, z=z, i=i, j=j, k=k, name=name, color=color, opacity=OPACITY, lighting=dict(ambient=0.5))\n",
    "\n",
    "# Centroid traces per metric\n",
    "conf_z, conf_y, conf_x = P_conf_in_2p_um[:,0], P_conf_in_2p_um[:,1], P_conf_in_2p_um[:,2]\n",
    "twoP_z, twoP_y, twoP_x = P_2p_um[:,0], P_2p_um[:,1], P_2p_um[:,2]\n",
    "\n",
    "def centroid_metric_traces(metric):\n",
    "    if metric == 'within_gate':\n",
    "        conf_vals = df_conf['label'].map(conf_within_map).fillna(False).to_numpy().astype(float)\n",
    "        twoP_vals = df_2p['label'].map(twoP_within_map).fillna(False).to_numpy().astype(float)\n",
    "        cs, cmin, cmax = WITHIN_CS, 0.0, 1.0\n",
    "    elif metric == 'distance_um':\n",
    "        conf_vals = df_conf['label'].map(conf_dist_map).fillna(0.0).to_numpy(dtype=float)\n",
    "        twoP_vals = df_2p['label'].map(twoP_dist_map).fillna(0.0).to_numpy(dtype=float)\n",
    "        cs, cmin, cmax = DIST_CS, 0.0, d_cap\n",
    "    elif metric == 'z_um':\n",
    "        conf_vals = conf_z\n",
    "        twoP_vals = twoP_z\n",
    "        zmin = float(min(conf_z.min(), twoP_z.min())) if P_conf_in_2p_um.size and P_2p_um.size else 0.0\n",
    "        zmax = float(max(conf_z.max(), twoP_z.max())) if P_conf_in_2p_um.size and P_2p_um.size else 1.0\n",
    "        cs, cmin, cmax = Z_CS, zmin, zmax\n",
    "    else:  # source\n",
    "        cs, cmin, cmax = None, None, None\n",
    "\n",
    "    if metric == 'source':\n",
    "        c_conf = go.Scatter3d(x=conf_x, y=conf_y, z=conf_z, mode='markers', name='Conf centroids', marker=dict(size=2, color=CONF_COLOR), showlegend=True)\n",
    "        c_twoP = go.Scatter3d(x=twoP_x, y=twoP_y, z=twoP_z, mode='markers', name='2P centroids',   marker=dict(size=2, color=TWO_P_COLOR), showlegend=True)\n",
    "    else:\n",
    "        c_conf = go.Scatter3d(x=conf_x, y=conf_y, z=conf_z, mode='markers', name='Conf centroids', marker=dict(size=2, color=conf_vals, colorscale=cs, cmin=cmin, cmax=cmax, colorbar=dict(title=metric, len=0.5)), showlegend=True)\n",
    "        c_twoP = go.Scatter3d(x=twoP_x, y=twoP_y, z=twoP_z, mode='markers', name='2P centroids',   marker=dict(size=2, color=twoP_vals, colorscale=cs, cmin=cmin, cmax=cmax, showscale=False), showlegend=True)\n",
    "    return c_conf, c_twoP\n",
    "\n",
    "all_traces = []\n",
    "trace_index = {}\n",
    "\n",
    "# within_gate metric\n",
    "t_conf_w = mesh_metric(xC, yC, zC, iC, jC, kC, conf_int_within, 'Confocal mask', WITHIN_CS, 0, 1, showscale=True)\n",
    "t_twoP_w = mesh_metric(xT, yT, zT, iT, jT, kT, twoP_int_within, '2P mask',      WITHIN_CS, 0, 1, showscale=False)\n",
    "idx0 = len(all_traces); all_traces += [t_conf_w, t_twoP_w]\n",
    "c_conf_w, c_twoP_w = centroid_metric_traces('within_gate')\n",
    "idx1 = len(all_traces); all_traces += [c_conf_w, c_twoP_w]\n",
    "trace_index['within_gate'] = [idx0, idx0 + 1, idx1, idx1 + 1]\n",
    "\n",
    "# distance metric\n",
    "t_conf_d = mesh_metric(xC, yC, zC, iC, jC, kC, conf_int_dist, 'Confocal mask', DIST_CS, 0, d_cap, showscale=True)\n",
    "t_twoP_d = mesh_metric(xT, yT, zT, iT, jT, kT, twoP_int_dist, '2P mask',      DIST_CS, 0, d_cap, showscale=False)\n",
    "idx0 = len(all_traces); all_traces += [t_conf_d, t_twoP_d]\n",
    "c_conf_d, c_twoP_d = centroid_metric_traces('distance_um')\n",
    "idx1 = len(all_traces); all_traces += [c_conf_d, c_twoP_d]\n",
    "trace_index['distance_um'] = [idx0, idx0 + 1, idx1, idx1 + 1]\n",
    "\n",
    "# z metric\n",
    "zmin = float(min(conf_int_z.min(), twoP_int_z.min())) if (len(conf_int_z) and len(twoP_int_z)) else 0.0\n",
    "zmax = float(max(conf_int_z.max(), twoP_int_z.max())) if (len(conf_int_z) and len(twoP_int_z)) else 1.0\n",
    "t_conf_z = mesh_metric(xC, yC, zC, iC, jC, kC, conf_int_z, 'Confocal mask', Z_CS, zmin, zmax, showscale=True)\n",
    "t_twoP_z = mesh_metric(xT, yT, zT, iT, jT, kT, twoP_int_z, '2P mask',      Z_CS, zmin, zmax, showscale=False)\n",
    "idx0 = len(all_traces); all_traces += [t_conf_z, t_twoP_z]\n",
    "c_conf_z, c_twoP_z = centroid_metric_traces('z_um')\n",
    "idx1 = len(all_traces); all_traces += [c_conf_z, c_twoP_z]\n",
    "trace_index['z_um'] = [idx0, idx0 + 1, idx1, idx1 + 1]\n",
    "\n",
    "# source metric (solid colors)\n",
    "t_conf_s = mesh_solid(xC, yC, zC, iC, jC, kC, CONF_COLOR, 'Confocal mask')\n",
    "t_twoP_s = mesh_solid(xT, yT, zT, iT, jT, kT, TWO_P_COLOR, '2P mask')\n",
    "idx0 = len(all_traces); all_traces += [t_conf_s, t_twoP_s]\n",
    "c_conf_s, c_twoP_s = centroid_metric_traces('source')\n",
    "idx1 = len(all_traces); all_traces += [c_conf_s, c_twoP_s]\n",
    "trace_index['source'] = [idx0, idx0 + 1, idx1, idx1 + 1]\n",
    "\n",
    "# Pair lines based on df_pairs mapping (conf_label -> matched twoP)\n",
    "conf_pts = df_conf[['label']].copy()\n",
    "conf_pts['z_um'] = conf_z; conf_pts['y_um'] = conf_y; conf_pts['x_um'] = conf_x\n",
    "conf_pts['matched_twoP_label'] = conf_pts['label'].map(conf_lookup['twoP_label']) if not conf_lookup.empty else np.nan\n",
    "\n",
    "twoP_pts = df_2p[['label']].copy()\n",
    "twoP_pts['z_um'] = twoP_z; twoP_pts['y_um'] = twoP_y; twoP_pts['x_um'] = twoP_x\n",
    "twoP_map = twoP_pts.set_index('label').to_dict('index')\n",
    "\n",
    "xl, yl, zl = [], [], []\n",
    "for _, r in conf_pts.iterrows():\n",
    "    tgt = r['matched_twoP_label']\n",
    "    if pd.isna(tgt):\n",
    "        continue\n",
    "    tgt = int(tgt)\n",
    "    if tgt not in twoP_map:\n",
    "        continue\n",
    "    x0, y0, z0 = float(r['x_um']), float(r['y_um']), float(r['z_um'])\n",
    "    p1 = twoP_map[tgt]\n",
    "    x1, y1, z1 = float(p1['x_um']), float(p1['y_um']), float(p1['z_um'])\n",
    "    xl += [x0, x1, None]; yl += [y0, y1, None]; zl += [z0, z1, None]\n",
    "\n",
    "pair_lines = go.Scatter3d(x=xl, y=yl, z=zl, mode='lines', name='pairs (centroid links)', line=dict(color=PAIR_LINE_COLOR, width=PAIR_LINE_WIDTH), hoverinfo='skip', showlegend=True)\n",
    "pair_idx = len(all_traces)\n",
    "all_traces.append(pair_lines)\n",
    "\n",
    "# Initial visibility: 'within_gate' + pair lines\n",
    "visible = [False] * len(all_traces)\n",
    "for i in trace_index['within_gate']:\n",
    "    visible[i] = True\n",
    "visible[pair_idx] = True\n",
    "for i, tr in enumerate(all_traces):\n",
    "    tr.visible = visible[i]\n",
    "\n",
    "fig = go.Figure(data=all_traces)\n",
    "\n",
    "# Dropdown to switch metric (keep pair-line state)\n",
    "metrics = ['within_gate','distance_um','z_um','source']\n",
    "buttons = []\n",
    "for metric in metrics:\n",
    "    vis = [False] * len(all_traces)\n",
    "    for i in trace_index[metric]:\n",
    "        vis[i] = True\n",
    "    vis[pair_idx] = visible[pair_idx]\n",
    "    buttons.append(dict(label=f'Color by: {metric}', method='update', args=[{'visible': vis}, {'title': f'Mask+Centroids — color by {metric}'}]))\n",
    "\n",
    "fig.update_layout(\n",
    "    width=1500, height=1000,\n",
    "    title='Mask+Centroids — color by within_gate',\n",
    "    scene=dict(xaxis_title='x (µm)', yaxis_title='y (µm)', zaxis_title='z (µm)', aspectmode='data'),\n",
    "    legend=dict(x=0.02, y=0.70, font=dict(size=10)),\n",
    "    updatemenus=[dict(type='dropdown', direction='down', x=1.05, y=0.95, showactive=True, xanchor='left', yanchor='top', buttons=buttons)],\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3dd3df",
   "metadata": {
    "tags": [
     "plots",
     "slice"
    ]
   },
   "outputs": [],
   "source": [
    "# SLAB_VIEWER_2D\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider, Checkbox\n",
    "\n",
    "# convenience projections\n",
    "Z2, Y2, X2 = P_2p_um[:,0], P_2p_um[:,1], P_2p_um[:,2]\n",
    "Zc, Yc, Xc = P_conf_in_2p_um[:,0], P_conf_in_2p_um[:,1], P_conf_in_2p_um[:,2]\n",
    "\n",
    "zmin = float(min(Z2.min(), Zc.min())) if P_2p_um.size and P_conf_in_2p_um.size else 0.0\n",
    "zmax = float(max(Z2.max(), Zc.max())) if P_2p_um.size and P_conf_in_2p_um.size else 1.0\n",
    "default_thick = 4.0\n",
    "\n",
    "def _plot_slice(z_um=0.0, thickness_um=default_thick, show_conf=True, show_2p=True, show_pairs=False):\n",
    "    plt.figure()\n",
    "    if show_2p:\n",
    "        m2 = np.abs(Z2 - z_um) <= thickness_um\n",
    "        plt.scatter(X2[m2], Y2[m2], s=8, label='2P', alpha=0.9)\n",
    "    if show_conf:\n",
    "        mc = np.abs(Zc - z_um) <= thickness_um\n",
    "        plt.scatter(Xc[mc], Yc[mc], s=8, alpha=0.6, label='Conf→2P')\n",
    "    if show_pairs and 'matches_for_plots' in globals():\n",
    "        # build label→coord maps\n",
    "        coord_2p = dict(zip(df_2p['label'].to_numpy(), P_2p_um))\n",
    "        coord_conf = dict(zip(df_conf['label'].to_numpy(), P_conf_in_2p_um))\n",
    "        for _, row in matches_for_plots.iterrows():\n",
    "            a = coord_conf.get(int(row['conf_label']))\n",
    "            b = coord_2p.get(int(row['twoP_label']))\n",
    "            if a is None or b is None:\n",
    "                continue\n",
    "            if (abs(a[0]-z_um) <= thickness_um) and (abs(b[0]-z_um) <= thickness_um):\n",
    "                plt.plot([a[2], b[2]], [a[1], b[1]], linewidth=0.5)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('x (µm)'); plt.ylabel('y (µm)')\n",
    "    plt.title(f'Centroids near z = {z_um:.2f} µm (±{thickness_um:.2f})')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "interact(\n",
    "    _plot_slice,\n",
    "    z_um=FloatSlider(min=zmin, max=zmax, step=0.5, value=(zmin+zmax)/2, description='z (µm)'),\n",
    "    thickness_um=FloatSlider(min=0.5, max=20.0, step=0.5, value=default_thick, description='slab ±µm'),\n",
    "    show_conf=Checkbox(value=True, description='show Conf→2P'),\n",
    "    show_2p=Checkbox(value=True, description='show 2P'),\n",
    "    show_pairs=Checkbox(value=False, description='show pair lines'),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b06a178",
   "metadata": {
    "tags": [
     "plots",
     "hist"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Interactive Histogram: final 1–1 good pairs (fixed bins + fixed Y) ---\n",
    "# Uses only `final_pairs` (1–1 + IoU “good”), and shows how many are ≤ threshold.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider, Checkbox\n",
    "\n",
    "# Resolve final 1–1 good pairs\n",
    "if 'final_pairs' in globals():\n",
    "    fp = final_pairs.copy()\n",
    "else:\n",
    "    # Fallback if not defined: try to build from matches\n",
    "    assert 'matches' in globals(), \"Need `final_pairs` or `matches`.\"\n",
    "    assert {'pair_type','quality'}.issubset(matches.columns), \\\n",
    "        \"If `final_pairs` is missing, `matches` must have 'pair_type' and 'quality'.\"\n",
    "    fp = matches[(matches['pair_type'] == '1-1') & (matches['quality'] == 'good')].copy()\n",
    "    assert not fp.empty, \"No 1–1 good pairs found. Run the QC cell to define `final_pairs`.\"\n",
    "\n",
    "assert 'distance_um' in fp.columns, \"`final_pairs` must include 'distance_um'.\"\n",
    "\n",
    "# Distances of final 1–1 good pairs\n",
    "d_final = fp['distance_um'].to_numpy(dtype=float)\n",
    "n_final = d_final.size\n",
    "if n_final == 0:\n",
    "    raise RuntimeError(\"No final 1–1 good pairs to plot.\")\n",
    "\n",
    "# Fixed bin edges (computed once)\n",
    "bins_count = max(20, min(200, int(np.sqrt(max(1, n_final)))))\n",
    "edges = np.histogram_bin_edges(d_final, bins=bins_count)\n",
    "\n",
    "# Fixed Y-axis (based on all final pairs)\n",
    "counts_all, _ = np.histogram(d_final, bins=edges)\n",
    "y_max = max(int(counts_all.max() * 1.10), 1)\n",
    "\n",
    "# Slider defaults (ensure max ≥ 10)\n",
    "thr_suggest = float(np.nanpercentile(d_final, 75)) if n_final else float(globals().get('MAX_DISTANCE_UM', 5.0))\n",
    "pct99 = float(np.nanpercentile(d_final, 99)) if n_final else float(globals().get('MAX_DISTANCE_UM', 10.0))\n",
    "slider_max = max(10.0, pct99)\n",
    "slider_value = min(thr_suggest, slider_max)\n",
    "\n",
    "def _plot(threshold_um=slider_value, update_globals=False):\n",
    "    # Threshold subset\n",
    "    accepted_mask = d_final <= float(threshold_um)\n",
    "    accepted_dists = d_final[accepted_mask]\n",
    "    accepted = int(accepted_mask.sum())\n",
    "    frac = (accepted / n_final) if n_final else 0.0\n",
    "\n",
    "    # Plot with fixed edges and fixed Y\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(d_final, bins=edges, color='lightgray', alpha=0.7, label=f'final 1–1 good (n={n_final})')\n",
    "    if accepted > 0:\n",
    "        plt.hist(accepted_dists, bins=edges, color='steelblue', alpha=0.85,\n",
    "                 label=f'≤ {threshold_um:.2f} µm (n={accepted}, {frac:.1%})')\n",
    "    plt.axvline(threshold_um, color='crimson', linestyle='--', linewidth=1.5,\n",
    "                label=f'Threshold = {threshold_um:.2f} µm')\n",
    "    plt.ylim(0, y_max)\n",
    "    plt.xlabel('distance (µm)'); plt.ylabel('count'); plt.legend()\n",
    "    plt.title('Final 1–1 good pairs vs distance threshold')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    print(f\"Accepted final pairs: {accepted} / {n_final}  ({frac:.1%})\")\n",
    "    if accepted > 0:\n",
    "        print(f\"Accepted stats — median: {np.median(accepted_dists):.3f} µm | \"\n",
    "              f\"p90: {np.percentile(accepted_dists, 90):.3f} µm | max: {np.max(accepted_dists):.3f} µm\")\n",
    "\n",
    "    # Optional: publish mask back to original `matches` for reuse\n",
    "    if update_globals and 'matches' in globals():\n",
    "        key_all = set(map(tuple, fp[['conf_label','twoP_label']].to_numpy()))\n",
    "        key_acc = set(map(tuple, fp.loc[accepted_mask, ['conf_label','twoP_label']].to_numpy()))\n",
    "        full_mask = matches[['conf_label','twoP_label']].apply(tuple, axis=1).isin(key_acc).to_numpy()\n",
    "\n",
    "        globals()['MAX_DISTANCE_UM'] = float(threshold_um)\n",
    "        globals()['accepted_dists'] = accepted_dists\n",
    "        globals()['distances_filtered'] = accepted_dists\n",
    "        globals()['within_gate_mask'] = full_mask\n",
    "        globals()['valid_mask'] = full_mask\n",
    "        print(\"Published: MAX_DISTANCE_UM, accepted_dists, distances_filtered, within_gate_mask, valid_mask\")\n",
    "\n",
    "interact(\n",
    "    _plot,\n",
    "    threshold_um=FloatSlider(min=0.0, max=slider_max, step=0.1, value=slider_value,\n",
    "                             description='MAX_DISTANCE_UM'),\n",
    "    update_globals=Checkbox(value=False, description='publish vars')\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "registrations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
